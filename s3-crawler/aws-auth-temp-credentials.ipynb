{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# AWS Authentication with Temporary Credentials\n",
    "\n",
    "This notebook provides flexible AWS authentication methods, including support for temporary credentials.\n",
    "\n",
    "## Features:\n",
    "- Multiple authentication methods (IAM user, temporary credentials, SSO)\n",
    "- Session token support for MFA-protected accounts\n",
    "- Credential verification and testing\n",
    "- Automatic credential refresh handling\n",
    "- Profile-based authentication\n",
    "- Environment variable configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Optional, Any\n",
    "from botocore.exceptions import ClientError, NoCredentialsError, ProfileNotFound\n",
    "from botocore.session import Session\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. AWS Authentication Methods\n",
    "\n",
    "Choose one of the following authentication methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Method 1: Temporary Credentials (STS Assume Role or MFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Enter temporary credentials directly\n",
    "# Uncomment and fill in your temporary credentials\n",
    "\n",
    "TEMP_ACCESS_KEY_ID = \"\"  # e.g., \"ASIA...\"\n",
    "TEMP_SECRET_ACCESS_KEY = \"\"  # Your temporary secret key\n",
    "TEMP_SESSION_TOKEN = \"\"  # Your session token (required for temporary credentials)\n",
    "\n",
    "# Optional: Set expiration time if known\n",
    "TEMP_EXPIRATION = \"\"  # e.g., \"2024-01-20T12:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Session created with temporary credentials\n"
     ]
    }
   ],
   "source": [
    "def create_session_with_temp_credentials(\n",
    "    access_key_id: str,\n",
    "    secret_access_key: str,\n",
    "    session_token: str,\n",
    "    region_name: str = 'us-west-2'\n",
    ") -> boto3.Session:\n",
    "    \"\"\"\n",
    "    Create a boto3 session using temporary credentials.\n",
    "    \n",
    "    Args:\n",
    "        access_key_id: Temporary AWS access key ID\n",
    "        secret_access_key: Temporary AWS secret access key\n",
    "        session_token: AWS session token\n",
    "        region_name: AWS region (default: us-east-1)\n",
    "        \n",
    "    Returns:\n",
    "        boto3.Session object configured with temporary credentials\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=access_key_id,\n",
    "        aws_secret_access_key=secret_access_key,\n",
    "        aws_session_token=session_token,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    return session\n",
    "\n",
    "# Create session with temporary credentials if provided\n",
    "if TEMP_ACCESS_KEY_ID and TEMP_SECRET_ACCESS_KEY and TEMP_SESSION_TOKEN:\n",
    "    session = create_session_with_temp_credentials(\n",
    "        TEMP_ACCESS_KEY_ID,\n",
    "        TEMP_SECRET_ACCESS_KEY,\n",
    "        TEMP_SESSION_TOKEN\n",
    "    )\n",
    "    print(\"âœ… Session created with temporary credentials\")\n",
    "else:\n",
    "    print(\"âš ï¸ Temporary credentials not provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 3. Credential Verification and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using session\n",
      "âœ… AWS Credentials Valid!\n",
      "\n",
      "Account Details:\n",
      "  Account ID: 444055461661\n",
      "  User ARN: arn:aws:iam::444055461661:user/klesinger\n",
      "  User ID: AIDAWOY6ET4O25743W2P7\n"
     ]
    }
   ],
   "source": [
    "def verify_credentials(session: Optional[boto3.Session] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Verify AWS credentials and get caller identity.\n",
    "    \n",
    "    Args:\n",
    "        session: boto3 Session object (uses default if None)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with account information or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if session:\n",
    "            print('using session')\n",
    "            sts_client = session.client('sts')\n",
    "        else:\n",
    "            sts_client = boto3.client('sts')\n",
    "        \n",
    "        # Get caller identity\n",
    "        response = sts_client.get_caller_identity()\n",
    "        \n",
    "        print(\"âœ… AWS Credentials Valid!\")\n",
    "        print(f\"\\nAccount Details:\")\n",
    "        print(f\"  Account ID: {response['Account']}\")\n",
    "        print(f\"  User ARN: {response['Arn']}\")\n",
    "        print(f\"  User ID: {response['UserId']}\")\n",
    "        \n",
    "        # Check if using temporary credentials\n",
    "        if 'assumed-role' in response['Arn'] or response['Arn'].startswith('arn:aws:sts'):\n",
    "            print(\"\\nğŸ“Œ Using temporary credentials (assumed role or session token)\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        print(\"âŒ No credentials found. Please configure credentials using one of the methods above.\")\n",
    "        return None\n",
    "    except ClientError as e:\n",
    "        print(f\"âŒ Error verifying credentials: {e}\")\n",
    "        return None\n",
    "\n",
    "# Verify current credentials\n",
    "if 'session' in locals():\n",
    "    identity = verify_credentials(session)\n",
    "else:\n",
    "    identity = verify_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. Test AWS Service Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Can access bucket: nasa-disasters\n",
      "  Found 5 objects (showing max 5)\n"
     ]
    }
   ],
   "source": [
    "def test_s3_access(session: Optional[boto3.Session] = None, bucket_name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Test S3 access with current credentials.\n",
    "    \n",
    "    Args:\n",
    "        session: boto3 Session object (uses default if None)\n",
    "        bucket_name: Specific bucket to test (lists all buckets if None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if session:\n",
    "            s3_client = session.client('s3')\n",
    "        else:\n",
    "            s3_client = boto3.client('s3')\n",
    "        \n",
    "        if bucket_name:\n",
    "            # Test specific bucket access\n",
    "            response = s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"âœ… Can access bucket: {bucket_name}\")\n",
    "            \n",
    "            # Try to list objects\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=5)\n",
    "            object_count = response.get('KeyCount', 0)\n",
    "            print(f\"  Found {object_count} objects (showing max 5)\")\n",
    "            \n",
    "        else:\n",
    "            # List all accessible buckets\n",
    "            response = s3_client.list_buckets()\n",
    "            buckets = response.get('Buckets', [])\n",
    "            \n",
    "            print(f\"âœ… S3 Access Confirmed\")\n",
    "            print(f\"\\nAccessible Buckets ({len(buckets)} total):\")\n",
    "            \n",
    "            for bucket in buckets[:10]:  # Show first 10\n",
    "                print(f\"  - {bucket['Name']} (created: {bucket['CreationDate'].strftime('%Y-%m-%d')})\")\n",
    "            \n",
    "            if len(buckets) > 10:\n",
    "                print(f\"  ... and {len(buckets) - 10} more\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AccessDenied':\n",
    "            print(f\"âŒ Access Denied to S3\")\n",
    "        elif error_code == 'NoSuchBucket':\n",
    "            print(f\"âŒ Bucket '{bucket_name}' does not exist\")\n",
    "        else:\n",
    "            print(f\"âŒ Error accessing S3: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Unexpected error: {e}\")\n",
    "\n",
    "# # Test S3 access\n",
    "# if 'session' in locals():\n",
    "#     test_s3_access(session)\n",
    "# else:\n",
    "#     test_s3_access()\n",
    "\n",
    "# Test specific bucket (uncomment and modify):\n",
    "test_s3_access(session, \"nasa-disasters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 8. Integration with S3 Bucket Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ready to use with S3 bucket crawler!\n",
      "\n",
      "You can now:\n",
      "1. Import the S3BucketCrawler class from the other notebook\n",
      "2. Pass this s3_client to the crawler: S3BucketCrawler('bucket-name', s3_client)\n",
      "\n",
      "Example:\n",
      "  crawler = S3BucketCrawler('nasa-disasters', s3_client)\n",
      "  result = crawler.crawl()\n"
     ]
    }
   ],
   "source": [
    "# Once authenticated, you can use the session with the S3 bucket crawler\n",
    "# or any other AWS operations\n",
    "\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    print(\"âœ… Ready to use with S3 bucket crawler!\")\n",
    "    print(\"\\nYou can now:\")\n",
    "    print(\"1. Import the S3BucketCrawler class from the other notebook\")\n",
    "    print(\"2. Pass this s3_client to the crawler: S3BucketCrawler('bucket-name', s3_client)\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  crawler = S3BucketCrawler('nasa-disasters', s3_client)\")\n",
    "    print(\"  result = crawler.crawl()\")\n",
    "else:\n",
    "    print(\"âš ï¸ Please complete authentication setup first using one of the methods above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3BucketCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "class S3BucketCrawler:\n",
    "    \"\"\"\n",
    "    Crawls S3 bucket structure and creates a nested dictionary of .tif files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories = set()\n",
    "        \n",
    "    def build_nested_structure(self, file_list: list) -> Dict:\n",
    "        \"\"\"\n",
    "        Convert flat S3 paths to nested dictionary structure.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of dictionaries containing file information\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing directory structure\n",
    "        \"\"\"\n",
    "        root = {}\n",
    "        \n",
    "        for file_info in file_list:\n",
    "            path_parts = file_info['key'].split('/')\n",
    "            current_level = root\n",
    "            \n",
    "            # Navigate/create the directory structure\n",
    "            for i, part in enumerate(path_parts[:-1]):\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {\n",
    "                        '_type': 'directory',\n",
    "                        '_path': '/'.join(path_parts[:i+1]) + '/',\n",
    "                        '_files': [],\n",
    "                        '_subdirs': {}\n",
    "                    }\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "                else:\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "            \n",
    "            # Add the file to its directory\n",
    "            file_name = path_parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            \n",
    "            current_level['_files'].append({\n",
    "                'name': file_name,\n",
    "                'path': file_info['key'],\n",
    "                'size_bytes': file_info['size'],\n",
    "                'size_readable': self._format_size(file_info['size']),\n",
    "                'last_modified': file_info['last_modified'],\n",
    "                'storage_class': file_info.get('storage_class', 'STANDARD')\n",
    "            })\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def crawl(self, \n",
    "              prefix: str = '', \n",
    "              show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the bucket and return nested structure of .tif files only.\n",
    "        \n",
    "        Args:\n",
    "            prefix: Start from this prefix (subdirectory)\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing bucket structure with .tif files\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ” Starting crawl of s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"ğŸ“Œ Filtering for .tif files only\")\n",
    "        \n",
    "        # Reset statistics\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        all_tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning bucket...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        self.total_files += 1\n",
    "                        self.total_size += obj.get('Size', 0)\n",
    "                        \n",
    "                        # Track directories\n",
    "                        dir_path = '/'.join(key.split('/')[:-1])\n",
    "                        if dir_path:\n",
    "                            self.total_directories.add(dir_path)\n",
    "                        \n",
    "                        # Add file info\n",
    "                        all_tif_files.append({\n",
    "                            'key': key,\n",
    "                            'size': obj.get('Size', 0),\n",
    "                            'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None,\n",
    "                            'storage_class': obj.get('StorageClass', 'STANDARD')\n",
    "                        })\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\nâœ… Found {self.total_files} .tif files\")\n",
    "        print(f\"ğŸ“ Across {len(self.total_directories)} directories\")\n",
    "        print(f\"ğŸ’¾ Total size: {self._format_size(self.total_size)}\")\n",
    "        \n",
    "        # Build nested structure\n",
    "        structure = self.build_nested_structure(all_tif_files)\n",
    "        \n",
    "        # Create final result with metadata\n",
    "        result = {\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"prefix\": prefix,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"file_filter\": \".tif\",\n",
    "                \"total_files\": self.total_files,\n",
    "                \"total_directories\": len(self.total_directories),\n",
    "                \"total_size_bytes\": self.total_size,\n",
    "                \"total_size_readable\": self._format_size(self.total_size)\n",
    "            },\n",
    "            \"structure\": structure\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _format_size(self, size_bytes: int) -> str:\n",
    "        \"\"\"Format bytes to human-readable size.\"\"\"\n",
    "        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if size_bytes < 1024.0:\n",
    "                return f\"{size_bytes:.2f} {unit}\"\n",
    "            size_bytes /= 1024.0\n",
    "        return f\"{size_bytes:.2f} PB\"\n",
    "\n",
    "print(\"âœ… S3BucketCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3DisastersCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class S3DisastersCrawler:\n",
    "    \"\"\"\n",
    "    Simplified crawler for drcs_activations directory that creates clean nested structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.activation_events = set()\n",
    "        \n",
    "    def build_clean_structure(self, file_list: list, prefix: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Build a clean nested structure organized by activation events.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of S3 keys\n",
    "            prefix: The prefix to remove (e.g., 'drcs_activations/')\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary with activation events and their files\n",
    "        \"\"\"\n",
    "        structure = {}\n",
    "        \n",
    "        for key in file_list:\n",
    "            # Remove the prefix to get relative path\n",
    "            relative_path = key.replace(prefix, '', 1) if key.startswith(prefix) else key\n",
    "            parts = relative_path.split('/')\n",
    "            \n",
    "            # Skip if not enough parts\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            # First part is the activation event (e.g., '202301_Flood_CA')\n",
    "            activation_event = parts[0]\n",
    "            self.activation_events.add(activation_event)\n",
    "            \n",
    "            # Initialize activation event if not exists\n",
    "            if activation_event not in structure:\n",
    "                structure[activation_event] = {}\n",
    "            \n",
    "            # Build nested structure for remaining parts\n",
    "            current_level = structure[activation_event]\n",
    "            \n",
    "            # Navigate through subdirectories\n",
    "            for part in parts[1:-1]:\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {}\n",
    "                # Check if current_level[part] is a list (files), if so convert to dict\n",
    "                if isinstance(current_level[part], list):\n",
    "                    current_level[part] = {'_files': current_level[part]}\n",
    "                current_level = current_level[part]\n",
    "            \n",
    "            # Add the file\n",
    "            filename = parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            current_level['_files'].append(filename)\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def crawl_drcs_activations(self, show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl only the drcs_activations directory for .tif files.\n",
    "        \n",
    "        Args:\n",
    "            show_progress: Show progress during crawl\n",
    "            \n",
    "        Returns:\n",
    "            Clean nested dictionary of activation events and their .tif files\n",
    "        \"\"\"\n",
    "        prefix = 'drcs_activations/'\n",
    "        print(f\"ğŸ” Crawling s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"ğŸ“Œ Filtering for .tif files in activation events\")\n",
    "        \n",
    "        # Reset counters\n",
    "        self.total_files = 0\n",
    "        self.activation_events.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning activation events...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        tif_files.append(key)\n",
    "                        self.total_files += 1\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\nâœ… Found {self.total_files} .tif files\")\n",
    "        \n",
    "        # Build clean structure\n",
    "        structure = self.build_clean_structure(tif_files, prefix)\n",
    "        \n",
    "        print(f\"ğŸ“ Across {len(self.activation_events)} activation events\")\n",
    "        \n",
    "        # Create result with drcs_activations as root\n",
    "        result = {\n",
    "            \"drcs_activations\": structure,\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"total_tif_files\": self.total_files,\n",
    "                \"total_activation_events\": len(self.activation_events),\n",
    "                \"activation_events\": sorted(list(self.activation_events))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"âœ… S3DisastersCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 10. Helper Functions for Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_to_json(data: Dict, filename: str, indent: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Save dictionary to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary to save\n",
    "        filename: Output filename\n",
    "        indent: JSON indentation (None for compact)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    filepath = os.path.abspath(filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=indent, default=str)\n",
    "    \n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"âœ… Saved to: {filepath}\")\n",
    "    print(f\"ğŸ“Š File size: {file_size:,} bytes\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def print_structure_preview(structure: Dict, max_depth: int = 3, current_depth: int = 0, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Print a tree-like preview of the structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Nested dictionary structure\n",
    "        max_depth: Maximum depth to display\n",
    "        current_depth: Current recursion depth\n",
    "        prefix: Prefix for tree display\n",
    "    \"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    # Process directories\n",
    "    for key, value in structure.items():\n",
    "        if isinstance(value, dict) and '_type' in value and value['_type'] == 'directory':\n",
    "            print(f\"{prefix}ğŸ“ {key}/\")\n",
    "            \n",
    "            # Show files in this directory\n",
    "            if '_files' in value:\n",
    "                for file in value['_files'][:3]:  # Show first 3 files\n",
    "                    print(f\"{prefix}  ğŸ“„ {file['name']} ({file['size_readable']})\")\n",
    "                if len(value['_files']) > 3:\n",
    "                    print(f\"{prefix}  ... and {len(value['_files']) - 3} more .tif files\")\n",
    "            \n",
    "            # Recurse into subdirectories\n",
    "            if '_subdirs' in value and current_depth < max_depth:\n",
    "                print_structure_preview(value['_subdirs'], max_depth, current_depth + 1, prefix + \"  \")\n",
    "\n",
    "def get_all_file_paths(structure: Dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract all file paths from nested structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Nested dictionary structure\n",
    "        \n",
    "    Returns:\n",
    "        List of all file paths\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    def extract_recursive(obj):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        file_paths.append(file_info['path'])\n",
    "                if '_subdirs' in value:\n",
    "                    extract_recursive(value['_subdirs'])\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        extract_recursive(structure['structure'])\n",
    "    else:\n",
    "        extract_recursive(structure)\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "def get_statistics(structure: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics from crawled structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Crawled structure dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with statistics\n",
    "    \"\"\"\n",
    "    files_data = []\n",
    "    \n",
    "    def analyze_recursive(obj, current_dir=\"\"):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        files_data.append({\n",
    "                            'directory': current_dir if current_dir else '/',\n",
    "                            'filename': file_info['name'],\n",
    "                            'full_path': file_info['path'],\n",
    "                            'size_bytes': file_info['size_bytes'],\n",
    "                            'size_readable': file_info['size_readable'],\n",
    "                            'last_modified': file_info['last_modified']\n",
    "                        })\n",
    "                if '_subdirs' in value:\n",
    "                    new_dir = f\"{current_dir}/{key}\" if current_dir else key\n",
    "                    analyze_recursive(value['_subdirs'], new_dir)\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        analyze_recursive(structure['structure'])\n",
    "    else:\n",
    "        analyze_recursive(structure)\n",
    "    \n",
    "    df = pd.DataFrame(files_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(\"\\nğŸ“Š Statistics:\")\n",
    "        print(f\"Total .tif files: {len(df)}\")\n",
    "        print(f\"Total size: {df['size_bytes'].sum():,} bytes\")\n",
    "        print(f\"Unique directories: {df['directory'].nunique()}\")\n",
    "        \n",
    "        # Top directories by file count\n",
    "        print(\"\\nTop directories by file count:\")\n",
    "        top_dirs = df['directory'].value_counts().head(5)\n",
    "        for dir_name, count in top_dirs.items():\n",
    "            print(f\"  {dir_name}: {count} files\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"âœ… Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 11. Main Execution - Crawl S3 Buckets for .tif Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Bucket: s3://nasa-disasters/drcs_activations\n",
      "  File filter: .tif files only\n",
      "  Output file: s3_tif_files_structure.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = \"nasa-disasters\"  # Change this to your target bucket\n",
    "PREFIX = \"drcs_activations\"                      # Leave empty for entire bucket, or specify a path like \"drcs_activations/\"\n",
    "OUTPUT_FILE = \"s3_tif_files_structure.json\"  # Output filename\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/{PREFIX}\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting crawl... This may take a while for large buckets.\n",
      "\n",
      "ğŸ” Starting crawl of s3://nasa-disasters/drcs_activations\n",
      "ğŸ“Œ Filtering for .tif files only\n",
      "Scanning bucket...\n",
      "  Found 34400 .tif files...\n",
      "âœ… Found 34460 .tif files\n",
      "ğŸ“ Across 1518 directories\n",
      "ğŸ’¾ Total size: 5.53 TB\n",
      "\n",
      "âœ… Crawl completed in 47.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize crawler and run\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = S3BucketCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"ğŸš€ Starting crawl... This may take a while for large buckets.\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform the crawl\n",
    "    result = crawler.crawl(\n",
    "        prefix=PREFIX,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"âŒ Please complete authentication setup first using the cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 12. Preview and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ³ Directory Structure Preview (max depth 3):\n",
      "\n",
      "s3://nasa-disasters/\n",
      "ğŸ“ drcs_activations/\n",
      "  ğŸ“ 2020/\n",
      "    ğŸ“ aegean_sea_earthquake_202010/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ bolivia_fires/\n",
      "      ğŸ“ aster/\n",
      "    ğŸ“ california_fires/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ dnbr/\n",
      "      ğŸ“ master/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ uavsar/\n",
      "    ğŸ“ colorado_fires/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_delta/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ radarsat2/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_sally/\n",
      "      ğŸ“ dfo/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_zeta/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricanes_eta_iota/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ dartmouth_flood_observatory/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ mi_dam_failure_2020/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ neuse_river_nc/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ volcano_research/\n",
      "      ğŸ“ lewotolok_20201129/\n",
      "    ğŸ“ washington_oregon_fires/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ sentinel2/\n",
      "  ğŸ“ 2021/\n",
      "    ğŸ“ australia_floods/\n",
      "      ğŸ“ dartmouth_flood_observatory/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ california_fires/\n",
      "      ğŸ“ aster/\n",
      "    ğŸ“ costarica_panama_flooding/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ guyana_floods/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ haiti_earthquake_202108/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ landslide_inventory/\n",
      "      ğŸ“ perusat/\n",
      "      ğŸ“ satellogic/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_elsa/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_ida/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarblehd/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ lis/\n",
      "      ğŸ“ sar_moldanado-20210903T190339Z-001/\n",
      "      ğŸ“ satellogic/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ tropics/\n",
      "      ğŸ“ uavsar/\n",
      "    ğŸ“ la_palma_eruption/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ omps_viirs/\n",
      "    ğŸ“ la_soufriere_eruption/\n",
      "      ğŸ“ alos2/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ ecostress/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ modis_viirs/\n",
      "      ğŸ“ omps/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ nepal_landslides_flooding_202106/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ north_carolina_hurrex/\n",
      "      ğŸ“ dnb/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ north_carolina_wildfire/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ pacificNW_flooding/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ peru_earthquake/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ satellogic_test/\n",
      "      ğŸ“ 20210304_164047_SN10_L1_Fuego_28327/\n",
      "      ğŸ“ 20210306_163150_SN15_L1_Pacaya_28326/\n",
      "    ğŸ“ semeru_eruption/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ tornado_outbreak_20211210/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ ts_fred/\n",
      "      ğŸ“ sentinel2/\n",
      "  ğŸ“ 2022/\n",
      "    ğŸ“ australia_flooding_202203/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ bangladesh_flooding_202206/\n",
      "      ğŸ“ eos_rs/\n",
      "    ğŸ“ brazil_flooding_202205/\n",
      "      ğŸ“ fpm/\n",
      "    ğŸ“ fiona_msfc/\n",
      "      ğŸ“ 9_22/\n",
      "      ğŸ“ 9_23/\n",
      "    ğŸ“ hurricane_fiona_2022/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ files/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ hurricane_ian_2022/\n",
      "      ğŸ“ DFO/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ landsat9/\n",
      "      ğŸ“ maxar/\n",
      "      ğŸ“ misr/\n",
      "      ğŸ“ planet.Overviews/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ radarsat/\n",
      "      ğŸ“ radarsat2/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_nicole_2022/\n",
      "      ğŸ“ radarsat2WE/\n",
      "    ğŸ“ ky_flooding_202207/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ trueColor/\n",
      "    ğŸ“ mauna_loa_eruption_2022/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ eos_rs/\n",
      "      ğŸ“ omps/\n",
      "      ğŸ“ viirs/\n",
      "    ğŸ“ southafrica_flooding_202204/\n",
      "      ğŸ“ fpm/\n",
      "    ğŸ“ tonga_volcano_tsunami_202201/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ omps/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ viirs/\n",
      "    ğŸ“ yellowstone_np_flooding_202206/\n",
      "      ğŸ“ aria/\n",
      "  ğŸ“ 2023/\n",
      "    ğŸ“ california_atmospheric_river/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ greece_wildfires/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ hawaii_wildfires_202308/\n",
      "      ğŸ“ DPM1_ALOS2/\n",
      "      ğŸ“ ECOSTRESS/\n",
      "      ğŸ“ ISS_Imagery/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ eos_rs/\n",
      "      ğŸ“ hawaii_wildfires.Overviews/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ opera/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_hilary/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_idalia/\n",
      "      ğŸ“ aria_opera/\n",
      "      ğŸ“ aria_opera_dswx/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet.Overviews/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_otis/\n",
      "      ğŸ“ Landslides/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "    ğŸ“ morocco_earthquake_202309/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ eos_rs/\n",
      "    ğŸ“ nc_tornado_20230719/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ newengland_flooding_202312/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ newengland_flooding_202312.Overviews/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ s2_swir_vermont_flooding/\n",
      "    ğŸ“ turkiye_earthquakes_202302/\n",
      "      ğŸ“ NIST/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ dnb/\n",
      "      ğŸ“ eos_rs/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ landslides_turkiye_earthquake_2023/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ typhoon_mawar/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ vermont_flooding_202307/\n",
      "      ğŸ“ planet.Overviews/\n",
      "      ğŸ“ planet/\n",
      "  ğŸ“ 202301_Flood_CA/\n",
      "    ğŸ“ sentinel1/\n",
      "  ğŸ“ 202302_Earthquake_Turkiye/\n",
      "    ğŸ“ NIST/\n",
      "      ğŸ“ NIST_additional_sites/\n",
      "      ğŸ“ NIST_hospital_sites/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ rgb/\n",
      "    ğŸ“ dnb/\n",
      "    ğŸ“ eos_rs/\n",
      "    ğŸ“ landsat/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ landsat9/\n",
      "    ğŸ“ landslides_turkiye_earthquake_2023/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202305_Typhoon_Mawar/\n",
      "    ğŸ“ planet/\n",
      "  ğŸ“ 20230719_SevereWx_NC/\n",
      "    ğŸ“ aria/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ cir/\n",
      "      ğŸ“ tc/\n",
      "    ğŸ“ sentinel1/\n",
      "  ğŸ“ 202307_Fire_Greece/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ Aigio_wildfire_3band_20230727_psscene_visual/\n",
      "      ğŸ“ Aigio_wildfires_3band_20230721_psscene_visual/\n",
      "      ğŸ“ Aigio_wildfires_3band_20230724_psscene_visual/\n",
      "      ğŸ“ Corfu_wildfire_3band_20230727_psscene_visual/\n",
      "      ğŸ“ Corfu_wildfires_3band_20230722_psscene_visual/\n",
      "      ğŸ“ Corfu_wildfires_3band_20230724_psscene_visual/\n",
      "      ğŸ“ Corfu_wildfires_3band_20230725_psscene_visual/\n",
      "      ğŸ“ Epanochori_wildfires_3band_20230724_psscene_visual/\n",
      "      ğŸ“ Rhodes_wildfire_3band_20230728_psscene_visual/\n",
      "      ğŸ“ Rhodes_wildfires_3band_20230718_psscene_visual/\n",
      "      ğŸ“ Rhodes_wildfires_3band_20230724_psscene_visual/\n",
      "      ğŸ“ Rhodes_wildfires_3band_20230725_psscene_visual/\n",
      "      ğŸ“ SouthernEvia_wildfires_3band_20230719_psscene_visual/\n",
      "      ğŸ“ SouthernEvia_wildfires_3band_20230725_psscene_visual/\n",
      "      ğŸ“ SouthernEvia_wildfires_3band_20230728_psscene_visual/\n",
      "  ğŸ“ 202307_Flood_VT/\n",
      "    ğŸ“ planet.Overviews/\n",
      "      ğŸ“ planet_20230711.Overviews/\n",
      "      ğŸ“ planet_colorinfrared.Overviews/\n",
      "      ğŸ“ planet_colorinfrared_20230711.Overviews/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ 20230711/\n",
      "      ğŸ“ 20230712/\n",
      "  ğŸ“ 202308_Hurricane_Hilary/\n",
      "    ğŸ“ landsat/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ landsat9/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202309_Earthquake_Morocco/\n",
      "    ğŸ“ aria/\n",
      "    ğŸ“ eos_rs/\n",
      "  ğŸ“ 202309_Hurricane_Idalia/\n",
      "    ğŸ“ aria_opera/\n",
      "      ğŸ“ pre_event/\n",
      "    ğŸ“ aria_opera_dswx/\n",
      "    ğŸ“ blackmarble_hd/\n",
      "    ğŸ“ landsat/\n",
      "      ğŸ“ pre_event/\n",
      "    ğŸ“ planet.Overviews/\n",
      "      ğŸ“ planet_colorinfrared_postevent.Overviews/\n",
      "      ğŸ“ planet_truecolor_postevent.Overviews/\n",
      "      ğŸ“ preevent_colorinfrared.Overviews/\n",
      "      ğŸ“ truecolor_preevent.Overviews/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ post_event/\n",
      "      ğŸ“ pre_event/\n",
      "    ğŸ“ sentinel2/\n",
      "      ğŸ“ pre_event/\n",
      "  ğŸ“ 202310_Hurricane_Otis/\n",
      "    ğŸ“ Landslides/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ BMHD_Otis_Nov8/\n",
      "      ğŸ“ BMHD_Otis_Nov9_13/\n",
      "  ğŸ“ 202312_Flood_NewEngland/\n",
      "    ğŸ“ blackmarble/\n",
      "      ğŸ“ BMHD_StormMaine_Dec19_21/\n",
      "      ğŸ“ BMHD_StormMaine_Dec22_23/\n",
      "    ğŸ“ newengland_flooding_202312.Overviews/\n",
      "      ğŸ“ planet_cir_20231219.Overviews/\n",
      "      ğŸ“ planet_tc_20231219.Overviews/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ colorIR/\n",
      "      ğŸ“ true/\n",
      "    ğŸ“ sentinel2/\n",
      "      ğŸ“ waldoboro/\n",
      "  ğŸ“ 2024/\n",
      "    ğŸ“ bangladesh_flood_202408/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ projects/\n",
      "    ğŸ“ brasil_flood_202405/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ brasil_flood_202405_daily.Overviews/\n",
      "      ğŸ“ brasil_flood_202405_daily_overviews.Overviews/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ california_atmospheric_river_202402/\n",
      "      ğŸ“ aria_opera/\n",
      "    ğŸ“ california_earthquake_202412/\n",
      "      ğŸ“ opera/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ projects/\n",
      "    ğŸ“ chile_wildfires_202402/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ guatemala_wildfire_202402/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ hurricane_beryl_202407/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ tropomi/\n",
      "    ğŸ“ hurricane_helene_202409/\n",
      "      ğŸ“ NDVIchange/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ usda/\n",
      "    ğŸ“ hurricane_milton_202410/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ iowa_flood_202406/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ nepal_flood_202408/\n",
      "      ğŸ“ Sentinel1_ASF/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ palos_verdes_landslides_202410/\n",
      "      ğŸ“ opera/\n",
      "    ğŸ“ se_us_severestorms_202401/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ se_us_severestorms_202401.Overviews/\n",
      "    ğŸ“ texas_extremeheat_202405/\n",
      "      ğŸ“ ECOSTRESS/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ blackmarble_processed/\n",
      "    ğŸ“ texas_flood_202405/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ texas_flood_202405_project_PR/\n",
      "    ğŸ“ texas_wildfires_202402/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ projects/\n",
      "    ğŸ“ tropical_cyclone_debby_202408/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ landsat9/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ precipitation/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ tropical_storm_ernesto_202408/\n",
      "      ğŸ“ TROPICS/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ tropical_storm_francine_202409/\n",
      "      ğŸ“ black_marble/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ tropical_storm_sara_202411/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ projects/\n",
      "      ğŸ“ sentinel1/\n",
      "  ğŸ“ 202401_SevereWx_SoutheastUS/\n",
      "    ğŸ“ landsat/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ landsat9/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ colorIR/\n",
      "      ğŸ“ true/\n",
      "    ğŸ“ se_us_severestorms_202401.Overviews/\n",
      "      ğŸ“ colorinfrared.Overviews/\n",
      "      ğŸ“ planet_truecolor_20240110.Overviews/\n",
      "  ğŸ“ 202402_Fire_Chile/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ 20240118/\n",
      "      ğŸ“ 20240203/\n",
      "      ğŸ“ 20240205/\n",
      "      ğŸ“ 20240206/\n",
      "  ğŸ“ 202402_Fire_Guatemala/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ Feb16/\n",
      "      ğŸ“ Feb22/\n",
      "      ğŸ“ Feb24/\n",
      "      ğŸ“ Feb25/\n",
      "    ğŸ“ sentinel2/\n",
      "      ğŸ“ swir/\n",
      "      ğŸ“ true/\n",
      "  ğŸ“ 202402_Fire_TX/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ post/\n",
      "      ğŸ“ pre/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ planet/\n",
      "  ğŸ“ 202402_Flood_CA/\n",
      "    ğŸ“ aria_opera/\n",
      "  ğŸ“ 202405_Flood_Brasil/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ dswx_hls/\n",
      "      ğŸ“ flood_depth/\n",
      "    ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ BMHD_Brazil2024-20240508T185236Z-001/\n",
      "    ğŸ“ brasil_flood_202405_daily.Overviews/\n",
      "      ğŸ“ planet_0506.Overviews/\n",
      "    ğŸ“ brasil_flood_202405_daily_overviews.Overviews/\n",
      "      ğŸ“ planet_0420.Overviews/\n",
      "      ğŸ“ planet_0506.Overviews/\n",
      "      ğŸ“ planet_0507.Overviews/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ brasil_20240420_psscene_visual/\n",
      "      ğŸ“ brasil_20240506_psscene_visual/\n",
      "      ğŸ“ brasil_20240507_psscene_visual/\n",
      "    ğŸ“ sentinel1/\n",
      "      ğŸ“ rgb/\n",
      "      ğŸ“ water_extent/\n",
      "    ğŸ“ sentinel2/\n",
      "      ğŸ“ cir/\n",
      "      ğŸ“ swir/\n",
      "      ğŸ“ true/\n",
      "  ğŸ“ 202405_Flood_TX/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ new_water_extents/\n",
      "      ğŸ“ water_extents/\n",
      "    ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel2/\n",
      "      ğŸ“ cir/\n",
      "      ğŸ“ swir/\n",
      "      ğŸ“ true/\n",
      "    ğŸ“ texas_flood_202405_project_PR/\n",
      "      ğŸ“ Planet_Mosaics.Overviews/\n",
      "  ğŸ“ 202405_Heat_TX/\n",
      "    ğŸ“ ECOSTRESS/\n",
      "      ğŸ“ ControlData_May2023/\n",
      "    ğŸ“ blackmarble_hd/\n",
      "      ğŸ“ BlackMarbleHD_Houston2024/\n",
      "    ğŸ“ blackmarble_processed/\n",
      "  ğŸ“ 202406_Flood_IA/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ minnesota/\n",
      "      ğŸ“ planet_overviews.Overviews/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202407_Hurricane_Beryl/\n",
      "    ğŸ“ blackmarble/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ BlackMarble_PR/\n",
      "    ğŸ“ tropomi/\n",
      "      ğŸ“ carbon_monoxide/\n",
      "      ğŸ“ methane/\n",
      "  ğŸ“ 202408_Flood_Bangladesh/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ dswx_hls/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ PerformanceTesting_PR/\n",
      "      ğŸ“ bangladesh_aoi1_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi2_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi3_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi4_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi5_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi6_psscene_visual/\n",
      "      ğŸ“ bangladesh_aoi7_psscene_visual/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ Planet_PR/\n",
      "  ğŸ“ 202408_Flood_Nepal/\n",
      "    ğŸ“ Sentinel1_ASF/\n",
      "      ğŸ“ 10August2024/\n",
      "      ğŸ“ 22August2024/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ 15August2024/\n",
      "      ğŸ“ 17August2024/\n",
      "      ğŸ“ 26August2024/\n",
      "  ğŸ“ 202408_TropicalStorm_Debby/\n",
      "    ğŸ“ landsat8/\n",
      "    ğŸ“ landsat9/\n",
      "    ğŸ“ planet/\n",
      "    ğŸ“ precipitation/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ Planet_PR/\n",
      "      ğŸ“ project_RS/\n",
      "      ğŸ“ sentinel_PR/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202408_TropicalStorm_Ernesto/\n",
      "    ğŸ“ TROPICS/\n",
      "    ğŸ“ blackmarble/\n",
      "      ğŸ“ BMHD/\n",
      "      ğŸ“ brdf_corrected/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ Sentinel1_PR/\n",
      "      ğŸ“ blackmarble_TL_test/\n",
      "    ğŸ“ sentinel1/\n",
      "      ğŸ“ rgb/\n",
      "      ğŸ“ wm/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202409_Hurricane_Helene/\n",
      "    ğŸ“ NDVIchange/\n",
      "      ğŸ“ Oct2/\n",
      "      ğŸ“ Oct7/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ DIST/\n",
      "      ğŸ“ DSWx/\n",
      "      ğŸ“ RTC/\n",
      "    ğŸ“ blackmarble/\n",
      "      ğŸ“ blackmarble_hd/\n",
      "    ğŸ“ blackmarble_hd/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ NDVI_Change/\n",
      "      ğŸ“ PowerChange_PR/\n",
      "      ğŸ“ PowerChange_RS/\n",
      "      ğŸ“ blackmarble_RS/\n",
      "      ğŸ“ landsat_PR/\n",
      "      ğŸ“ landslides_RS/\n",
      "      ğŸ“ nrt_static_PR/\n",
      "      ğŸ“ sentinel_PR/\n",
      "    ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel2/\n",
      "    ğŸ“ usda/\n",
      "  ğŸ“ 202409_TropicalStorm_Francine/\n",
      "    ğŸ“ black_marble/\n",
      "      ğŸ“ BMHD/\n",
      "    ğŸ“ landsat/\n",
      "      ğŸ“ corrected_20240909/\n",
      "    ğŸ“ planet/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ planet_PR/\n",
      "      ğŸ“ s2_landsat_PR/\n",
      "    ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202410_Hurricane_Milton/\n",
      "    ğŸ“ NDVIchange/\n",
      "    ğŸ“ blackmarble/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ opera/\n",
      "      ğŸ“ dist/\n",
      "      ğŸ“ dswx/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ PowerChange_PR/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ sentinel/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel1/\n",
      "    ğŸ“ sentinel2/\n",
      "    ğŸ“ uavsar/\n",
      "      ğŸ“ 20241011/\n",
      "      ğŸ“ 20241012/\n",
      "      ğŸ“ 20241013/\n",
      "      ğŸ“ 20241014/\n",
      "      ğŸ“ post_deployment/\n",
      "  ğŸ“ 202410_Landslide_PalosVerdes/\n",
      "    ğŸ“ opera/\n",
      "      ğŸ“ coherence/\n",
      "      ğŸ“ displacement/\n",
      "      ğŸ“ uavsar/\n",
      "  ğŸ“ 202411_TropicalStorm_Sara/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ Nov20_cr_psscene_visual/\n",
      "      ğŸ“ Nov21_cr_psscene_visual/\n",
      "      ğŸ“ Nov24_cr_psscene_visual/\n",
      "      ğŸ“ Nov25_cr_psscene_visual/\n",
      "      ğŸ“ Nov26_cr_psscene_visual/\n",
      "      ğŸ“ Nov27_cr_psscene_visual/\n",
      "      ğŸ“ Nov28_cr_psscene_visual/\n",
      "      ğŸ“ oct5_cr_psscene_visual/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ pr/\n",
      "    ğŸ“ sentinel1/\n",
      "  ğŸ“ 202412_Earthquake_CA/\n",
      "    ğŸ“ opera/\n",
      "      ğŸ“ displacement/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ Dec3_CA_psscene_visual/\n",
      "      ğŸ“ Dec5_CA_psscene_visual/\n",
      "      ğŸ“ nov29_CA_psscene_visual/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ planet/\n",
      "  ğŸ“ 202501_Fire_CA/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ asf/\n",
      "      ğŸ“ dist/\n",
      "    ğŸ“ aviris-3/\n",
      "      ğŸ“ charash/\n",
      "      ğŸ“ dnbr/\n",
      "      ğŸ“ earlylook/\n",
      "      ğŸ“ pca/\n",
      "    ğŸ“ burn_severity/\n",
      "    ğŸ“ ecostress/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ maxar_chng/\n",
      "    ğŸ“ planet/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ Planet_PR/\n",
      "    ğŸ“ s1_2_fire_severity/\n",
      "    ğŸ“ s1_changedetection_asf/\n",
      "    ğŸ“ s1_dmgassessment/\n",
      "      ğŸ“ Jan16_Delivery/\n",
      "    ğŸ“ s2_dnbr/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202502_Flood_OhioValley/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ landslides_RV/\n",
      "    ğŸ“ sentinel2/\n",
      "  ğŸ“ 202503_SevereWx_US/\n",
      "    ğŸ“ planet/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ planet_SGF/\n",
      "      ğŸ“ planet_lix/\n",
      "      ğŸ“ planet_lsx/\n",
      "      ğŸ“ planet_lzk/\n",
      "      ğŸ“ planet_meg/\n",
      "      ğŸ“ planet_mob/\n",
      "      ğŸ“ planet_shv/\n",
      "    ğŸ“ sentinel-2/\n",
      "  ğŸ“ 202504_SevereWx_US/\n",
      "    ğŸ“ landsat/\n",
      "    ğŸ“ planet/\n",
      "      ğŸ“ 042025_SevWxFloods_AOI1/\n",
      "      ğŸ“ 042025_SevWxFloods_AOI2/\n",
      "      ğŸ“ 042025_SevWxFloods_AOI3/\n",
      "      ğŸ“ 042025_SevWxFloods_AOI4/\n",
      "      ğŸ“ Cincy-PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Cincy_PostEvent2_1_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Cincy_PostEvent2_2_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Cincy_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Lexington_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Lexington_PreEvent_1_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Lexington_PreEvent_2_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ LouisvilleKY_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ LouisvilleKY_PreEvent_1_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ LouisvilleKY_PreEvent_2_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ LouisvilleKY_PreEvent_3_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Salyersville_PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Saylersville_and_Vanceburg_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      ğŸ“ Vanceburg_PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "    ğŸ“ projects/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel2_jan/\n",
      "      ğŸ“ sentinel2_lzk/\n",
      "      ğŸ“ sentinel2_meg/\n",
      "      ğŸ“ sentinel2_ohx/\n",
      "      ğŸ“ sentinel2_pah/\n",
      "      ğŸ“ sentinel2_shv/\n",
      "    ğŸ“ sentinel2/\n",
      "    ğŸ“ viirs_flood/\n",
      "  ğŸ“ 202506_Fire_NM/\n",
      "    ğŸ“ aria/\n",
      "      ğŸ“ dist_hls/\n",
      "      ğŸ“ dswx_hls/\n",
      "    ğŸ“ planet/\n",
      "    ğŸ“ sentinel/\n",
      "    ğŸ“ sentinel1_flood/\n",
      "  ğŸ“ AdHoc_Disaster_products/\n",
      "    ğŸ“ Alaska_Quake_201811/\n",
      "    ğŸ“ Brumadinho_Brazil_Dam_Collapse_2019/\n",
      "      ğŸ“ ISS/\n",
      "    ğŸ“ CA_Fires_082018/\n",
      "      ğŸ“ CA_Fires_2018.Overviews/\n",
      "      ğŸ“ ER-2/\n",
      "      ğŸ“ ISS/\n",
      "      ğŸ“ Pleiades/\n",
      "    ğŸ“ CA_Fires_112018/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ gsfc_sar/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ recover/\n",
      "      ğŸ“ sentinel/\n",
      "      ğŸ“ uavsar/\n",
      "      ğŸ“ wrf-sfire/\n",
      "    ğŸ“ Fuego_Volcano_18/\n",
      "      ğŸ“ ARIA/\n",
      "      ğŸ“ Digital_Globe/\n",
      "      ğŸ“ Landsat/\n",
      "      ğŸ“ Ortho/\n",
      "      ğŸ“ Sentinel_1/\n",
      "      ğŸ“ Sentinel_2/\n",
      "    ğŸ“ Gita_18/\n",
      "    ğŸ“ Hokkaido_Earthquake_2018/\n",
      "    ğŸ“ Huricane Harvey 17/\n",
      "    ğŸ“ Hurricane_Florence_2018/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ dnb/\n",
      "      ğŸ“ gfms/\n",
      "      ğŸ“ hurricane_florence.Overviews/\n",
      "      ğŸ“ landsat7/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ nwc/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ uavsar/\n",
      "    ğŸ“ Hurricane_Lane_2018/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ Hurricane_Maria_17/\n",
      "      ğŸ“ blackmarble/\n",
      "    ğŸ“ Hurricane_Michael_2018/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ black_marble/\n",
      "      ğŸ“ dnb_sport/\n",
      "      ğŸ“ gfms/\n",
      "      ğŸ“ hurricane_michael.Overviews/\n",
      "      ğŸ“ iss/\n",
      "      ğŸ“ landsat7/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ radarsat2/\n",
      "      ğŸ“ sentinel_1/\n",
      "    ğŸ“ Hurricane_Willa_2018/\n",
      "      ğŸ“ dfo/\n",
      "      ğŸ“ gfms/\n",
      "      ğŸ“ hurricane_willa.Overviews/\n",
      "    ğŸ“ Kalaheo_Hi_Flood_2018/\n",
      "    ğŸ“ Kilauea_Volcano_2018/\n",
      "      ğŸ“ Aster/\n",
      "      ğŸ“ Interferogram/\n",
      "      ğŸ“ Landsat/\n",
      "      ğŸ“ MODIS/\n",
      "      ğŸ“ OMPS/\n",
      "      ğŸ“ SAR_RGB/\n",
      "      ğŸ“ VIIRS/\n",
      "    ğŸ“ LaosDamFailure/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel_alos/\n",
      "    ğŸ“ Lombok_2018/\n",
      "    ğŸ“ MexicoCity Earthquake 17/\n",
      "    ğŸ“ MidWest flooding 18/\n",
      "      ğŸ“ 20180218/\n",
      "      ğŸ“ 20180223/\n",
      "      ğŸ“ 20180225/\n",
      "      ğŸ“ 20180228/\n",
      "      ğŸ“ 20180302/\n",
      "    ğŸ“ Missouri Flood_17/\n",
      "    ğŸ“ NLE2018/\n",
      "      ğŸ“ dnb/\n",
      "    ğŸ“ Oaxaca_Mexico_18/\n",
      "    ğŸ“ PA_Flood_201808/\n",
      "    ğŸ“ Peru Flooding 17/\n",
      "    ğŸ“ SoCal_Fire_17/\n",
      "      ğŸ“ ARIA_Damage_Proxy_Map_v0.6_geotiff/\n",
      "    ğŸ“ Sulawesi_Quake_Tsunami_2018/\n",
      "      ğŸ“ landsat/\n",
      "    ğŸ“ Super_Typhoon_Yutu_2018/\n",
      "    ğŸ“ ak_fires_2019/\n",
      "      ğŸ“ AI_GeoTIFF_images/\n",
      "      ğŸ“ ASF_SAR/\n",
      "      ğŸ“ ak_fires_2019.Overviews/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ australian_fires_2019_2020/\n",
      "      ğŸ“ iss/\n",
      "      ğŸ“ misr/\n",
      "    ğŸ“ bolivia_landslide_2019/\n",
      "    ğŸ“ ca_earthquake_201907/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ uas/\n",
      "    ğŸ“ ca_fires_201910/\n",
      "    ğŸ“ ca_fires_202008/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ imagecat/\n",
      "      ğŸ“ misr/\n",
      "      ğŸ“ uavsar/\n",
      "    ğŸ“ ca_flooding_201902/\n",
      "      ğŸ“ floodmap/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ centralus_storms_052019/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ modis/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ covid19/\n",
      "      ğŸ“ tropomi/\n",
      "    ğŸ“ croatia_earthquake_202003/\n",
      "    ğŸ“ cyclone_amphan_2020/\n",
      "      ğŸ“ cyclone_amphan_2020.Overviews/\n",
      "    ğŸ“ cyclone_fani_2019/\n",
      "      ğŸ“ blackmarblehd/\n",
      "    ğŸ“ cyclone_harold_2020/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ iss/\n",
      "    ğŸ“ cyclone_idai_2019/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ blackmarble/\n",
      "      ğŸ“ blackmarblehd/\n",
      "      ğŸ“ cyclone_idai_2019.Overviews/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ smap/\n",
      "    ğŸ“ cyclone_kennneth_2019/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ cyclone_kenneth_2019.Overviews/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ hurricane_dorian_2019/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ goes/\n",
      "      ğŸ“ gpm/\n",
      "      ğŸ“ hurricane_dorian_2019.Overviews/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "      ğŸ“ sport/\n",
      "      ğŸ“ viirs/\n",
      "    ğŸ“ hurricane_douglas_202008/\n",
      "      ğŸ“ misr/\n",
      "    ğŸ“ hurricane_isaias_2020/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ hurricane_laura_2020/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ planet/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ india_floods_201909/\n",
      "    ğŸ“ indonesia_flooding_202001/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ japan_flooding_202007/\n",
      "    ğŸ“ krakatau_volcano_202004/\n",
      "      ğŸ“ VIIRSTIR Map of Krakatau SO2 Plume20200421103750/\n",
      "    ğŸ“ lebanon_explosion_202008/\n",
      "      ğŸ“ iss/\n",
      "      ğŸ“ planet/\n",
      "    ğŸ“ mexico_earthquake_202006/\n",
      "      ğŸ“ gsfc/\n",
      "    ğŸ“ mi_dam_failure_2020/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ midwest_flooding_201903/\n",
      "      ğŸ“ landsat/\n",
      "      ğŸ“ modis/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ nashville_tornado_202003/\n",
      "    ğŸ“ nepal_15/\n",
      "      ğŸ“ 102001003AB04100/\n",
      "      ğŸ“ 102001003C9F6700/\n",
      "      ğŸ“ DNB/\n",
      "      ğŸ“ SEDAC/\n",
      "      ğŸ“ aster/\n",
      "      ğŸ“ eo1/\n",
      "      ğŸ“ landsat8/\n",
      "    ğŸ“ nishinoshima_volcano_20200624/\n",
      "    ğŸ“ puerto_rico_earthquake_202001/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ viirs/\n",
      "    ğŸ“ quebec_flooding_2019/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ raikoke_volcano_2019/\n",
      "      ğŸ“ airs/\n",
      "      ğŸ“ krotkov_ams/\n",
      "      ğŸ“ omps/\n",
      "    ğŸ“ se_severe_weather_spring_2020/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ test_landsat8_ESRItesting/\n",
      "      ğŸ“ LC08_L1TP_021035_20160306_20170224_01_T1/\n",
      "      ğŸ“ LC08_L1TP_021035_20160322_20170224_01_T1/\n",
      "      ğŸ“ LC08_L1TP_021036_20160306_20170224_01_T1/\n",
      "      ğŸ“ LC08_L1TP_021036_20160322_20170224_01_T1/\n",
      "      ğŸ“ LC08_L1TP_022035_20160329_20170223_01_T1/\n",
      "      ğŸ“ SNPP_VIIRS/\n",
      "    ğŸ“ tropicalstorm_barry_2019/\n",
      "      ğŸ“ aria_alos2/\n",
      "      ğŸ“ hurricane_barry.Overviews/\n",
      "      ğŸ“ landsat8/\n",
      "      ğŸ“ lis/\n",
      "      ğŸ“ modis/\n",
      "      ğŸ“ sentinel1/\n",
      "      ğŸ“ sentinel2/\n",
      "    ğŸ“ tropicalstorm_christobal_2020/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ tropicalstorm_karen_2019/\n",
      "      ğŸ“ viirs_dnb/\n",
      "    ğŸ“ typhoon_hagibis_2019/\n",
      "    ğŸ“ typhoon_kammuri_2019/\n",
      "      ğŸ“ aria/\n",
      "    ğŸ“ ulawun_volcano_2019/\n",
      "      ğŸ“ omps/\n",
      "    ğŸ“ uruguay_floods_201906/\n",
      "      ğŸ“ alos2/\n",
      "      ğŸ“ modis/\n",
      "      ğŸ“ sentinel1/\n",
      "    ğŸ“ utah_earthquake_202003/\n",
      "      ğŸ“ aria/\n",
      "      ğŸ“ sarviews/\n",
      "    ğŸ“ volcano_research/\n",
      "      ğŸ“ lewotolok_20201129/\n"
     ]
    }
   ],
   "source": [
    "# Display structure preview\n",
    "if 'result' in locals():\n",
    "    print(\"ğŸŒ³ Directory Structure Preview (max depth 3):\\n\")\n",
    "    print(f\"s3://{BUCKET_NAME}/\")\n",
    "    print_structure_preview(result['structure'], max_depth=3)\n",
    "else:\n",
    "    print(\"No results to display. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure.json\n",
      "ğŸ“Š File size: 18,984,715 bytes\n",
      "\n",
      "ğŸ“ Structure saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure.json\n",
      "âœ… Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure_compact.json\n",
      "ğŸ“Š File size: 11,176,941 bytes\n",
      "ğŸ“ Compact version saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure_compact.json\n"
     ]
    }
   ],
   "source": [
    "# Save the complete structure to JSON\n",
    "if 'result' in locals():\n",
    "    filepath = save_to_json(result, OUTPUT_FILE)\n",
    "    print(f\"\\nğŸ“ Structure saved to: {filepath}\")\n",
    "    \n",
    "    # Option to save compact version (no indentation, smaller file)\n",
    "    compact_file = OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(result, compact_file, indent=None)\n",
    "    print(f\"ğŸ“ Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## 13. Generate Statistics and Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display statistics\n",
    "if 'result' in locals():\n",
    "    df_stats = get_statistics(result)\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    if not df_stats.empty:\n",
    "        stats_file = OUTPUT_FILE.replace('.json', '_statistics.csv')\n",
    "        df_stats.to_csv(stats_file, index=False)\n",
    "        print(f\"\\nğŸ“Š Statistics saved to: {stats_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample of .tif files:\")\n",
    "        display(df_stats.head(10))\n",
    "else:\n",
    "    print(\"No results to analyze. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total .tif files in bucket: 0\n",
      "\n",
      "First 10 file paths:\n",
      "\n",
      "ğŸ“„ File list saved to: s3_tif_files_structure_file_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Export flat list of all .tif file paths\n",
    "if 'result' in locals():\n",
    "    all_files = get_all_file_paths(result)\n",
    "    print(f\"\\nTotal .tif files in bucket: {len(all_files)}\")\n",
    "    print(\"\\nFirst 10 file paths:\")\n",
    "    for path in all_files[:10]:\n",
    "        print(f\"  s3://{BUCKET_NAME}/{path}\")\n",
    "    \n",
    "    # Save file list\n",
    "    file_list_path = OUTPUT_FILE.replace('.json', '_file_list.txt')\n",
    "    with open(file_list_path, 'w') as f:\n",
    "        for path in all_files:\n",
    "            f.write(f\"s3://{BUCKET_NAME}/{path}\\n\")\n",
    "    print(f\"\\nğŸ“„ File list saved to: {file_list_path}\")\n",
    "else:\n",
    "    print(\"No results to export. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## 14. Load and Search Previously Saved Results\n",
    "\n",
    "Use these functions to load and search through previously crawled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_structure(filename: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Load a previously saved JSON structure.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Loaded structure from: {filename}\")\n",
    "    \n",
    "    if '_metadata' in data:\n",
    "        meta = data['_metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Bucket: {meta.get('bucket')}\")\n",
    "        print(f\"  Crawled at: {meta.get('crawled_at')}\")\n",
    "        print(f\"  Total .tif files: {meta.get('total_files')}\")\n",
    "        print(f\"  Total directories: {meta.get('total_directories')}\")\n",
    "        print(f\"  Total size: {meta.get('total_size_readable')}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def search_files(structure: Dict, pattern: str) -> list:\n",
    "    \"\"\"\n",
    "    Search for files matching a pattern in the structure.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    def search_recursive(obj):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        if pattern.lower() in file_info['name'].lower():\n",
    "                            matches.append(file_info)\n",
    "                if '_subdirs' in value:\n",
    "                    search_recursive(value['_subdirs'])\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        search_recursive(structure['structure'])\n",
    "    else:\n",
    "        search_recursive(structure)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example: Load saved file\n",
    "# loaded_data = load_json_structure(OUTPUT_FILE)\n",
    "\n",
    "# Example: Search for specific files\n",
    "# matching_files = search_files(loaded_data, \"flood\")\n",
    "# print(f\"Found {len(matching_files)} files containing 'flood'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## 15. Crawl DRCS Activations Directory Only\n",
    "\n",
    "Focused crawler for disaster activation events with simplified output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Bucket: s3://nasa-disasters/drcs_activations/\n",
      "  File filter: .tif files only\n",
      "  Output file: drcs_activations_tif_files.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration for DRCS Activations\n",
    "BUCKET_NAME = \"nasa-disasters\"\n",
    "OUTPUT_FILE = \"drcs_activations_tif_files.json\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/drcs_activations/\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting focused crawl of drcs_activations...\n",
      "\n",
      "ğŸ” Crawling s3://nasa-disasters/drcs_activations/\n",
      "ğŸ“Œ Filtering for .tif files in activation events\n",
      "Scanning activation events...\n",
      "  Found 34400 .tif files...\n",
      "âœ… Found 34460 .tif files\n",
      "ğŸ“ Across 42 activation events\n",
      "\n",
      "âœ… Crawl completed in 52.65 seconds\n",
      "\n",
      "Sample activation events:\n",
      "  - 2020\n",
      "  - 2021\n",
      "  - 2022\n",
      "  - 2023\n",
      "  - 202301_Flood_CA\n",
      "  - 202302_Earthquake_Turkiye\n",
      "  - 202305_Typhoon_Mawar\n",
      "  - 20230719_SevereWx_NC\n",
      "  - 202307_Fire_Greece\n",
      "  - 202307_Flood_VT\n",
      "  ... and 32 more\n"
     ]
    }
   ],
   "source": [
    "# Run the focused crawler\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    # Initialize the disasters crawler\n",
    "    disasters_crawler = S3DisastersCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl drcs_activations\n",
    "    print(\"ğŸš€ Starting focused crawl of drcs_activations...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform the crawl\n",
    "    drcs_result = disasters_crawler.crawl_drcs_activations(show_progress=True)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Show sample activation events\n",
    "    if '_metadata' in drcs_result:\n",
    "        events = drcs_result['_metadata']['activation_events'][:10]\n",
    "        print(f\"\\nSample activation events:\")\n",
    "        for event in events:\n",
    "            print(f\"  - {event}\")\n",
    "        if len(drcs_result['_metadata']['activation_events']) > 10:\n",
    "            print(f\"  ... and {len(drcs_result['_metadata']['activation_events']) - 10} more\")\n",
    "else:\n",
    "    print(\"âŒ Please complete authentication setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Structure Preview:\n",
      "\n",
      "ğŸ“ 2020/\n",
      "  â””â”€â”€ aegean_sea_earthquake_202010/ (2 .tif files in subdirs)\n",
      "  â””â”€â”€ bolivia_fires/ (1 .tif files in subdirs)\n",
      "  â””â”€â”€ california_fires/ (43 .tif files in subdirs)\n",
      "  â””â”€â”€ colorado_fires/ (28 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_delta/ (574 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_sally/ (8 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_zeta/ (8 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricanes_eta_iota/ (535 .tif files in subdirs)\n",
      "  â””â”€â”€ mi_dam_failure_2020/ (22 .tif files in subdirs)\n",
      "  â””â”€â”€ neuse_river_nc/ (12 .tif files)\n",
      "  â””â”€â”€ volcano_research/ (0 .tif files in subdirs)\n",
      "  â””â”€â”€ washington_oregon_fires/ (1 .tif files in subdirs)\n",
      "\n",
      "ğŸ“ 2021/\n",
      "  â””â”€â”€ australia_floods/ (11 .tif files in subdirs)\n",
      "  â””â”€â”€ california_fires/ (1 .tif files in subdirs)\n",
      "  â””â”€â”€ costarica_panama_flooding/ (6 .tif files in subdirs)\n",
      "  â””â”€â”€ guyana_floods/ (26 .tif files in subdirs)\n",
      "  â””â”€â”€ haiti_earthquake_202108/ (6 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_elsa/ (174 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_ida/ (380 .tif files in subdirs)\n",
      "  â””â”€â”€ la_palma_eruption/ (2 .tif files in subdirs)\n",
      "  â””â”€â”€ la_soufriere_eruption/ (16 .tif files in subdirs)\n",
      "  â””â”€â”€ nepal_landslides_flooding_202106/ (1 .tif files in subdirs)\n",
      "  â””â”€â”€ north_carolina_hurrex/ (31 .tif files in subdirs)\n",
      "  â””â”€â”€ north_carolina_wildfire/ (3 .tif files in subdirs)\n",
      "  â””â”€â”€ pacificNW_flooding/ (81 .tif files in subdirs)\n",
      "  â””â”€â”€ peru_earthquake/ (1 .tif files in subdirs)\n",
      "  â””â”€â”€ satellogic_test/ (5 .tif files in subdirs)\n",
      "  â””â”€â”€ semeru_eruption/ (3 .tif files in subdirs)\n",
      "  â””â”€â”€ tornado_outbreak_20211210/ (40 .tif files in subdirs)\n",
      "  â””â”€â”€ ts_fred/ (3 .tif files in subdirs)\n",
      "\n",
      "ğŸ“ 2022/\n",
      "  â””â”€â”€ australia_flooding_202203/ (8 .tif files)\n",
      "  â””â”€â”€ bangladesh_flooding_202206/ (2 .tif files in subdirs)\n",
      "  â””â”€â”€ brazil_flooding_202205/ (3 .tif files in subdirs)\n",
      "  â””â”€â”€ fiona_msfc/ (2 .tif files)\n",
      "  â””â”€â”€ hurricane_fiona_2022/ (65 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_ian_2022/ (152 .tif files in subdirs)\n",
      "  â””â”€â”€ hurricane_nicole_2022/ (4 .tif files in subdirs)\n",
      "  â””â”€â”€ ky_flooding_202207/ (23 .tif files in subdirs)\n",
      "  â””â”€â”€ mauna_loa_eruption_2022/ (4 .tif files in subdirs)\n",
      "  â””â”€â”€ southafrica_flooding_202204/ (1 .tif files in subdirs)\n",
      "  â””â”€â”€ tonga_volcano_tsunami_202201/ (117 .tif files in subdirs)\n",
      "  â””â”€â”€ yellowstone_np_flooding_202206/ (1 .tif files in subdirs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the structure\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"ğŸ“Š Structure Preview:\\n\")\n",
    "    \n",
    "    # Show first 3 activation events and their structure\n",
    "    events = list(drcs_result['drcs_activations'].keys())[:3]\n",
    "    \n",
    "    for event in events:\n",
    "        print(f\"ğŸ“ {event}/\")\n",
    "        event_data = drcs_result['drcs_activations'][event]\n",
    "        \n",
    "        # Show subdirectories and file counts\n",
    "        for subdir, content in event_data.items():\n",
    "            if isinstance(content, dict):\n",
    "                if '_files' in content:\n",
    "                    print(f\"  â””â”€â”€ {subdir}/ ({len(content['_files'])} .tif files)\")\n",
    "                else:\n",
    "                    # Count nested files\n",
    "                    total = sum(len(v['_files']) if isinstance(v, dict) and '_files' in v else 0 \n",
    "                               for v in content.values())\n",
    "                    print(f\"  â””â”€â”€ {subdir}/ ({total} .tif files in subdirs)\")\n",
    "            elif isinstance(content, list):\n",
    "                print(f\"  â””â”€â”€ {len(content)} .tif files directly in {event}/\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results to preview. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files.json\n",
      "ğŸ“Š File size: 2,256,239 bytes\n",
      "\n",
      "ğŸ“ Simplified structure saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files.json\n",
      "âœ… Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files_compact.json\n",
      "ğŸ“Š File size: 1,701,317 bytes\n",
      "ğŸ“ Compact version saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files_compact.json\n"
     ]
    }
   ],
   "source": [
    "# Save the simplified structure\n",
    "if 'drcs_result' in locals():\n",
    "    filepath = save_to_json(drcs_result, OUTPUT_FILE)\n",
    "    print(f\"\\nğŸ“ Simplified structure saved to: {filepath}\")\n",
    "    \n",
    "    # Also save a compact version\n",
    "    compact_file = OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(drcs_result, compact_file, indent=None)\n",
    "    print(f\"ğŸ“ Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Access Examples:\n",
      "\n",
      "Total activation events: 42\n",
      "\n",
      "Accessing files for '2020':\n",
      "  Total .tif files: 4384\n",
      "\n",
      "  First 5 file paths:\n",
      "    s3://nasa-disasters/drcs_activations/2020/aegean_sea_earthquake_202010/aria/ARIA_DPM_Sentinel-1_v0.3.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/aegean_sea_earthquake_202010/aria/S1_A131_20201030_20201024_disp_cm.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/bolivia_fires/aster/santacruz-nite-tif.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/california_fires/aria/ARIA_S1_DPM_CreekFire_Sep_13_Sep_19_7am.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/california_fires/aria/ARIA_S1_DPM_CreekFire_Sep_7_7am.tif\n"
     ]
    }
   ],
   "source": [
    "# Access example - show how to work with the data\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"ğŸ” Access Examples:\\n\")\n",
    "    \n",
    "    # Get all activation events\n",
    "    all_events = list(drcs_result['drcs_activations'].keys())\n",
    "    print(f\"Total activation events: {len(all_events)}\")\n",
    "    \n",
    "    # Access files for a specific event\n",
    "    if all_events:\n",
    "        sample_event = all_events[0]\n",
    "        print(f\"\\nAccessing files for '{sample_event}':\")\n",
    "        \n",
    "        event_data = drcs_result['drcs_activations'][sample_event]\n",
    "        \n",
    "        # Count total files in this event\n",
    "        def count_files(obj):\n",
    "            total = 0\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    total += len(obj['_files'])\n",
    "                for value in obj.values():\n",
    "                    if isinstance(value, dict):\n",
    "                        total += count_files(value)\n",
    "            return total\n",
    "        \n",
    "        total_files = count_files(event_data)\n",
    "        print(f\"  Total .tif files: {total_files}\")\n",
    "        \n",
    "        # Show how to iterate through all files\n",
    "        def get_all_files(obj, prefix=''):\n",
    "            files = []\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    for f in obj['_files']:\n",
    "                        files.append(prefix + f)\n",
    "                for key, value in obj.items():\n",
    "                    if key != '_files' and isinstance(value, dict):\n",
    "                        files.extend(get_all_files(value, prefix + key + '/'))\n",
    "            return files\n",
    "        \n",
    "        all_files = get_all_files(event_data, f'{sample_event}/')\n",
    "        print(f\"\\n  First 5 file paths:\")\n",
    "        for f in all_files[:5]:\n",
    "            print(f\"    s3://nasa-disasters/drcs_activations/{f}\")\n",
    "else:\n",
    "    print(\"No results available. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Split DRCS Data by Year\n",
    "\n",
    "Automatically split the crawled DRCS activation data into separate files for each year (2020-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Year-splitting functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def extract_year_from_event_name(event_name):\n",
    "    \"\"\"Extract year from event name.\"\"\"\n",
    "    # Check if it's a plain year (e.g., \"2020\")\n",
    "    if event_name in [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]:\n",
    "        return int(event_name)\n",
    "    \n",
    "    # Check if it starts with a year (e.g., \"202301_Flood_CA\" or \"20230719_SevereWx_NC\")\n",
    "    if event_name[:4].isdigit():\n",
    "        year = int(event_name[:4])\n",
    "        if 2020 <= year <= 2025:\n",
    "            return year\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_files_recursive(data):\n",
    "    \"\"\"Count total .tif files in a nested structure.\"\"\"\n",
    "    total = 0\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        if '_files' in data:\n",
    "            total += len(data['_files'])\n",
    "        for key, value in data.items():\n",
    "            if key != '_files' and isinstance(value, dict):\n",
    "                total += count_files_recursive(value)\n",
    "    \n",
    "    return total\n",
    "\n",
    "def split_drcs_by_year(drcs_result):\n",
    "    \"\"\"\n",
    "    Split DRCS activation data into separate files by year.\n",
    "    \n",
    "    Args:\n",
    "        drcs_result: The crawled DRCS data dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping years to their data\n",
    "    \"\"\"\n",
    "    # Initialize year-based dictionaries\n",
    "    years_data = {year: {} for year in range(2020, 2026)}\n",
    "    \n",
    "    # Process each activation event\n",
    "    drcs_data = drcs_result.get('drcs_activations', {})\n",
    "    \n",
    "    for event_name, event_data in drcs_data.items():\n",
    "        year = extract_year_from_event_name(event_name)\n",
    "        \n",
    "        if year and year in years_data:\n",
    "            # If it's a plain year folder (e.g., \"2020\"), merge its contents\n",
    "            if event_name == str(year):\n",
    "                # This is a year folder, add all its contents\n",
    "                for sub_event, sub_data in event_data.items():\n",
    "                    years_data[year][sub_event] = sub_data\n",
    "            else:\n",
    "                # This is a named event (e.g., \"202301_Flood_CA\")\n",
    "                years_data[year][event_name] = event_data\n",
    "    \n",
    "    return years_data\n",
    "\n",
    "print(\"âœ… Year-splitting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“… Splitting DRCS data by year (2020-2025)...\n",
      "\n",
      "âœ… Created drcs_activations_2020.json\n",
      "   - Events: 12\n",
      "   - Total .tif files: 4384\n",
      "   - Sample events: ['aegean_sea_earthquake_202010', 'bolivia_fires', 'california_fires', '... and 9 more']\n",
      "\n",
      "âœ… Created drcs_activations_2021.json\n",
      "   - Events: 18\n",
      "   - Total .tif files: 2257\n",
      "   - Sample events: ['australia_floods', 'california_fires', 'costarica_panama_flooding', '... and 15 more']\n",
      "\n",
      "âœ… Created drcs_activations_2022.json\n",
      "   - Events: 12\n",
      "   - Total .tif files: 2223\n",
      "   - Sample events: ['australia_flooding_202203', 'bangladesh_flooding_202206', 'brazil_flooding_202205', '... and 9 more']\n",
      "\n",
      "âœ… Created drcs_activations_2023.json\n",
      "   - Events: 24\n",
      "   - Total .tif files: 3821\n",
      "   - Sample events: ['california_atmospheric_river', 'greece_wildfires', 'hawaii_wildfires_202308', '... and 21 more']\n",
      "\n",
      "âœ… Created drcs_activations_2024.json\n",
      "   - Events: 40\n",
      "   - Total .tif files: 8799\n",
      "   - Sample events: ['bangladesh_flood_202408', 'brasil_flood_202405', 'california_atmospheric_river_202402', '... and 37 more']\n",
      "\n",
      "âœ… Created drcs_activations_2025.json\n",
      "   - Events: 5\n",
      "   - Total .tif files: 2153\n",
      "   - Sample events: ['202501_Fire_CA', '202502_Flood_OhioValley', '202503_SevereWx_US', '... and 2 more']\n",
      "\n",
      "ğŸ“Š Year-based splitting complete!\n"
     ]
    }
   ],
   "source": [
    "# Automatically split DRCS data by year after saving\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"\\nğŸ“… Splitting DRCS data by year (2020-2025)...\\n\")\n",
    "    \n",
    "    # Split the data by year\n",
    "    years_data = split_drcs_by_year(drcs_result)\n",
    "    \n",
    "    # Create separate JSON files for each year\n",
    "    for year in range(2020, 2026):\n",
    "        output_file = f'drcs_activations_{year}.json'\n",
    "        \n",
    "        # Count statistics\n",
    "        event_count = len(years_data[year])\n",
    "        total_files = sum(count_files_recursive(event_data) \n",
    "                         for event_data in years_data[year].values())\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            \"drcs_activations\": years_data[year],\n",
    "            \"_metadata\": {\n",
    "                \"year\": year,\n",
    "                \"extracted_from\": OUTPUT_FILE,\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"total_events\": event_count,\n",
    "                \"total_tif_files\": total_files,\n",
    "                \"events\": sorted(list(years_data[year].keys()))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"âœ… Created {output_file}\")\n",
    "        print(f\"   - Events: {event_count}\")\n",
    "        print(f\"   - Total .tif files: {total_files}\")\n",
    "        if event_count > 0:\n",
    "            sample_events = list(years_data[year].keys())[:3]\n",
    "            if event_count > 3:\n",
    "                sample_events.append(f\"... and {event_count - 3} more\")\n",
    "            print(f\"   - Sample events: {sample_events}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"ğŸ“Š Year-based splitting complete!\")\n",
    "else:\n",
    "    print(\"âš ï¸ No DRCS data available. Run the DRCS crawler first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
