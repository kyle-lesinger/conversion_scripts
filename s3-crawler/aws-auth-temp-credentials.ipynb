{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# AWS Authentication with Temporary Credentials\n",
    "\n",
    "This notebook provides flexible AWS authentication methods, including support for temporary credentials.\n",
    "\n",
    "## Features:\n",
    "- Multiple authentication methods (IAM user, temporary credentials, SSO)\n",
    "- Session token support for MFA-protected accounts\n",
    "- Credential verification and testing\n",
    "- Automatic credential refresh handling\n",
    "- Profile-based authentication\n",
    "- Environment variable configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, Optional, Any\n",
    "from botocore.exceptions import ClientError, NoCredentialsError, ProfileNotFound\n",
    "from botocore.session import Session\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. AWS Authentication Methods\n",
    "\n",
    "Choose one of the following authentication methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### Method 1: Temporary Credentials (STS Assume Role or MFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Enter temporary credentials directly\n",
    "# Uncomment and fill in your temporary credentials\n",
    "\n",
    "TEMP_ACCESS_KEY_ID = \"\"  # e.g., \"ASIA...\"\n",
    "TEMP_SECRET_ACCESS_KEY = \"\"  # Your temporary secret key\n",
    "TEMP_SESSION_TOKEN = \"\"  # Your session token (required for temporary credentials)\n",
    "\n",
    "# Optional: Set expiration time if known\n",
    "TEMP_EXPIRATION = \"\"  # e.g., \"2024-01-20T12:00:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Session created with temporary credentials\n"
     ]
    }
   ],
   "source": [
    "def create_session_with_temp_credentials(\n",
    "    access_key_id: str,\n",
    "    secret_access_key: str,\n",
    "    session_token: str,\n",
    "    region_name: str = 'us-west-2'\n",
    ") -> boto3.Session:\n",
    "    \"\"\"\n",
    "    Create a boto3 session using temporary credentials.\n",
    "    \n",
    "    Args:\n",
    "        access_key_id: Temporary AWS access key ID\n",
    "        secret_access_key: Temporary AWS secret access key\n",
    "        session_token: AWS session token\n",
    "        region_name: AWS region (default: us-east-1)\n",
    "        \n",
    "    Returns:\n",
    "        boto3.Session object configured with temporary credentials\n",
    "    \"\"\"\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=access_key_id,\n",
    "        aws_secret_access_key=secret_access_key,\n",
    "        aws_session_token=session_token,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    \n",
    "    return session\n",
    "\n",
    "# Create session with temporary credentials if provided\n",
    "if TEMP_ACCESS_KEY_ID and TEMP_SECRET_ACCESS_KEY and TEMP_SESSION_TOKEN:\n",
    "    session = create_session_with_temp_credentials(\n",
    "        TEMP_ACCESS_KEY_ID,\n",
    "        TEMP_SECRET_ACCESS_KEY,\n",
    "        TEMP_SESSION_TOKEN\n",
    "    )\n",
    "    print(\"‚úÖ Session created with temporary credentials\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Temporary credentials not provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 3. Credential Verification and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using session\n",
      "‚úÖ AWS Credentials Valid!\n",
      "\n",
      "Account Details:\n",
      "  Account ID: 444055461661\n",
      "  User ARN: arn:aws:iam::444055461661:user/klesinger\n",
      "  User ID: AIDAWOY6ET4O25743W2P7\n"
     ]
    }
   ],
   "source": [
    "def verify_credentials(session: Optional[boto3.Session] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Verify AWS credentials and get caller identity.\n",
    "    \n",
    "    Args:\n",
    "        session: boto3 Session object (uses default if None)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with account information or None if failed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if session:\n",
    "            print('using session')\n",
    "            sts_client = session.client('sts')\n",
    "        else:\n",
    "            sts_client = boto3.client('sts')\n",
    "        \n",
    "        # Get caller identity\n",
    "        response = sts_client.get_caller_identity()\n",
    "        \n",
    "        print(\"‚úÖ AWS Credentials Valid!\")\n",
    "        print(f\"\\nAccount Details:\")\n",
    "        print(f\"  Account ID: {response['Account']}\")\n",
    "        print(f\"  User ARN: {response['Arn']}\")\n",
    "        print(f\"  User ID: {response['UserId']}\")\n",
    "        \n",
    "        # Check if using temporary credentials\n",
    "        if 'assumed-role' in response['Arn'] or response['Arn'].startswith('arn:aws:sts'):\n",
    "            print(\"\\nüìå Using temporary credentials (assumed role or session token)\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except NoCredentialsError:\n",
    "        print(\"‚ùå No credentials found. Please configure credentials using one of the methods above.\")\n",
    "        return None\n",
    "    except ClientError as e:\n",
    "        print(f\"‚ùå Error verifying credentials: {e}\")\n",
    "        return None\n",
    "\n",
    "# Verify current credentials\n",
    "if 'session' in locals():\n",
    "    identity = verify_credentials(session)\n",
    "else:\n",
    "    identity = verify_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. Test AWS Service Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Can access bucket: nasa-disasters\n",
      "  Found 5 objects (showing max 5)\n"
     ]
    }
   ],
   "source": [
    "def test_s3_access(session: Optional[boto3.Session] = None, bucket_name: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Test S3 access with current credentials.\n",
    "    \n",
    "    Args:\n",
    "        session: boto3 Session object (uses default if None)\n",
    "        bucket_name: Specific bucket to test (lists all buckets if None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if session:\n",
    "            s3_client = session.client('s3')\n",
    "        else:\n",
    "            s3_client = boto3.client('s3')\n",
    "        \n",
    "        if bucket_name:\n",
    "            # Test specific bucket access\n",
    "            response = s3_client.head_bucket(Bucket=bucket_name)\n",
    "            print(f\"‚úÖ Can access bucket: {bucket_name}\")\n",
    "            \n",
    "            # Try to list objects\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, MaxKeys=5)\n",
    "            object_count = response.get('KeyCount', 0)\n",
    "            print(f\"  Found {object_count} objects (showing max 5)\")\n",
    "            \n",
    "        else:\n",
    "            # List all accessible buckets\n",
    "            response = s3_client.list_buckets()\n",
    "            buckets = response.get('Buckets', [])\n",
    "            \n",
    "            print(f\"‚úÖ S3 Access Confirmed\")\n",
    "            print(f\"\\nAccessible Buckets ({len(buckets)} total):\")\n",
    "            \n",
    "            for bucket in buckets[:10]:  # Show first 10\n",
    "                print(f\"  - {bucket['Name']} (created: {bucket['CreationDate'].strftime('%Y-%m-%d')})\")\n",
    "            \n",
    "            if len(buckets) > 10:\n",
    "                print(f\"  ... and {len(buckets) - 10} more\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        if error_code == 'AccessDenied':\n",
    "            print(f\"‚ùå Access Denied to S3\")\n",
    "        elif error_code == 'NoSuchBucket':\n",
    "            print(f\"‚ùå Bucket '{bucket_name}' does not exist\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error accessing S3: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "# # Test S3 access\n",
    "# if 'session' in locals():\n",
    "#     test_s3_access(session)\n",
    "# else:\n",
    "#     test_s3_access()\n",
    "\n",
    "# Test specific bucket (uncomment and modify):\n",
    "test_s3_access(session, \"nasa-disasters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 8. Integration with S3 Bucket Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ready to use with S3 bucket crawler!\n",
      "\n",
      "You can now:\n",
      "1. Import the S3BucketCrawler class from the other notebook\n",
      "2. Pass this s3_client to the crawler: S3BucketCrawler('bucket-name', s3_client)\n",
      "\n",
      "Example:\n",
      "  crawler = S3BucketCrawler('nasa-disasters', s3_client)\n",
      "  result = crawler.crawl()\n"
     ]
    }
   ],
   "source": [
    "# Once authenticated, you can use the session with the S3 bucket crawler\n",
    "# or any other AWS operations\n",
    "\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    print(\"‚úÖ Ready to use with S3 bucket crawler!\")\n",
    "    print(\"\\nYou can now:\")\n",
    "    print(\"1. Import the S3BucketCrawler class from the other notebook\")\n",
    "    print(\"2. Pass this s3_client to the crawler: S3BucketCrawler('bucket-name', s3_client)\")\n",
    "    print(\"\\nExample:\")\n",
    "    print(\"  crawler = S3BucketCrawler('nasa-disasters', s3_client)\")\n",
    "    print(\"  result = crawler.crawl()\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please complete authentication setup first using one of the methods above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3BucketCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "class S3BucketCrawler:\n",
    "    \"\"\"\n",
    "    Crawls S3 bucket structure and creates a nested dictionary of .tif files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories = set()\n",
    "        \n",
    "    def build_nested_structure(self, file_list: list) -> Dict:\n",
    "        \"\"\"\n",
    "        Convert flat S3 paths to nested dictionary structure.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of dictionaries containing file information\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing directory structure\n",
    "        \"\"\"\n",
    "        root = {}\n",
    "        \n",
    "        for file_info in file_list:\n",
    "            path_parts = file_info['key'].split('/')\n",
    "            current_level = root\n",
    "            \n",
    "            # Navigate/create the directory structure\n",
    "            for i, part in enumerate(path_parts[:-1]):\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {\n",
    "                        '_type': 'directory',\n",
    "                        '_path': '/'.join(path_parts[:i+1]) + '/',\n",
    "                        '_files': [],\n",
    "                        '_subdirs': {}\n",
    "                    }\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "                else:\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "            \n",
    "            # Add the file to its directory\n",
    "            file_name = path_parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            \n",
    "            current_level['_files'].append({\n",
    "                'name': file_name,\n",
    "                'path': file_info['key'],\n",
    "                'size_bytes': file_info['size'],\n",
    "                'size_readable': self._format_size(file_info['size']),\n",
    "                'last_modified': file_info['last_modified'],\n",
    "                'storage_class': file_info.get('storage_class', 'STANDARD')\n",
    "            })\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def crawl(self, \n",
    "              prefix: str = '', \n",
    "              show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the bucket and return nested structure of .tif files only.\n",
    "        \n",
    "        Args:\n",
    "            prefix: Start from this prefix (subdirectory)\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing bucket structure with .tif files\n",
    "        \"\"\"\n",
    "        print(f\"üîç Starting crawl of s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"üìå Filtering for .tif files only\")\n",
    "        \n",
    "        # Reset statistics\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        all_tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning bucket...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        self.total_files += 1\n",
    "                        self.total_size += obj.get('Size', 0)\n",
    "                        \n",
    "                        # Track directories\n",
    "                        dir_path = '/'.join(key.split('/')[:-1])\n",
    "                        if dir_path:\n",
    "                            self.total_directories.add(dir_path)\n",
    "                        \n",
    "                        # Add file info\n",
    "                        all_tif_files.append({\n",
    "                            'key': key,\n",
    "                            'size': obj.get('Size', 0),\n",
    "                            'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None,\n",
    "                            'storage_class': obj.get('StorageClass', 'STANDARD')\n",
    "                        })\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {self.total_files} .tif files\")\n",
    "        print(f\"üìÅ Across {len(self.total_directories)} directories\")\n",
    "        print(f\"üíæ Total size: {self._format_size(self.total_size)}\")\n",
    "        \n",
    "        # Build nested structure\n",
    "        structure = self.build_nested_structure(all_tif_files)\n",
    "        \n",
    "        # Create final result with metadata\n",
    "        result = {\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"prefix\": prefix,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"file_filter\": \".tif\",\n",
    "                \"total_files\": self.total_files,\n",
    "                \"total_directories\": len(self.total_directories),\n",
    "                \"total_size_bytes\": self.total_size,\n",
    "                \"total_size_readable\": self._format_size(self.total_size)\n",
    "            },\n",
    "            \"structure\": structure\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _format_size(self, size_bytes: int) -> str:\n",
    "        \"\"\"Format bytes to human-readable size.\"\"\"\n",
    "        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if size_bytes < 1024.0:\n",
    "                return f\"{size_bytes:.2f} {unit}\"\n",
    "            size_bytes /= 1024.0\n",
    "        return f\"{size_bytes:.2f} PB\"\n",
    "\n",
    "print(\"‚úÖ S3BucketCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3DisastersCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class S3DisastersCrawler:\n",
    "    \"\"\"\n",
    "    Simplified crawler for drcs_activations directory that creates clean nested structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.activation_events = set()\n",
    "        \n",
    "    def build_clean_structure(self, file_list: list, prefix: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Build a clean nested structure organized by activation events.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of S3 keys\n",
    "            prefix: The prefix to remove (e.g., 'drcs_activations/')\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary with activation events and their files\n",
    "        \"\"\"\n",
    "        structure = {}\n",
    "        \n",
    "        for key in file_list:\n",
    "            # Remove the prefix to get relative path\n",
    "            relative_path = key.replace(prefix, '', 1) if key.startswith(prefix) else key\n",
    "            parts = relative_path.split('/')\n",
    "            \n",
    "            # Skip if not enough parts\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            # First part is the activation event (e.g., '202301_Flood_CA')\n",
    "            activation_event = parts[0]\n",
    "            self.activation_events.add(activation_event)\n",
    "            \n",
    "            # Initialize activation event if not exists\n",
    "            if activation_event not in structure:\n",
    "                structure[activation_event] = {}\n",
    "            \n",
    "            # Build nested structure for remaining parts\n",
    "            current_level = structure[activation_event]\n",
    "            \n",
    "            # Navigate through subdirectories\n",
    "            for part in parts[1:-1]:\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {}\n",
    "                # Check if current_level[part] is a list (files), if so convert to dict\n",
    "                if isinstance(current_level[part], list):\n",
    "                    current_level[part] = {'_files': current_level[part]}\n",
    "                current_level = current_level[part]\n",
    "            \n",
    "            # Add the file\n",
    "            filename = parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            current_level['_files'].append(filename)\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def crawl_drcs_activations(self, show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl only the drcs_activations directory for .tif files.\n",
    "        \n",
    "        Args:\n",
    "            show_progress: Show progress during crawl\n",
    "            \n",
    "        Returns:\n",
    "            Clean nested dictionary of activation events and their .tif files\n",
    "        \"\"\"\n",
    "        prefix = 'drcs_activations/'\n",
    "        print(f\"üîç Crawling s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"üìå Filtering for .tif files in activation events\")\n",
    "        \n",
    "        # Reset counters\n",
    "        self.total_files = 0\n",
    "        self.activation_events.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning activation events...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        tif_files.append(key)\n",
    "                        self.total_files += 1\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {self.total_files} .tif files\")\n",
    "        \n",
    "        # Build clean structure\n",
    "        structure = self.build_clean_structure(tif_files, prefix)\n",
    "        \n",
    "        print(f\"üìÅ Across {len(self.activation_events)} activation events\")\n",
    "        \n",
    "        # Create result with drcs_activations as root\n",
    "        result = {\n",
    "            \"drcs_activations\": structure,\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"total_tif_files\": self.total_files,\n",
    "                \"total_activation_events\": len(self.activation_events),\n",
    "                \"activation_events\": sorted(list(self.activation_events))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"‚úÖ S3DisastersCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 10. Helper Functions for Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_to_json(data: Dict, filename: str, indent: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Save dictionary to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary to save\n",
    "        filename: Output filename\n",
    "        indent: JSON indentation (None for compact)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    filepath = os.path.abspath(filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=indent, default=str)\n",
    "    \n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"‚úÖ Saved to: {filepath}\")\n",
    "    print(f\"üìä File size: {file_size:,} bytes\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def print_structure_preview(structure: Dict, max_depth: int = 3, current_depth: int = 0, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Print a tree-like preview of the structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Nested dictionary structure\n",
    "        max_depth: Maximum depth to display\n",
    "        current_depth: Current recursion depth\n",
    "        prefix: Prefix for tree display\n",
    "    \"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    # Process directories\n",
    "    for key, value in structure.items():\n",
    "        if isinstance(value, dict) and '_type' in value and value['_type'] == 'directory':\n",
    "            print(f\"{prefix}üìÅ {key}/\")\n",
    "            \n",
    "            # Show files in this directory\n",
    "            if '_files' in value:\n",
    "                for file in value['_files'][:3]:  # Show first 3 files\n",
    "                    print(f\"{prefix}  üìÑ {file['name']} ({file['size_readable']})\")\n",
    "                if len(value['_files']) > 3:\n",
    "                    print(f\"{prefix}  ... and {len(value['_files']) - 3} more .tif files\")\n",
    "            \n",
    "            # Recurse into subdirectories\n",
    "            if '_subdirs' in value and current_depth < max_depth:\n",
    "                print_structure_preview(value['_subdirs'], max_depth, current_depth + 1, prefix + \"  \")\n",
    "\n",
    "def get_all_file_paths(structure: Dict) -> list:\n",
    "    \"\"\"\n",
    "    Extract all file paths from nested structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Nested dictionary structure\n",
    "        \n",
    "    Returns:\n",
    "        List of all file paths\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    def extract_recursive(obj):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        file_paths.append(file_info['path'])\n",
    "                if '_subdirs' in value:\n",
    "                    extract_recursive(value['_subdirs'])\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        extract_recursive(structure['structure'])\n",
    "    else:\n",
    "        extract_recursive(structure)\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "def get_statistics(structure: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics from crawled structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Crawled structure dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with statistics\n",
    "    \"\"\"\n",
    "    files_data = []\n",
    "    \n",
    "    def analyze_recursive(obj, current_dir=\"\"):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        files_data.append({\n",
    "                            'directory': current_dir if current_dir else '/',\n",
    "                            'filename': file_info['name'],\n",
    "                            'full_path': file_info['path'],\n",
    "                            'size_bytes': file_info['size_bytes'],\n",
    "                            'size_readable': file_info['size_readable'],\n",
    "                            'last_modified': file_info['last_modified']\n",
    "                        })\n",
    "                if '_subdirs' in value:\n",
    "                    new_dir = f\"{current_dir}/{key}\" if current_dir else key\n",
    "                    analyze_recursive(value['_subdirs'], new_dir)\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        analyze_recursive(structure['structure'])\n",
    "    else:\n",
    "        analyze_recursive(structure)\n",
    "    \n",
    "    df = pd.DataFrame(files_data)\n",
    "    \n",
    "    if not df.empty:\n",
    "        print(\"\\nüìä Statistics:\")\n",
    "        print(f\"Total .tif files: {len(df)}\")\n",
    "        print(f\"Total size: {df['size_bytes'].sum():,} bytes\")\n",
    "        print(f\"Unique directories: {df['directory'].nunique()}\")\n",
    "        \n",
    "        # Top directories by file count\n",
    "        print(\"\\nTop directories by file count:\")\n",
    "        top_dirs = df['directory'].value_counts().head(5)\n",
    "        for dir_name, count in top_dirs.items():\n",
    "            print(f\"  {dir_name}: {count} files\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 11. Main Execution - Crawl S3 Buckets for .tif Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Bucket: s3://nasa-disasters/drcs_activations\n",
      "  File filter: .tif files only\n",
      "  Output file: s3_tif_files_structure.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = \"nasa-disasters\"  # Change this to your target bucket\n",
    "PREFIX = \"drcs_activations\"                      # Leave empty for entire bucket, or specify a path like \"drcs_activations/\"\n",
    "OUTPUT_FILE = \"s3_tif_files_structure.json\"  # Output filename\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/{PREFIX}\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting crawl... This may take a while for large buckets.\n",
      "\n",
      "üîç Starting crawl of s3://nasa-disasters/drcs_activations\n",
      "üìå Filtering for .tif files only\n",
      "Scanning bucket...\n",
      "  Found 34400 .tif files...\n",
      "‚úÖ Found 34460 .tif files\n",
      "üìÅ Across 1518 directories\n",
      "üíæ Total size: 5.53 TB\n",
      "\n",
      "‚úÖ Crawl completed in 47.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize crawler and run\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = S3BucketCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"üöÄ Starting crawl... This may take a while for large buckets.\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform the crawl\n",
    "    result = crawler.crawl(\n",
    "        prefix=PREFIX,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "else:\n",
    "    print(\"‚ùå Please complete authentication setup first using the cells above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## 12. Preview and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Directory Structure Preview (max depth 3):\n",
      "\n",
      "s3://nasa-disasters/\n",
      "üìÅ drcs_activations/\n",
      "  üìÅ 2020/\n",
      "    üìÅ aegean_sea_earthquake_202010/\n",
      "      üìÅ aria/\n",
      "    üìÅ bolivia_fires/\n",
      "      üìÅ aster/\n",
      "    üìÅ california_fires/\n",
      "      üìÅ aria/\n",
      "      üìÅ aster/\n",
      "      üìÅ dnbr/\n",
      "      üìÅ master/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ uavsar/\n",
      "    üìÅ colorado_fires/\n",
      "      üìÅ aster/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_delta/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ radarsat2/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_sally/\n",
      "      üìÅ dfo/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_zeta/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricanes_eta_iota/\n",
      "      üìÅ aria/\n",
      "      üìÅ dartmouth_flood_observatory/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ mi_dam_failure_2020/\n",
      "      üìÅ aria/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ neuse_river_nc/\n",
      "      üìÅ planet/\n",
      "    üìÅ volcano_research/\n",
      "      üìÅ lewotolok_20201129/\n",
      "    üìÅ washington_oregon_fires/\n",
      "      üìÅ aster/\n",
      "      üìÅ sentinel2/\n",
      "  üìÅ 2021/\n",
      "    üìÅ australia_floods/\n",
      "      üìÅ dartmouth_flood_observatory/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ california_fires/\n",
      "      üìÅ aster/\n",
      "    üìÅ costarica_panama_flooding/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ guyana_floods/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ haiti_earthquake_202108/\n",
      "      üìÅ aria/\n",
      "      üìÅ aster/\n",
      "      üìÅ landslide_inventory/\n",
      "      üìÅ perusat/\n",
      "      üìÅ satellogic/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_elsa/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_ida/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarblehd/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ lis/\n",
      "      üìÅ sar_moldanado-20210903T190339Z-001/\n",
      "      üìÅ satellogic/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ tropics/\n",
      "      üìÅ uavsar/\n",
      "    üìÅ la_palma_eruption/\n",
      "      üìÅ aria/\n",
      "      üìÅ omps_viirs/\n",
      "    üìÅ la_soufriere_eruption/\n",
      "      üìÅ alos2/\n",
      "      üìÅ aria/\n",
      "      üìÅ aster/\n",
      "      üìÅ ecostress/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ modis_viirs/\n",
      "      üìÅ omps/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ nepal_landslides_flooding_202106/\n",
      "      üìÅ aria/\n",
      "    üìÅ north_carolina_hurrex/\n",
      "      üìÅ dnb/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ north_carolina_wildfire/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ pacificNW_flooding/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ peru_earthquake/\n",
      "      üìÅ aria/\n",
      "    üìÅ satellogic_test/\n",
      "      üìÅ 20210304_164047_SN10_L1_Fuego_28327/\n",
      "      üìÅ 20210306_163150_SN15_L1_Pacaya_28326/\n",
      "    üìÅ semeru_eruption/\n",
      "      üìÅ aria/\n",
      "    üìÅ tornado_outbreak_20211210/\n",
      "      üìÅ aria/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ ts_fred/\n",
      "      üìÅ sentinel2/\n",
      "  üìÅ 2022/\n",
      "    üìÅ australia_flooding_202203/\n",
      "      üìÅ planet/\n",
      "    üìÅ bangladesh_flooding_202206/\n",
      "      üìÅ eos_rs/\n",
      "    üìÅ brazil_flooding_202205/\n",
      "      üìÅ fpm/\n",
      "    üìÅ fiona_msfc/\n",
      "      üìÅ 9_22/\n",
      "      üìÅ 9_23/\n",
      "    üìÅ hurricane_fiona_2022/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarble_hd/\n",
      "      üìÅ files/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ hurricane_ian_2022/\n",
      "      üìÅ DFO/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarble_hd/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ landsat9/\n",
      "      üìÅ maxar/\n",
      "      üìÅ misr/\n",
      "      üìÅ planet.Overviews/\n",
      "      üìÅ planet/\n",
      "      üìÅ radarsat/\n",
      "      üìÅ radarsat2/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_nicole_2022/\n",
      "      üìÅ radarsat2WE/\n",
      "    üìÅ ky_flooding_202207/\n",
      "      üìÅ aria/\n",
      "      üìÅ trueColor/\n",
      "    üìÅ mauna_loa_eruption_2022/\n",
      "      üìÅ aria/\n",
      "      üìÅ eos_rs/\n",
      "      üìÅ omps/\n",
      "      üìÅ viirs/\n",
      "    üìÅ southafrica_flooding_202204/\n",
      "      üìÅ fpm/\n",
      "    üìÅ tonga_volcano_tsunami_202201/\n",
      "      üìÅ aria/\n",
      "      üìÅ omps/\n",
      "      üìÅ planet/\n",
      "      üìÅ viirs/\n",
      "    üìÅ yellowstone_np_flooding_202206/\n",
      "      üìÅ aria/\n",
      "  üìÅ 2023/\n",
      "    üìÅ california_atmospheric_river/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ greece_wildfires/\n",
      "      üìÅ planet/\n",
      "    üìÅ hawaii_wildfires_202308/\n",
      "      üìÅ DPM1_ALOS2/\n",
      "      üìÅ ECOSTRESS/\n",
      "      üìÅ ISS_Imagery/\n",
      "      üìÅ aria/\n",
      "      üìÅ eos_rs/\n",
      "      üìÅ hawaii_wildfires.Overviews/\n",
      "      üìÅ landsat/\n",
      "      üìÅ opera/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_hilary/\n",
      "      üìÅ landsat/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_idalia/\n",
      "      üìÅ aria_opera/\n",
      "      üìÅ aria_opera_dswx/\n",
      "      üìÅ blackmarble_hd/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet.Overviews/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_otis/\n",
      "      üìÅ Landslides/\n",
      "      üìÅ blackmarble_hd/\n",
      "    üìÅ morocco_earthquake_202309/\n",
      "      üìÅ aria/\n",
      "      üìÅ eos_rs/\n",
      "    üìÅ nc_tornado_20230719/\n",
      "      üìÅ aria/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ newengland_flooding_202312/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ newengland_flooding_202312.Overviews/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ s2_swir_vermont_flooding/\n",
      "    üìÅ turkiye_earthquakes_202302/\n",
      "      üìÅ NIST/\n",
      "      üìÅ aria/\n",
      "      üìÅ dnb/\n",
      "      üìÅ eos_rs/\n",
      "      üìÅ landsat/\n",
      "      üìÅ landslides_turkiye_earthquake_2023/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ typhoon_mawar/\n",
      "      üìÅ planet/\n",
      "    üìÅ vermont_flooding_202307/\n",
      "      üìÅ planet.Overviews/\n",
      "      üìÅ planet/\n",
      "  üìÅ 202301_Flood_CA/\n",
      "    üìÅ sentinel1/\n",
      "  üìÅ 202302_Earthquake_Turkiye/\n",
      "    üìÅ NIST/\n",
      "      üìÅ NIST_additional_sites/\n",
      "      üìÅ NIST_hospital_sites/\n",
      "    üìÅ aria/\n",
      "      üìÅ rgb/\n",
      "    üìÅ dnb/\n",
      "    üìÅ eos_rs/\n",
      "    üìÅ landsat/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ landsat9/\n",
      "    üìÅ landslides_turkiye_earthquake_2023/\n",
      "      üìÅ planet/\n",
      "    üìÅ sentinel1/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202305_Typhoon_Mawar/\n",
      "    üìÅ planet/\n",
      "  üìÅ 20230719_SevereWx_NC/\n",
      "    üìÅ aria/\n",
      "    üìÅ landsat/\n",
      "    üìÅ planet/\n",
      "      üìÅ cir/\n",
      "      üìÅ tc/\n",
      "    üìÅ sentinel1/\n",
      "  üìÅ 202307_Fire_Greece/\n",
      "    üìÅ planet/\n",
      "      üìÅ Aigio_wildfire_3band_20230727_psscene_visual/\n",
      "      üìÅ Aigio_wildfires_3band_20230721_psscene_visual/\n",
      "      üìÅ Aigio_wildfires_3band_20230724_psscene_visual/\n",
      "      üìÅ Corfu_wildfire_3band_20230727_psscene_visual/\n",
      "      üìÅ Corfu_wildfires_3band_20230722_psscene_visual/\n",
      "      üìÅ Corfu_wildfires_3band_20230724_psscene_visual/\n",
      "      üìÅ Corfu_wildfires_3band_20230725_psscene_visual/\n",
      "      üìÅ Epanochori_wildfires_3band_20230724_psscene_visual/\n",
      "      üìÅ Rhodes_wildfire_3band_20230728_psscene_visual/\n",
      "      üìÅ Rhodes_wildfires_3band_20230718_psscene_visual/\n",
      "      üìÅ Rhodes_wildfires_3band_20230724_psscene_visual/\n",
      "      üìÅ Rhodes_wildfires_3band_20230725_psscene_visual/\n",
      "      üìÅ SouthernEvia_wildfires_3band_20230719_psscene_visual/\n",
      "      üìÅ SouthernEvia_wildfires_3band_20230725_psscene_visual/\n",
      "      üìÅ SouthernEvia_wildfires_3band_20230728_psscene_visual/\n",
      "  üìÅ 202307_Flood_VT/\n",
      "    üìÅ planet.Overviews/\n",
      "      üìÅ planet_20230711.Overviews/\n",
      "      üìÅ planet_colorinfrared.Overviews/\n",
      "      üìÅ planet_colorinfrared_20230711.Overviews/\n",
      "    üìÅ planet/\n",
      "      üìÅ 20230711/\n",
      "      üìÅ 20230712/\n",
      "  üìÅ 202308_Hurricane_Hilary/\n",
      "    üìÅ landsat/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ landsat9/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202309_Earthquake_Morocco/\n",
      "    üìÅ aria/\n",
      "    üìÅ eos_rs/\n",
      "  üìÅ 202309_Hurricane_Idalia/\n",
      "    üìÅ aria_opera/\n",
      "      üìÅ pre_event/\n",
      "    üìÅ aria_opera_dswx/\n",
      "    üìÅ blackmarble_hd/\n",
      "    üìÅ landsat/\n",
      "      üìÅ pre_event/\n",
      "    üìÅ planet.Overviews/\n",
      "      üìÅ planet_colorinfrared_postevent.Overviews/\n",
      "      üìÅ planet_truecolor_postevent.Overviews/\n",
      "      üìÅ preevent_colorinfrared.Overviews/\n",
      "      üìÅ truecolor_preevent.Overviews/\n",
      "    üìÅ planet/\n",
      "      üìÅ post_event/\n",
      "      üìÅ pre_event/\n",
      "    üìÅ sentinel2/\n",
      "      üìÅ pre_event/\n",
      "  üìÅ 202310_Hurricane_Otis/\n",
      "    üìÅ Landslides/\n",
      "      üìÅ planet/\n",
      "    üìÅ blackmarble_hd/\n",
      "      üìÅ BMHD_Otis_Nov8/\n",
      "      üìÅ BMHD_Otis_Nov9_13/\n",
      "  üìÅ 202312_Flood_NewEngland/\n",
      "    üìÅ blackmarble/\n",
      "      üìÅ BMHD_StormMaine_Dec19_21/\n",
      "      üìÅ BMHD_StormMaine_Dec22_23/\n",
      "    üìÅ newengland_flooding_202312.Overviews/\n",
      "      üìÅ planet_cir_20231219.Overviews/\n",
      "      üìÅ planet_tc_20231219.Overviews/\n",
      "    üìÅ planet/\n",
      "      üìÅ colorIR/\n",
      "      üìÅ true/\n",
      "    üìÅ sentinel2/\n",
      "      üìÅ waldoboro/\n",
      "  üìÅ 2024/\n",
      "    üìÅ bangladesh_flood_202408/\n",
      "      üìÅ aria/\n",
      "      üìÅ planet/\n",
      "      üìÅ projects/\n",
      "    üìÅ brasil_flood_202405/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarble_hd/\n",
      "      üìÅ brasil_flood_202405_daily.Overviews/\n",
      "      üìÅ brasil_flood_202405_daily_overviews.Overviews/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ california_atmospheric_river_202402/\n",
      "      üìÅ aria_opera/\n",
      "    üìÅ california_earthquake_202412/\n",
      "      üìÅ opera/\n",
      "      üìÅ planet/\n",
      "      üìÅ projects/\n",
      "    üìÅ chile_wildfires_202402/\n",
      "      üìÅ planet/\n",
      "    üìÅ guatemala_wildfire_202402/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ hurricane_beryl_202407/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ projects/\n",
      "      üìÅ tropomi/\n",
      "    üìÅ hurricane_helene_202409/\n",
      "      üìÅ NDVIchange/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ landsat/\n",
      "      üìÅ projects/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ usda/\n",
      "    üìÅ hurricane_milton_202410/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ iowa_flood_202406/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ nepal_flood_202408/\n",
      "      üìÅ Sentinel1_ASF/\n",
      "      üìÅ planet/\n",
      "    üìÅ palos_verdes_landslides_202410/\n",
      "      üìÅ opera/\n",
      "    üìÅ se_us_severestorms_202401/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet/\n",
      "      üìÅ se_us_severestorms_202401.Overviews/\n",
      "    üìÅ texas_extremeheat_202405/\n",
      "      üìÅ ECOSTRESS/\n",
      "      üìÅ blackmarble_hd/\n",
      "      üìÅ blackmarble_processed/\n",
      "    üìÅ texas_flood_202405/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ texas_flood_202405_project_PR/\n",
      "    üìÅ texas_wildfires_202402/\n",
      "      üìÅ planet/\n",
      "      üìÅ projects/\n",
      "    üìÅ tropical_cyclone_debby_202408/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ landsat9/\n",
      "      üìÅ planet/\n",
      "      üìÅ precipitation/\n",
      "      üìÅ projects/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ tropical_storm_ernesto_202408/\n",
      "      üìÅ TROPICS/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ projects/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ tropical_storm_francine_202409/\n",
      "      üìÅ black_marble/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet/\n",
      "      üìÅ projects/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ tropical_storm_sara_202411/\n",
      "      üìÅ planet/\n",
      "      üìÅ projects/\n",
      "      üìÅ sentinel1/\n",
      "  üìÅ 202401_SevereWx_SoutheastUS/\n",
      "    üìÅ landsat/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ landsat9/\n",
      "    üìÅ planet/\n",
      "      üìÅ colorIR/\n",
      "      üìÅ true/\n",
      "    üìÅ se_us_severestorms_202401.Overviews/\n",
      "      üìÅ colorinfrared.Overviews/\n",
      "      üìÅ planet_truecolor_20240110.Overviews/\n",
      "  üìÅ 202402_Fire_Chile/\n",
      "    üìÅ planet/\n",
      "      üìÅ 20240118/\n",
      "      üìÅ 20240203/\n",
      "      üìÅ 20240205/\n",
      "      üìÅ 20240206/\n",
      "  üìÅ 202402_Fire_Guatemala/\n",
      "    üìÅ planet/\n",
      "      üìÅ Feb16/\n",
      "      üìÅ Feb22/\n",
      "      üìÅ Feb24/\n",
      "      üìÅ Feb25/\n",
      "    üìÅ sentinel2/\n",
      "      üìÅ swir/\n",
      "      üìÅ true/\n",
      "  üìÅ 202402_Fire_TX/\n",
      "    üìÅ planet/\n",
      "      üìÅ post/\n",
      "      üìÅ pre/\n",
      "    üìÅ projects/\n",
      "      üìÅ planet/\n",
      "  üìÅ 202402_Flood_CA/\n",
      "    üìÅ aria_opera/\n",
      "  üìÅ 202405_Flood_Brasil/\n",
      "    üìÅ aria/\n",
      "      üìÅ dswx_hls/\n",
      "      üìÅ flood_depth/\n",
      "    üìÅ blackmarble_hd/\n",
      "      üìÅ BMHD_Brazil2024-20240508T185236Z-001/\n",
      "    üìÅ brasil_flood_202405_daily.Overviews/\n",
      "      üìÅ planet_0506.Overviews/\n",
      "    üìÅ brasil_flood_202405_daily_overviews.Overviews/\n",
      "      üìÅ planet_0420.Overviews/\n",
      "      üìÅ planet_0506.Overviews/\n",
      "      üìÅ planet_0507.Overviews/\n",
      "    üìÅ planet/\n",
      "      üìÅ brasil_20240420_psscene_visual/\n",
      "      üìÅ brasil_20240506_psscene_visual/\n",
      "      üìÅ brasil_20240507_psscene_visual/\n",
      "    üìÅ sentinel1/\n",
      "      üìÅ rgb/\n",
      "      üìÅ water_extent/\n",
      "    üìÅ sentinel2/\n",
      "      üìÅ cir/\n",
      "      üìÅ swir/\n",
      "      üìÅ true/\n",
      "  üìÅ 202405_Flood_TX/\n",
      "    üìÅ planet/\n",
      "      üìÅ new_water_extents/\n",
      "      üìÅ water_extents/\n",
      "    üìÅ sentinel1/\n",
      "    üìÅ sentinel2/\n",
      "      üìÅ cir/\n",
      "      üìÅ swir/\n",
      "      üìÅ true/\n",
      "    üìÅ texas_flood_202405_project_PR/\n",
      "      üìÅ Planet_Mosaics.Overviews/\n",
      "  üìÅ 202405_Heat_TX/\n",
      "    üìÅ ECOSTRESS/\n",
      "      üìÅ ControlData_May2023/\n",
      "    üìÅ blackmarble_hd/\n",
      "      üìÅ BlackMarbleHD_Houston2024/\n",
      "    üìÅ blackmarble_processed/\n",
      "  üìÅ 202406_Flood_IA/\n",
      "    üìÅ landsat/\n",
      "    üìÅ planet/\n",
      "      üìÅ minnesota/\n",
      "      üìÅ planet_overviews.Overviews/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202407_Hurricane_Beryl/\n",
      "    üìÅ blackmarble/\n",
      "    üìÅ projects/\n",
      "      üìÅ BlackMarble_PR/\n",
      "    üìÅ tropomi/\n",
      "      üìÅ carbon_monoxide/\n",
      "      üìÅ methane/\n",
      "  üìÅ 202408_Flood_Bangladesh/\n",
      "    üìÅ aria/\n",
      "      üìÅ dswx_hls/\n",
      "    üìÅ planet/\n",
      "      üìÅ PerformanceTesting_PR/\n",
      "      üìÅ bangladesh_aoi1_psscene_visual/\n",
      "      üìÅ bangladesh_aoi2_psscene_visual/\n",
      "      üìÅ bangladesh_aoi3_psscene_visual/\n",
      "      üìÅ bangladesh_aoi4_psscene_visual/\n",
      "      üìÅ bangladesh_aoi5_psscene_visual/\n",
      "      üìÅ bangladesh_aoi6_psscene_visual/\n",
      "      üìÅ bangladesh_aoi7_psscene_visual/\n",
      "    üìÅ projects/\n",
      "      üìÅ Planet_PR/\n",
      "  üìÅ 202408_Flood_Nepal/\n",
      "    üìÅ Sentinel1_ASF/\n",
      "      üìÅ 10August2024/\n",
      "      üìÅ 22August2024/\n",
      "    üìÅ planet/\n",
      "      üìÅ 15August2024/\n",
      "      üìÅ 17August2024/\n",
      "      üìÅ 26August2024/\n",
      "  üìÅ 202408_TropicalStorm_Debby/\n",
      "    üìÅ landsat8/\n",
      "    üìÅ landsat9/\n",
      "    üìÅ planet/\n",
      "    üìÅ precipitation/\n",
      "    üìÅ projects/\n",
      "      üìÅ Planet_PR/\n",
      "      üìÅ project_RS/\n",
      "      üìÅ sentinel_PR/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202408_TropicalStorm_Ernesto/\n",
      "    üìÅ TROPICS/\n",
      "    üìÅ blackmarble/\n",
      "      üìÅ BMHD/\n",
      "      üìÅ brdf_corrected/\n",
      "    üìÅ projects/\n",
      "      üìÅ Sentinel1_PR/\n",
      "      üìÅ blackmarble_TL_test/\n",
      "    üìÅ sentinel1/\n",
      "      üìÅ rgb/\n",
      "      üìÅ wm/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202409_Hurricane_Helene/\n",
      "    üìÅ NDVIchange/\n",
      "      üìÅ Oct2/\n",
      "      üìÅ Oct7/\n",
      "    üìÅ aria/\n",
      "      üìÅ DIST/\n",
      "      üìÅ DSWx/\n",
      "      üìÅ RTC/\n",
      "    üìÅ blackmarble/\n",
      "      üìÅ blackmarble_hd/\n",
      "    üìÅ blackmarble_hd/\n",
      "    üìÅ landsat/\n",
      "    üìÅ projects/\n",
      "      üìÅ NDVI_Change/\n",
      "      üìÅ PowerChange_PR/\n",
      "      üìÅ PowerChange_RS/\n",
      "      üìÅ blackmarble_RS/\n",
      "      üìÅ landsat_PR/\n",
      "      üìÅ landslides_RS/\n",
      "      üìÅ nrt_static_PR/\n",
      "      üìÅ sentinel_PR/\n",
      "    üìÅ sentinel1/\n",
      "    üìÅ sentinel2/\n",
      "    üìÅ usda/\n",
      "  üìÅ 202409_TropicalStorm_Francine/\n",
      "    üìÅ black_marble/\n",
      "      üìÅ BMHD/\n",
      "    üìÅ landsat/\n",
      "      üìÅ corrected_20240909/\n",
      "    üìÅ planet/\n",
      "    üìÅ projects/\n",
      "      üìÅ planet_PR/\n",
      "      üìÅ s2_landsat_PR/\n",
      "    üìÅ sentinel1/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202410_Hurricane_Milton/\n",
      "    üìÅ NDVIchange/\n",
      "    üìÅ blackmarble/\n",
      "    üìÅ landsat/\n",
      "    üìÅ opera/\n",
      "      üìÅ dist/\n",
      "      üìÅ dswx/\n",
      "    üìÅ projects/\n",
      "      üìÅ PowerChange_PR/\n",
      "      üìÅ landsat/\n",
      "      üìÅ sentinel/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ sentinel1/\n",
      "    üìÅ sentinel2/\n",
      "    üìÅ uavsar/\n",
      "      üìÅ 20241011/\n",
      "      üìÅ 20241012/\n",
      "      üìÅ 20241013/\n",
      "      üìÅ 20241014/\n",
      "      üìÅ post_deployment/\n",
      "  üìÅ 202410_Landslide_PalosVerdes/\n",
      "    üìÅ opera/\n",
      "      üìÅ coherence/\n",
      "      üìÅ displacement/\n",
      "      üìÅ uavsar/\n",
      "  üìÅ 202411_TropicalStorm_Sara/\n",
      "    üìÅ planet/\n",
      "      üìÅ Nov20_cr_psscene_visual/\n",
      "      üìÅ Nov21_cr_psscene_visual/\n",
      "      üìÅ Nov24_cr_psscene_visual/\n",
      "      üìÅ Nov25_cr_psscene_visual/\n",
      "      üìÅ Nov26_cr_psscene_visual/\n",
      "      üìÅ Nov27_cr_psscene_visual/\n",
      "      üìÅ Nov28_cr_psscene_visual/\n",
      "      üìÅ oct5_cr_psscene_visual/\n",
      "    üìÅ projects/\n",
      "      üìÅ pr/\n",
      "    üìÅ sentinel1/\n",
      "  üìÅ 202412_Earthquake_CA/\n",
      "    üìÅ opera/\n",
      "      üìÅ displacement/\n",
      "    üìÅ planet/\n",
      "      üìÅ Dec3_CA_psscene_visual/\n",
      "      üìÅ Dec5_CA_psscene_visual/\n",
      "      üìÅ nov29_CA_psscene_visual/\n",
      "    üìÅ projects/\n",
      "      üìÅ planet/\n",
      "  üìÅ 202501_Fire_CA/\n",
      "    üìÅ aria/\n",
      "      üìÅ asf/\n",
      "      üìÅ dist/\n",
      "    üìÅ aviris-3/\n",
      "      üìÅ charash/\n",
      "      üìÅ dnbr/\n",
      "      üìÅ earlylook/\n",
      "      üìÅ pca/\n",
      "    üìÅ burn_severity/\n",
      "    üìÅ ecostress/\n",
      "    üìÅ landsat/\n",
      "    üìÅ maxar_chng/\n",
      "    üìÅ planet/\n",
      "    üìÅ projects/\n",
      "      üìÅ Planet_PR/\n",
      "    üìÅ s1_2_fire_severity/\n",
      "    üìÅ s1_changedetection_asf/\n",
      "    üìÅ s1_dmgassessment/\n",
      "      üìÅ Jan16_Delivery/\n",
      "    üìÅ s2_dnbr/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202502_Flood_OhioValley/\n",
      "    üìÅ landsat/\n",
      "    üìÅ projects/\n",
      "      üìÅ landslides_RV/\n",
      "    üìÅ sentinel2/\n",
      "  üìÅ 202503_SevereWx_US/\n",
      "    üìÅ planet/\n",
      "    üìÅ projects/\n",
      "      üìÅ planet_SGF/\n",
      "      üìÅ planet_lix/\n",
      "      üìÅ planet_lsx/\n",
      "      üìÅ planet_lzk/\n",
      "      üìÅ planet_meg/\n",
      "      üìÅ planet_mob/\n",
      "      üìÅ planet_shv/\n",
      "    üìÅ sentinel-2/\n",
      "  üìÅ 202504_SevereWx_US/\n",
      "    üìÅ landsat/\n",
      "    üìÅ planet/\n",
      "      üìÅ 042025_SevWxFloods_AOI1/\n",
      "      üìÅ 042025_SevWxFloods_AOI2/\n",
      "      üìÅ 042025_SevWxFloods_AOI3/\n",
      "      üìÅ 042025_SevWxFloods_AOI4/\n",
      "      üìÅ Cincy-PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Cincy_PostEvent2_1_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Cincy_PostEvent2_2_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Cincy_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Lexington_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Lexington_PreEvent_1_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Lexington_PreEvent_2_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ LouisvilleKY_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ LouisvilleKY_PreEvent_1_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ LouisvilleKY_PreEvent_2_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ LouisvilleKY_PreEvent_3_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Salyersville_PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Saylersville_and_Vanceburg_PostEvent_psscene_analytic_8b_sr_udm2/\n",
      "      üìÅ Vanceburg_PreEvent_psscene_analytic_8b_sr_udm2/\n",
      "    üìÅ projects/\n",
      "      üìÅ landsat/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel2_jan/\n",
      "      üìÅ sentinel2_lzk/\n",
      "      üìÅ sentinel2_meg/\n",
      "      üìÅ sentinel2_ohx/\n",
      "      üìÅ sentinel2_pah/\n",
      "      üìÅ sentinel2_shv/\n",
      "    üìÅ sentinel2/\n",
      "    üìÅ viirs_flood/\n",
      "  üìÅ 202506_Fire_NM/\n",
      "    üìÅ aria/\n",
      "      üìÅ dist_hls/\n",
      "      üìÅ dswx_hls/\n",
      "    üìÅ planet/\n",
      "    üìÅ sentinel/\n",
      "    üìÅ sentinel1_flood/\n",
      "  üìÅ AdHoc_Disaster_products/\n",
      "    üìÅ Alaska_Quake_201811/\n",
      "    üìÅ Brumadinho_Brazil_Dam_Collapse_2019/\n",
      "      üìÅ ISS/\n",
      "    üìÅ CA_Fires_082018/\n",
      "      üìÅ CA_Fires_2018.Overviews/\n",
      "      üìÅ ER-2/\n",
      "      üìÅ ISS/\n",
      "      üìÅ Pleiades/\n",
      "    üìÅ CA_Fires_112018/\n",
      "      üìÅ aria/\n",
      "      üìÅ aster/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ gsfc_sar/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ recover/\n",
      "      üìÅ sentinel/\n",
      "      üìÅ uavsar/\n",
      "      üìÅ wrf-sfire/\n",
      "    üìÅ Fuego_Volcano_18/\n",
      "      üìÅ ARIA/\n",
      "      üìÅ Digital_Globe/\n",
      "      üìÅ Landsat/\n",
      "      üìÅ Ortho/\n",
      "      üìÅ Sentinel_1/\n",
      "      üìÅ Sentinel_2/\n",
      "    üìÅ Gita_18/\n",
      "    üìÅ Hokkaido_Earthquake_2018/\n",
      "    üìÅ Huricane Harvey 17/\n",
      "    üìÅ Hurricane_Florence_2018/\n",
      "      üìÅ aria/\n",
      "      üìÅ dnb/\n",
      "      üìÅ gfms/\n",
      "      üìÅ hurricane_florence.Overviews/\n",
      "      üìÅ landsat7/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ nwc/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ uavsar/\n",
      "    üìÅ Hurricane_Lane_2018/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ Hurricane_Maria_17/\n",
      "      üìÅ blackmarble/\n",
      "    üìÅ Hurricane_Michael_2018/\n",
      "      üìÅ aria/\n",
      "      üìÅ black_marble/\n",
      "      üìÅ dnb_sport/\n",
      "      üìÅ gfms/\n",
      "      üìÅ hurricane_michael.Overviews/\n",
      "      üìÅ iss/\n",
      "      üìÅ landsat7/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ radarsat2/\n",
      "      üìÅ sentinel_1/\n",
      "    üìÅ Hurricane_Willa_2018/\n",
      "      üìÅ dfo/\n",
      "      üìÅ gfms/\n",
      "      üìÅ hurricane_willa.Overviews/\n",
      "    üìÅ Kalaheo_Hi_Flood_2018/\n",
      "    üìÅ Kilauea_Volcano_2018/\n",
      "      üìÅ Aster/\n",
      "      üìÅ Interferogram/\n",
      "      üìÅ Landsat/\n",
      "      üìÅ MODIS/\n",
      "      üìÅ OMPS/\n",
      "      üìÅ SAR_RGB/\n",
      "      üìÅ VIIRS/\n",
      "    üìÅ LaosDamFailure/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel_alos/\n",
      "    üìÅ Lombok_2018/\n",
      "    üìÅ MexicoCity Earthquake 17/\n",
      "    üìÅ MidWest flooding 18/\n",
      "      üìÅ 20180218/\n",
      "      üìÅ 20180223/\n",
      "      üìÅ 20180225/\n",
      "      üìÅ 20180228/\n",
      "      üìÅ 20180302/\n",
      "    üìÅ Missouri Flood_17/\n",
      "    üìÅ NLE2018/\n",
      "      üìÅ dnb/\n",
      "    üìÅ Oaxaca_Mexico_18/\n",
      "    üìÅ PA_Flood_201808/\n",
      "    üìÅ Peru Flooding 17/\n",
      "    üìÅ SoCal_Fire_17/\n",
      "      üìÅ ARIA_Damage_Proxy_Map_v0.6_geotiff/\n",
      "    üìÅ Sulawesi_Quake_Tsunami_2018/\n",
      "      üìÅ landsat/\n",
      "    üìÅ Super_Typhoon_Yutu_2018/\n",
      "    üìÅ ak_fires_2019/\n",
      "      üìÅ AI_GeoTIFF_images/\n",
      "      üìÅ ASF_SAR/\n",
      "      üìÅ ak_fires_2019.Overviews/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ australian_fires_2019_2020/\n",
      "      üìÅ iss/\n",
      "      üìÅ misr/\n",
      "    üìÅ bolivia_landslide_2019/\n",
      "    üìÅ ca_earthquake_201907/\n",
      "      üìÅ aria/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ uas/\n",
      "    üìÅ ca_fires_201910/\n",
      "    üìÅ ca_fires_202008/\n",
      "      üìÅ aria/\n",
      "      üìÅ aster/\n",
      "      üìÅ imagecat/\n",
      "      üìÅ misr/\n",
      "      üìÅ uavsar/\n",
      "    üìÅ ca_flooding_201902/\n",
      "      üìÅ floodmap/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ centralus_storms_052019/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ modis/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ covid19/\n",
      "      üìÅ tropomi/\n",
      "    üìÅ croatia_earthquake_202003/\n",
      "    üìÅ cyclone_amphan_2020/\n",
      "      üìÅ cyclone_amphan_2020.Overviews/\n",
      "    üìÅ cyclone_fani_2019/\n",
      "      üìÅ blackmarblehd/\n",
      "    üìÅ cyclone_harold_2020/\n",
      "      üìÅ aria/\n",
      "      üìÅ iss/\n",
      "    üìÅ cyclone_idai_2019/\n",
      "      üìÅ aria/\n",
      "      üìÅ blackmarble/\n",
      "      üìÅ blackmarblehd/\n",
      "      üìÅ cyclone_idai_2019.Overviews/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ smap/\n",
      "    üìÅ cyclone_kennneth_2019/\n",
      "      üìÅ aria/\n",
      "      üìÅ cyclone_kenneth_2019.Overviews/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ hurricane_dorian_2019/\n",
      "      üìÅ aria/\n",
      "      üìÅ goes/\n",
      "      üìÅ gpm/\n",
      "      üìÅ hurricane_dorian_2019.Overviews/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "      üìÅ sport/\n",
      "      üìÅ viirs/\n",
      "    üìÅ hurricane_douglas_202008/\n",
      "      üìÅ misr/\n",
      "    üìÅ hurricane_isaias_2020/\n",
      "      üìÅ aria/\n",
      "    üìÅ hurricane_laura_2020/\n",
      "      üìÅ aria/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ planet/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ india_floods_201909/\n",
      "    üìÅ indonesia_flooding_202001/\n",
      "      üìÅ aria/\n",
      "    üìÅ japan_flooding_202007/\n",
      "    üìÅ krakatau_volcano_202004/\n",
      "      üìÅ VIIRSTIR Map of Krakatau SO2 Plume20200421103750/\n",
      "    üìÅ lebanon_explosion_202008/\n",
      "      üìÅ iss/\n",
      "      üìÅ planet/\n",
      "    üìÅ mexico_earthquake_202006/\n",
      "      üìÅ gsfc/\n",
      "    üìÅ mi_dam_failure_2020/\n",
      "      üìÅ aria/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ midwest_flooding_201903/\n",
      "      üìÅ landsat/\n",
      "      üìÅ modis/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ nashville_tornado_202003/\n",
      "    üìÅ nepal_15/\n",
      "      üìÅ 102001003AB04100/\n",
      "      üìÅ 102001003C9F6700/\n",
      "      üìÅ DNB/\n",
      "      üìÅ SEDAC/\n",
      "      üìÅ aster/\n",
      "      üìÅ eo1/\n",
      "      üìÅ landsat8/\n",
      "    üìÅ nishinoshima_volcano_20200624/\n",
      "    üìÅ puerto_rico_earthquake_202001/\n",
      "      üìÅ aria/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ viirs/\n",
      "    üìÅ quebec_flooding_2019/\n",
      "      üìÅ aria/\n",
      "    üìÅ raikoke_volcano_2019/\n",
      "      üìÅ airs/\n",
      "      üìÅ krotkov_ams/\n",
      "      üìÅ omps/\n",
      "    üìÅ se_severe_weather_spring_2020/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ test_landsat8_ESRItesting/\n",
      "      üìÅ LC08_L1TP_021035_20160306_20170224_01_T1/\n",
      "      üìÅ LC08_L1TP_021035_20160322_20170224_01_T1/\n",
      "      üìÅ LC08_L1TP_021036_20160306_20170224_01_T1/\n",
      "      üìÅ LC08_L1TP_021036_20160322_20170224_01_T1/\n",
      "      üìÅ LC08_L1TP_022035_20160329_20170223_01_T1/\n",
      "      üìÅ SNPP_VIIRS/\n",
      "    üìÅ tropicalstorm_barry_2019/\n",
      "      üìÅ aria_alos2/\n",
      "      üìÅ hurricane_barry.Overviews/\n",
      "      üìÅ landsat8/\n",
      "      üìÅ lis/\n",
      "      üìÅ modis/\n",
      "      üìÅ sentinel1/\n",
      "      üìÅ sentinel2/\n",
      "    üìÅ tropicalstorm_christobal_2020/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ tropicalstorm_karen_2019/\n",
      "      üìÅ viirs_dnb/\n",
      "    üìÅ typhoon_hagibis_2019/\n",
      "    üìÅ typhoon_kammuri_2019/\n",
      "      üìÅ aria/\n",
      "    üìÅ ulawun_volcano_2019/\n",
      "      üìÅ omps/\n",
      "    üìÅ uruguay_floods_201906/\n",
      "      üìÅ alos2/\n",
      "      üìÅ modis/\n",
      "      üìÅ sentinel1/\n",
      "    üìÅ utah_earthquake_202003/\n",
      "      üìÅ aria/\n",
      "      üìÅ sarviews/\n",
      "    üìÅ volcano_research/\n",
      "      üìÅ lewotolok_20201129/\n"
     ]
    }
   ],
   "source": [
    "# Display structure preview\n",
    "if 'result' in locals():\n",
    "    print(\"üå≥ Directory Structure Preview (max depth 3):\\n\")\n",
    "    print(f\"s3://{BUCKET_NAME}/\")\n",
    "    print_structure_preview(result['structure'], max_depth=3)\n",
    "else:\n",
    "    print(\"No results to display. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure.json\n",
      "üìä File size: 18,984,715 bytes\n",
      "\n",
      "üìù Structure saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure.json\n",
      "‚úÖ Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure_compact.json\n",
      "üìä File size: 11,176,941 bytes\n",
      "üìù Compact version saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/s3_tif_files_structure_compact.json\n"
     ]
    }
   ],
   "source": [
    "# Save the complete structure to JSON\n",
    "if 'result' in locals():\n",
    "    filepath = save_to_json(result, OUTPUT_FILE)\n",
    "    print(f\"\\nüìù Structure saved to: {filepath}\")\n",
    "    \n",
    "    # Option to save compact version (no indentation, smaller file)\n",
    "    compact_file = OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(result, compact_file, indent=None)\n",
    "    print(f\"üìù Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## 13. Generate Statistics and Export Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display statistics\n",
    "if 'result' in locals():\n",
    "    df_stats = get_statistics(result)\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    if not df_stats.empty:\n",
    "        stats_file = OUTPUT_FILE.replace('.json', '_statistics.csv')\n",
    "        df_stats.to_csv(stats_file, index=False)\n",
    "        print(f\"\\nüìä Statistics saved to: {stats_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample of .tif files:\")\n",
    "        display(df_stats.head(10))\n",
    "else:\n",
    "    print(\"No results to analyze. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total .tif files in bucket: 0\n",
      "\n",
      "First 10 file paths:\n",
      "\n",
      "üìÑ File list saved to: s3_tif_files_structure_file_list.txt\n"
     ]
    }
   ],
   "source": [
    "# Export flat list of all .tif file paths\n",
    "if 'result' in locals():\n",
    "    all_files = get_all_file_paths(result)\n",
    "    print(f\"\\nTotal .tif files in bucket: {len(all_files)}\")\n",
    "    print(\"\\nFirst 10 file paths:\")\n",
    "    for path in all_files[:10]:\n",
    "        print(f\"  s3://{BUCKET_NAME}/{path}\")\n",
    "    \n",
    "    # Save file list\n",
    "    file_list_path = OUTPUT_FILE.replace('.json', '_file_list.txt')\n",
    "    with open(file_list_path, 'w') as f:\n",
    "        for path in all_files:\n",
    "            f.write(f\"s3://{BUCKET_NAME}/{path}\\n\")\n",
    "    print(f\"\\nüìÑ File list saved to: {file_list_path}\")\n",
    "else:\n",
    "    print(\"No results to export. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## 14. Load and Search Previously Saved Results\n",
    "\n",
    "Use these functions to load and search through previously crawled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_structure(filename: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Load a previously saved JSON structure.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded structure from: {filename}\")\n",
    "    \n",
    "    if '_metadata' in data:\n",
    "        meta = data['_metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Bucket: {meta.get('bucket')}\")\n",
    "        print(f\"  Crawled at: {meta.get('crawled_at')}\")\n",
    "        print(f\"  Total .tif files: {meta.get('total_files')}\")\n",
    "        print(f\"  Total directories: {meta.get('total_directories')}\")\n",
    "        print(f\"  Total size: {meta.get('total_size_readable')}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def search_files(structure: Dict, pattern: str) -> list:\n",
    "    \"\"\"\n",
    "    Search for files matching a pattern in the structure.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    def search_recursive(obj):\n",
    "        for key, value in obj.items():\n",
    "            if isinstance(value, dict):\n",
    "                if '_files' in value:\n",
    "                    for file_info in value['_files']:\n",
    "                        if pattern.lower() in file_info['name'].lower():\n",
    "                            matches.append(file_info)\n",
    "                if '_subdirs' in value:\n",
    "                    search_recursive(value['_subdirs'])\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        search_recursive(structure['structure'])\n",
    "    else:\n",
    "        search_recursive(structure)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example: Load saved file\n",
    "# loaded_data = load_json_structure(OUTPUT_FILE)\n",
    "\n",
    "# Example: Search for specific files\n",
    "# matching_files = search_files(loaded_data, \"flood\")\n",
    "# print(f\"Found {len(matching_files)} files containing 'flood'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## 15. Crawl DRCS Activations Directory Only\n",
    "\n",
    "Focused crawler for disaster activation events with simplified output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Bucket: s3://nasa-disasters/drcs_activations/\n",
      "  File filter: .tif files only\n",
      "  Output file: drcs_activations_tif_files.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration for DRCS Activations\n",
    "BUCKET_NAME = \"nasa-disasters\"\n",
    "OUTPUT_FILE = \"drcs_activations_tif_files.json\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/drcs_activations/\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting focused crawl of drcs_activations...\n",
      "\n",
      "üîç Crawling s3://nasa-disasters/drcs_activations/\n",
      "üìå Filtering for .tif files in activation events\n",
      "Scanning activation events...\n",
      "  Found 34400 .tif files...\n",
      "‚úÖ Found 34460 .tif files\n",
      "üìÅ Across 42 activation events\n",
      "\n",
      "‚úÖ Crawl completed in 52.65 seconds\n",
      "\n",
      "Sample activation events:\n",
      "  - 2020\n",
      "  - 2021\n",
      "  - 2022\n",
      "  - 2023\n",
      "  - 202301_Flood_CA\n",
      "  - 202302_Earthquake_Turkiye\n",
      "  - 202305_Typhoon_Mawar\n",
      "  - 20230719_SevereWx_NC\n",
      "  - 202307_Fire_Greece\n",
      "  - 202307_Flood_VT\n",
      "  ... and 32 more\n"
     ]
    }
   ],
   "source": [
    "# Run the focused crawler\n",
    "if 'session' in locals() and session:\n",
    "    # Create S3 client from authenticated session\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    # Initialize the disasters crawler\n",
    "    disasters_crawler = S3DisastersCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl drcs_activations\n",
    "    print(\"üöÄ Starting focused crawl of drcs_activations...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform the crawl\n",
    "    drcs_result = disasters_crawler.crawl_drcs_activations(show_progress=True)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Show sample activation events\n",
    "    if '_metadata' in drcs_result:\n",
    "        events = drcs_result['_metadata']['activation_events'][:10]\n",
    "        print(f\"\\nSample activation events:\")\n",
    "        for event in events:\n",
    "            print(f\"  - {event}\")\n",
    "        if len(drcs_result['_metadata']['activation_events']) > 10:\n",
    "            print(f\"  ... and {len(drcs_result['_metadata']['activation_events']) - 10} more\")\n",
    "else:\n",
    "    print(\"‚ùå Please complete authentication setup first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Structure Preview:\n",
      "\n",
      "üìÅ 2020/\n",
      "  ‚îî‚îÄ‚îÄ aegean_sea_earthquake_202010/ (2 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ bolivia_fires/ (1 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ california_fires/ (43 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ colorado_fires/ (28 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_delta/ (574 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_sally/ (8 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_zeta/ (8 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricanes_eta_iota/ (535 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ mi_dam_failure_2020/ (22 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ neuse_river_nc/ (12 .tif files)\n",
      "  ‚îî‚îÄ‚îÄ volcano_research/ (0 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ washington_oregon_fires/ (1 .tif files in subdirs)\n",
      "\n",
      "üìÅ 2021/\n",
      "  ‚îî‚îÄ‚îÄ australia_floods/ (11 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ california_fires/ (1 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ costarica_panama_flooding/ (6 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ guyana_floods/ (26 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ haiti_earthquake_202108/ (6 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_elsa/ (174 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_ida/ (380 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ la_palma_eruption/ (2 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ la_soufriere_eruption/ (16 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ nepal_landslides_flooding_202106/ (1 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ north_carolina_hurrex/ (31 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ north_carolina_wildfire/ (3 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ pacificNW_flooding/ (81 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ peru_earthquake/ (1 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ satellogic_test/ (5 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ semeru_eruption/ (3 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ tornado_outbreak_20211210/ (40 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ ts_fred/ (3 .tif files in subdirs)\n",
      "\n",
      "üìÅ 2022/\n",
      "  ‚îî‚îÄ‚îÄ australia_flooding_202203/ (8 .tif files)\n",
      "  ‚îî‚îÄ‚îÄ bangladesh_flooding_202206/ (2 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ brazil_flooding_202205/ (3 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ fiona_msfc/ (2 .tif files)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_fiona_2022/ (65 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_ian_2022/ (152 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ hurricane_nicole_2022/ (4 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ ky_flooding_202207/ (23 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ mauna_loa_eruption_2022/ (4 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ southafrica_flooding_202204/ (1 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ tonga_volcano_tsunami_202201/ (117 .tif files in subdirs)\n",
      "  ‚îî‚îÄ‚îÄ yellowstone_np_flooding_202206/ (1 .tif files in subdirs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview the structure\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"üìä Structure Preview:\\n\")\n",
    "    \n",
    "    # Show first 3 activation events and their structure\n",
    "    events = list(drcs_result['drcs_activations'].keys())[:3]\n",
    "    \n",
    "    for event in events:\n",
    "        print(f\"üìÅ {event}/\")\n",
    "        event_data = drcs_result['drcs_activations'][event]\n",
    "        \n",
    "        # Show subdirectories and file counts\n",
    "        for subdir, content in event_data.items():\n",
    "            if isinstance(content, dict):\n",
    "                if '_files' in content:\n",
    "                    print(f\"  ‚îî‚îÄ‚îÄ {subdir}/ ({len(content['_files'])} .tif files)\")\n",
    "                else:\n",
    "                    # Count nested files\n",
    "                    total = sum(len(v['_files']) if isinstance(v, dict) and '_files' in v else 0 \n",
    "                               for v in content.values())\n",
    "                    print(f\"  ‚îî‚îÄ‚îÄ {subdir}/ ({total} .tif files in subdirs)\")\n",
    "            elif isinstance(content, list):\n",
    "                print(f\"  ‚îî‚îÄ‚îÄ {len(content)} .tif files directly in {event}/\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results to preview. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files.json\n",
      "üìä File size: 2,256,239 bytes\n",
      "\n",
      "üìù Simplified structure saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files.json\n",
      "‚úÖ Saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files_compact.json\n",
      "üìä File size: 1,701,317 bytes\n",
      "üìù Compact version saved to: /Users/klesinger/github/conversion_scripts/s3-crawler/drcs_activations_tif_files_compact.json\n"
     ]
    }
   ],
   "source": [
    "# Save the simplified structure\n",
    "if 'drcs_result' in locals():\n",
    "    filepath = save_to_json(drcs_result, OUTPUT_FILE)\n",
    "    print(f\"\\nüìù Simplified structure saved to: {filepath}\")\n",
    "    \n",
    "    # Also save a compact version\n",
    "    compact_file = OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(drcs_result, compact_file, indent=None)\n",
    "    print(f\"üìù Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Access Examples:\n",
      "\n",
      "Total activation events: 42\n",
      "\n",
      "Accessing files for '2020':\n",
      "  Total .tif files: 4384\n",
      "\n",
      "  First 5 file paths:\n",
      "    s3://nasa-disasters/drcs_activations/2020/aegean_sea_earthquake_202010/aria/ARIA_DPM_Sentinel-1_v0.3.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/aegean_sea_earthquake_202010/aria/S1_A131_20201030_20201024_disp_cm.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/bolivia_fires/aster/santacruz-nite-tif.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/california_fires/aria/ARIA_S1_DPM_CreekFire_Sep_13_Sep_19_7am.tif\n",
      "    s3://nasa-disasters/drcs_activations/2020/california_fires/aria/ARIA_S1_DPM_CreekFire_Sep_7_7am.tif\n"
     ]
    }
   ],
   "source": [
    "# Access example - show how to work with the data\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"üîç Access Examples:\\n\")\n",
    "    \n",
    "    # Get all activation events\n",
    "    all_events = list(drcs_result['drcs_activations'].keys())\n",
    "    print(f\"Total activation events: {len(all_events)}\")\n",
    "    \n",
    "    # Access files for a specific event\n",
    "    if all_events:\n",
    "        sample_event = all_events[0]\n",
    "        print(f\"\\nAccessing files for '{sample_event}':\")\n",
    "        \n",
    "        event_data = drcs_result['drcs_activations'][sample_event]\n",
    "        \n",
    "        # Count total files in this event\n",
    "        def count_files(obj):\n",
    "            total = 0\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    total += len(obj['_files'])\n",
    "                for value in obj.values():\n",
    "                    if isinstance(value, dict):\n",
    "                        total += count_files(value)\n",
    "            return total\n",
    "        \n",
    "        total_files = count_files(event_data)\n",
    "        print(f\"  Total .tif files: {total_files}\")\n",
    "        \n",
    "        # Show how to iterate through all files\n",
    "        def get_all_files(obj, prefix=''):\n",
    "            files = []\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    for f in obj['_files']:\n",
    "                        files.append(prefix + f)\n",
    "                for key, value in obj.items():\n",
    "                    if key != '_files' and isinstance(value, dict):\n",
    "                        files.extend(get_all_files(value, prefix + key + '/'))\n",
    "            return files\n",
    "        \n",
    "        all_files = get_all_files(event_data, f'{sample_event}/')\n",
    "        print(f\"\\n  First 5 file paths:\")\n",
    "        for f in all_files[:5]:\n",
    "            print(f\"    s3://nasa-disasters/drcs_activations/{f}\")\n",
    "else:\n",
    "    print(\"No results available. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Split DRCS Data by Year\n",
    "\n",
    "Automatically split the crawled DRCS activation data into separate files for each year (2020-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Year-splitting functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def extract_year_from_event_name(event_name):\n",
    "    \"\"\"Extract year from event name.\"\"\"\n",
    "    # Check if it's a plain year (e.g., \"2020\")\n",
    "    if event_name in [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]:\n",
    "        return int(event_name)\n",
    "    \n",
    "    # Check if it starts with a year (e.g., \"202301_Flood_CA\" or \"20230719_SevereWx_NC\")\n",
    "    if event_name[:4].isdigit():\n",
    "        year = int(event_name[:4])\n",
    "        if 2020 <= year <= 2025:\n",
    "            return year\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_files_recursive(data):\n",
    "    \"\"\"Count total .tif files in a nested structure.\"\"\"\n",
    "    total = 0\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        if '_files' in data:\n",
    "            total += len(data['_files'])\n",
    "        for key, value in data.items():\n",
    "            if key != '_files' and isinstance(value, dict):\n",
    "                total += count_files_recursive(value)\n",
    "    \n",
    "    return total\n",
    "\n",
    "def split_drcs_by_year(drcs_result):\n",
    "    \"\"\"\n",
    "    Split DRCS activation data into separate files by year.\n",
    "    \n",
    "    Args:\n",
    "        drcs_result: The crawled DRCS data dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping years to their data\n",
    "    \"\"\"\n",
    "    # Initialize year-based dictionaries\n",
    "    years_data = {year: {} for year in range(2020, 2026)}\n",
    "    \n",
    "    # Process each activation event\n",
    "    drcs_data = drcs_result.get('drcs_activations', {})\n",
    "    \n",
    "    for event_name, event_data in drcs_data.items():\n",
    "        year = extract_year_from_event_name(event_name)\n",
    "        \n",
    "        if year and year in years_data:\n",
    "            # If it's a plain year folder (e.g., \"2020\"), merge its contents\n",
    "            if event_name == str(year):\n",
    "                # This is a year folder, add all its contents\n",
    "                for sub_event, sub_data in event_data.items():\n",
    "                    years_data[year][sub_event] = sub_data\n",
    "            else:\n",
    "                # This is a named event (e.g., \"202301_Flood_CA\")\n",
    "                years_data[year][event_name] = event_data\n",
    "    \n",
    "    return years_data\n",
    "\n",
    "print(\"‚úÖ Year-splitting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Splitting DRCS data by year (2020-2025)...\n",
      "\n",
      "‚úÖ Created drcs_activations_2020.json\n",
      "   - Events: 12\n",
      "   - Total .tif files: 4384\n",
      "   - Sample events: ['aegean_sea_earthquake_202010', 'bolivia_fires', 'california_fires', '... and 9 more']\n",
      "\n",
      "‚úÖ Created drcs_activations_2021.json\n",
      "   - Events: 18\n",
      "   - Total .tif files: 2257\n",
      "   - Sample events: ['australia_floods', 'california_fires', 'costarica_panama_flooding', '... and 15 more']\n",
      "\n",
      "‚úÖ Created drcs_activations_2022.json\n",
      "   - Events: 12\n",
      "   - Total .tif files: 2223\n",
      "   - Sample events: ['australia_flooding_202203', 'bangladesh_flooding_202206', 'brazil_flooding_202205', '... and 9 more']\n",
      "\n",
      "‚úÖ Created drcs_activations_2023.json\n",
      "   - Events: 24\n",
      "   - Total .tif files: 3821\n",
      "   - Sample events: ['california_atmospheric_river', 'greece_wildfires', 'hawaii_wildfires_202308', '... and 21 more']\n",
      "\n",
      "‚úÖ Created drcs_activations_2024.json\n",
      "   - Events: 40\n",
      "   - Total .tif files: 8799\n",
      "   - Sample events: ['bangladesh_flood_202408', 'brasil_flood_202405', 'california_atmospheric_river_202402', '... and 37 more']\n",
      "\n",
      "‚úÖ Created drcs_activations_2025.json\n",
      "   - Events: 5\n",
      "   - Total .tif files: 2153\n",
      "   - Sample events: ['202501_Fire_CA', '202502_Flood_OhioValley', '202503_SevereWx_US', '... and 2 more']\n",
      "\n",
      "üìä Year-based splitting complete!\n"
     ]
    }
   ],
   "source": [
    "# Automatically split DRCS data by year after saving\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"\\nüìÖ Splitting DRCS data by year (2020-2025)...\\n\")\n",
    "    \n",
    "    # Split the data by year\n",
    "    years_data = split_drcs_by_year(drcs_result)\n",
    "    \n",
    "    # Create separate JSON files for each year\n",
    "    for year in range(2020, 2026):\n",
    "        output_file = f'drcs_activations_{year}.json'\n",
    "        \n",
    "        # Count statistics\n",
    "        event_count = len(years_data[year])\n",
    "        total_files = sum(count_files_recursive(event_data) \n",
    "                         for event_data in years_data[year].values())\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            \"drcs_activations\": years_data[year],\n",
    "            \"_metadata\": {\n",
    "                \"year\": year,\n",
    "                \"extracted_from\": OUTPUT_FILE,\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"total_events\": event_count,\n",
    "                \"total_tif_files\": total_files,\n",
    "                \"events\": sorted(list(years_data[year].keys()))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Created {output_file}\")\n",
    "        print(f\"   - Events: {event_count}\")\n",
    "        print(f\"   - Total .tif files: {total_files}\")\n",
    "        if event_count > 0:\n",
    "            sample_events = list(years_data[year].keys())[:3]\n",
    "            if event_count > 3:\n",
    "                sample_events.append(f\"... and {event_count - 3} more\")\n",
    "            print(f\"   - Sample events: {sample_events}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"üìä Year-based splitting complete!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No DRCS data available. Run the DRCS crawler first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
