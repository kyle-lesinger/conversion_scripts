{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS S3 Bucket Directory Crawler\n",
    "\n",
    "This notebook crawls through AWS S3 bucket directories and saves the complete structure to a JSON file.\n",
    "\n",
    "## Features:\n",
    "- Recursive traversal of S3 bucket structure\n",
    "- Nested JSON output format\n",
    "- File metadata collection (size, last modified, storage class)\n",
    "- Progress tracking for large buckets\n",
    "- Configurable depth and filtering options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "import sys\n",
    "!{sys.executable} -m pip install boto3 tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "import pandas as pd\n",
    "import fsspec\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AWS Configuration\n",
    "\n",
    "Make sure your AWS credentials are configured. You can set them using:\n",
    "- Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "- AWS CLI configuration (~/.aws/credentials)\n",
    "- IAM role (if running on EC2/Lambda)\n",
    "- Or configure them directly below (not recommended for production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initiate file systems for reading and (temporary) writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this code were re-used for a protected bucket, anon should be False. Disasters is a protected\n",
    "# bucket, so we set to False\n",
    "anon = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate file system\n",
    "fs_read = fsspec.filesystem(\"s3\", anon=anon, skip_instance_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 14 directories discovered from s3://nasa-disasters/*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://nasa-disasters/browseui',\n",
       " 's3://nasa-disasters/california_wildfires_202501',\n",
       " 's3://nasa-disasters/disasters',\n",
       " 's3://nasa-disasters/drcs_activations',\n",
       " 's3://nasa-disasters/event_testing',\n",
       " 's3://nasa-disasters/nrt',\n",
       " 's3://nasa-disasters/scripts',\n",
       " 's3://nasa-disasters/servir-esi',\n",
       " 's3://nasa-disasters/sport-lis',\n",
       " 's3://nasa-disasters/testing-access',\n",
       " 's3://nasa-disasters/tmp-cog-speed-test',\n",
       " 's3://nasa-disasters/tmp-cog-speed-test-preserved',\n",
       " 's3://nasa-disasters/tmp-vsm-soil-moisture',\n",
       " 's3://nasa-disasters/us_svrwx_202504']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available files within the nasa-disasters bucket\n",
    "s3_path = f\"s3://nasa-disasters/*\"\n",
    "\n",
    "all_dirs = sorted([\"s3://\" + f for f in fs_read.glob(s3_path)])\n",
    "print(f\"Discovered {len(all_dirs)} directories discovered from {s3_path}\")\n",
    "all_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 43 directories discovered from s3://nasa-disasters/drcs_activations/*\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://nasa-disasters/drcs_activations/2020',\n",
       " 's3://nasa-disasters/drcs_activations/2021',\n",
       " 's3://nasa-disasters/drcs_activations/2022',\n",
       " 's3://nasa-disasters/drcs_activations/2023',\n",
       " 's3://nasa-disasters/drcs_activations/202301_Flood_CA',\n",
       " 's3://nasa-disasters/drcs_activations/202302_Earthquake_Turkiye',\n",
       " 's3://nasa-disasters/drcs_activations/202305_Typhoon_Mawar',\n",
       " 's3://nasa-disasters/drcs_activations/20230719_SevereWx_NC',\n",
       " 's3://nasa-disasters/drcs_activations/202307_Fire_Greece',\n",
       " 's3://nasa-disasters/drcs_activations/202307_Flood_VT',\n",
       " 's3://nasa-disasters/drcs_activations/202308_Hurricane_Hilary',\n",
       " 's3://nasa-disasters/drcs_activations/202309_Earthquake_Morocco',\n",
       " 's3://nasa-disasters/drcs_activations/202309_Hurricane_Idalia',\n",
       " 's3://nasa-disasters/drcs_activations/202310_Hurricane_Otis',\n",
       " 's3://nasa-disasters/drcs_activations/202312_Flood_NewEngland',\n",
       " 's3://nasa-disasters/drcs_activations/2024',\n",
       " 's3://nasa-disasters/drcs_activations/202401_SevereWx_SoutheastUS',\n",
       " 's3://nasa-disasters/drcs_activations/202402_Fire_Chile',\n",
       " 's3://nasa-disasters/drcs_activations/202402_Fire_Guatemala',\n",
       " 's3://nasa-disasters/drcs_activations/202402_Fire_TX',\n",
       " 's3://nasa-disasters/drcs_activations/202402_Flood_CA',\n",
       " 's3://nasa-disasters/drcs_activations/202405_Flood_Brasil',\n",
       " 's3://nasa-disasters/drcs_activations/202405_Flood_TX',\n",
       " 's3://nasa-disasters/drcs_activations/202405_Heat_TX',\n",
       " 's3://nasa-disasters/drcs_activations/202406_Flood_IA',\n",
       " 's3://nasa-disasters/drcs_activations/202407_Hurricane_Beryl',\n",
       " 's3://nasa-disasters/drcs_activations/202408_Flood_Bangladesh',\n",
       " 's3://nasa-disasters/drcs_activations/202408_Flood_Nepal',\n",
       " 's3://nasa-disasters/drcs_activations/202408_TropicalStorm_Debby',\n",
       " 's3://nasa-disasters/drcs_activations/202408_TropicalStorm_Ernesto',\n",
       " 's3://nasa-disasters/drcs_activations/202409_Hurricane_Helene',\n",
       " 's3://nasa-disasters/drcs_activations/202409_TropicalStorm_Francine',\n",
       " 's3://nasa-disasters/drcs_activations/202410_Hurricane_Milton',\n",
       " 's3://nasa-disasters/drcs_activations/202410_Landslide_PalosVerdes',\n",
       " 's3://nasa-disasters/drcs_activations/202411_TropicalStorm_Sara',\n",
       " 's3://nasa-disasters/drcs_activations/202412_Earthquake_CA',\n",
       " 's3://nasa-disasters/drcs_activations/202501_Fire_CA',\n",
       " 's3://nasa-disasters/drcs_activations/202502_Flood_OhioValley',\n",
       " 's3://nasa-disasters/drcs_activations/202503_SevereWx_US',\n",
       " 's3://nasa-disasters/drcs_activations/202504_SevereWx_US',\n",
       " 's3://nasa-disasters/drcs_activations/202506_Fire_NM',\n",
       " 's3://nasa-disasters/drcs_activations/202507_Flood_TX',\n",
       " 's3://nasa-disasters/drcs_activations/AdHoc_Disaster_products']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List available files within the drcs_activations directory\n",
    "s3_path = f\"s3://nasa-disasters/drcs_activations/*\"\n",
    "\n",
    "all_dirs = sorted([\"s3://\" + f for f in fs_read.glob(s3_path)])\n",
    "print(f\"Discovered {len(all_dirs)} directories discovered from {s3_path}\")\n",
    "all_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Initialize AWS S3 Client\n",
    "\n",
    "Create the S3 client for the crawler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create S3 client using default credentials or environment variables\ntry:\n    # Create S3 client - will use AWS credentials from:\n    # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n    # 2. AWS CLI configuration (~/.aws/credentials)\n    # 3. IAM role (if running on EC2/Lambda)\n    s3_client = boto3.client('s3')\n    \n    # Try to test the connection with a simple operation\n    # Note: We don't need ListBuckets permission, just access to the specific bucket\n    try:\n        # Try to list buckets (this might fail due to permissions)\n        response = s3_client.list_buckets()\n        print(f\"✅ S3 client initialized successfully\")\n        print(f\"   Found {len(response.get('Buckets', []))} accessible buckets\")\n    except ClientError as e:\n        # If ListBuckets fails, that's OK - we might still have access to specific buckets\n        if e.response['Error']['Code'] == 'AccessDenied':\n            print(f\"⚠️ S3 client initialized but cannot list all buckets (this is OK)\")\n            print(f\"   Will attempt to access specific bucket: nasa-disasters\")\n            # Test access to the specific bucket we need\n            try:\n                s3_client.head_bucket(Bucket='nasa-disasters')\n                print(f\"✅ Confirmed access to nasa-disasters bucket\")\n            except ClientError as bucket_error:\n                print(f\"❌ Cannot access nasa-disasters bucket: {bucket_error}\")\n        else:\n            raise e\n    \nexcept NoCredentialsError:\n    print(\"❌ No AWS credentials found. Please configure credentials using:\")\n    print(\"   - Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\")\n    print(\"   - AWS CLI: aws configure\")\n    print(\"   - IAM role (if on EC2)\")\n    s3_client = None\nexcept Exception as e:\n    print(f\"❌ Error initializing S3 client: {e}\")\n    s3_client = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S3 Bucket Crawler Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S3BucketCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class S3BucketCrawler:\n",
    "    \"\"\"\n",
    "    Crawls S3 bucket structure and creates a nested dictionary of .tif files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories = set()\n",
    "        \n",
    "    def build_nested_structure(self, file_list: list) -> Dict:\n",
    "        \"\"\"\n",
    "        Convert flat S3 paths to nested dictionary structure.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of dictionaries containing file information\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing directory structure\n",
    "        \"\"\"\n",
    "        root = {}\n",
    "        \n",
    "        for file_info in file_list:\n",
    "            path_parts = file_info['key'].split('/')\n",
    "            current_level = root\n",
    "            \n",
    "            # Navigate/create the directory structure\n",
    "            for i, part in enumerate(path_parts[:-1]):\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {\n",
    "                        '_type': 'directory',\n",
    "                        '_path': '/'.join(path_parts[:i+1]) + '/',\n",
    "                        '_files': [],\n",
    "                        '_subdirs': {}\n",
    "                    }\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "                else:\n",
    "                    current_level = current_level[part]['_subdirs']\n",
    "            \n",
    "            # Add the file to its directory\n",
    "            file_name = path_parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            \n",
    "            current_level['_files'].append({\n",
    "                'name': file_name,\n",
    "                'path': file_info['key'],\n",
    "                'size_bytes': file_info['size'],\n",
    "                'size_readable': self._format_size(file_info['size']),\n",
    "                'last_modified': file_info['last_modified'],\n",
    "                'storage_class': file_info.get('storage_class', 'STANDARD')\n",
    "            })\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def crawl(self, \n",
    "              prefix: str = '', \n",
    "              show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the bucket and return nested structure of .tif files only.\n",
    "        \n",
    "        Args:\n",
    "            prefix: Start from this prefix (subdirectory)\n",
    "            show_progress: Show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary representing bucket structure with .tif files\n",
    "        \"\"\"\n",
    "        print(f\"🔍 Starting crawl of s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"📌 Filtering for .tif files only\")\n",
    "        \n",
    "        # Reset statistics\n",
    "        self.total_files = 0\n",
    "        self.total_size = 0\n",
    "        self.total_directories.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        all_tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning bucket...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        self.total_files += 1\n",
    "                        self.total_size += obj.get('Size', 0)\n",
    "                        \n",
    "                        # Track directories\n",
    "                        dir_path = '/'.join(key.split('/')[:-1])\n",
    "                        if dir_path:\n",
    "                            self.total_directories.add(dir_path)\n",
    "                        \n",
    "                        # Add file info\n",
    "                        all_tif_files.append({\n",
    "                            'key': key,\n",
    "                            'size': obj.get('Size', 0),\n",
    "                            'last_modified': obj.get('LastModified').isoformat() if obj.get('LastModified') else None,\n",
    "                            'storage_class': obj.get('StorageClass', 'STANDARD')\n",
    "                        })\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n✅ Found {self.total_files} .tif files\")\n",
    "        print(f\"📁 Across {len(self.total_directories)} directories\")\n",
    "        print(f\"💾 Total size: {self._format_size(self.total_size)}\")\n",
    "        \n",
    "        # Build nested structure\n",
    "        structure = self.build_nested_structure(all_tif_files)\n",
    "        \n",
    "        # Create final result with metadata\n",
    "        result = {\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"prefix\": prefix,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"file_filter\": \".tif\",\n",
    "                \"total_files\": self.total_files,\n",
    "                \"total_directories\": len(self.total_directories),\n",
    "                \"total_size_bytes\": self.total_size,\n",
    "                \"total_size_readable\": self._format_size(self.total_size)\n",
    "            },\n",
    "            \"structure\": structure\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _format_size(self, size_bytes: int) -> str:\n",
    "        \"\"\"Format bytes to human-readable size.\"\"\"\n",
    "        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "            if size_bytes < 1024.0:\n",
    "                return f\"{size_bytes:.2f} {unit}\"\n",
    "            size_bytes /= 1024.0\n",
    "        return f\"{size_bytes:.2f} PB\"\n",
    "\n",
    "print(\"✅ S3BucketCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ S3DisastersCrawler class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class S3DisastersCrawler:\n",
    "    \"\"\"\n",
    "    Simplified crawler for drcs_activations directory that creates clean nested structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, s3_client=None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the S3 bucket\n",
    "            s3_client: Boto3 S3 client (creates new one if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name.replace('s3://', '').rstrip('/')\n",
    "        self.s3_client = s3_client or boto3.client('s3')\n",
    "        self.total_files = 0\n",
    "        self.activation_events = set()\n",
    "        \n",
    "    def build_clean_structure(self, file_list: list, prefix: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Build a clean nested structure organized by activation events.\n",
    "        \n",
    "        Args:\n",
    "            file_list: List of S3 keys\n",
    "            prefix: The prefix to remove (e.g., 'drcs_activations/')\n",
    "            \n",
    "        Returns:\n",
    "            Nested dictionary with activation events and their files\n",
    "        \"\"\"\n",
    "        structure = {}\n",
    "        \n",
    "        for key in file_list:\n",
    "            # Remove the prefix to get relative path\n",
    "            relative_path = key.replace(prefix, '', 1) if key.startswith(prefix) else key\n",
    "            parts = relative_path.split('/')\n",
    "            \n",
    "            # Skip if not enough parts\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            # First part is the activation event (e.g., '202301_Flood_CA')\n",
    "            activation_event = parts[0]\n",
    "            self.activation_events.add(activation_event)\n",
    "            \n",
    "            # Initialize activation event if not exists\n",
    "            if activation_event not in structure:\n",
    "                structure[activation_event] = {}\n",
    "            \n",
    "            # Build nested structure for remaining parts\n",
    "            current_level = structure[activation_event]\n",
    "            \n",
    "            # Navigate through subdirectories\n",
    "            for part in parts[1:-1]:\n",
    "                if part not in current_level:\n",
    "                    current_level[part] = {}\n",
    "                # Check if current_level[part] is a list (files), if so convert to dict\n",
    "                if isinstance(current_level[part], list):\n",
    "                    current_level[part] = {'_files': current_level[part]}\n",
    "                current_level = current_level[part]\n",
    "            \n",
    "            # Add the file\n",
    "            filename = parts[-1]\n",
    "            if '_files' not in current_level:\n",
    "                current_level['_files'] = []\n",
    "            current_level['_files'].append(filename)\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def crawl_drcs_activations(self, show_progress: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl only the drcs_activations directory for .tif files.\n",
    "        \n",
    "        Args:\n",
    "            show_progress: Show progress during crawl\n",
    "            \n",
    "        Returns:\n",
    "            Clean nested dictionary of activation events and their .tif files\n",
    "        \"\"\"\n",
    "        prefix = 'drcs_activations/'\n",
    "        print(f\"🔍 Crawling s3://{self.bucket_name}/{prefix}\")\n",
    "        print(\"📌 Filtering for .tif files in activation events\")\n",
    "        \n",
    "        # Reset counters\n",
    "        self.total_files = 0\n",
    "        self.activation_events.clear()\n",
    "        \n",
    "        # Collect all .tif files\n",
    "        tif_files = []\n",
    "        \n",
    "        # Use paginator for large buckets\n",
    "        paginator = self.s3_client.get_paginator('list_objects_v2')\n",
    "        page_iterator = paginator.paginate(\n",
    "            Bucket=self.bucket_name,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        # Process pages\n",
    "        print(\"Scanning activation events...\")\n",
    "        for page in page_iterator:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    key = obj['Key']\n",
    "                    \n",
    "                    # Filter for .tif files only\n",
    "                    if key.lower().endswith('.tif'):\n",
    "                        tif_files.append(key)\n",
    "                        self.total_files += 1\n",
    "                        \n",
    "                        # Show progress\n",
    "                        if show_progress and self.total_files % 100 == 0:\n",
    "                            print(f\"  Found {self.total_files} .tif files...\", end=\"\\r\")\n",
    "        \n",
    "        print(f\"\\n✅ Found {self.total_files} .tif files\")\n",
    "        \n",
    "        # Build clean structure\n",
    "        structure = self.build_clean_structure(tif_files, prefix)\n",
    "        \n",
    "        print(f\"📁 Across {len(self.activation_events)} activation events\")\n",
    "        \n",
    "        # Create result with drcs_activations as root\n",
    "        result = {\n",
    "            \"drcs_activations\": structure,\n",
    "            \"_metadata\": {\n",
    "                \"bucket\": self.bucket_name,\n",
    "                \"crawled_at\": datetime.now().isoformat(),\n",
    "                \"total_tif_files\": self.total_files,\n",
    "                \"total_activation_events\": len(self.activation_events),\n",
    "                \"activation_events\": sorted(list(self.activation_events))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✅ S3DisastersCrawler class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_to_json(data: Dict, filename: str, indent: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Save dictionary to JSON file.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary to save\n",
    "        filename: Output filename\n",
    "        indent: JSON indentation (None for compact)\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    filepath = os.path.abspath(filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=indent, default=str)\n",
    "    \n",
    "    file_size = os.path.getsize(filepath)\n",
    "    print(f\"✅ Saved to: {filepath}\")\n",
    "    print(f\"📊 File size: {file_size:,} bytes\")\n",
    "    \n",
    "    return filepath\n",
    "\n",
    "def print_structure_preview(structure: Dict, max_depth: int = 3, current_depth: int = 0, prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Print a tree-like preview of the structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Nested dictionary structure\n",
    "        max_depth: Maximum depth to display\n",
    "        current_depth: Current recursion depth\n",
    "        prefix: Prefix for tree display\n",
    "    \"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    # Skip metadata in display\n",
    "    items = [(k, v) for k, v in structure.items() if not k.startswith('_')]\n",
    "    \n",
    "    for i, (key, value) in enumerate(items[:20]):  # Limit to 20 items per level\n",
    "        is_last = i == len(items) - 1\n",
    "        \n",
    "        # Determine if it's a file or directory\n",
    "        if isinstance(value, dict):\n",
    "            if value.get('_type') == 'file':\n",
    "                icon = \"📄\"\n",
    "                size = value.get('size_readable', '')\n",
    "                print(f\"{prefix}{'└── ' if is_last else '├── '}{icon} {key} ({size})\")\n",
    "            elif value.get('_type') == 'directory':\n",
    "                icon = \"📁\"\n",
    "                print(f\"{prefix}{'└── ' if is_last else '├── '}{icon} {key}/\")\n",
    "                \n",
    "                # Recurse into directory\n",
    "                new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                if 'contents' in value:\n",
    "                    print_structure_preview(value['contents'], max_depth, current_depth + 1, new_prefix)\n",
    "            else:\n",
    "                print(f\"{prefix}{'└── ' if is_last else '├── '}{key}\")\n",
    "                new_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "                print_structure_preview(value, max_depth, current_depth + 1, new_prefix)\n",
    "    \n",
    "    if len(items) > 20:\n",
    "        print(f\"{prefix}... and {len(items) - 20} more items\")\n",
    "\n",
    "def get_statistics(structure: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate statistics from crawled structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: Crawled structure dictionary\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with statistics\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    \n",
    "    def analyze_recursive(obj, path=\"\"):\n",
    "        for key, value in obj.items():\n",
    "            if key.startswith('_'):\n",
    "                continue\n",
    "                \n",
    "            if isinstance(value, dict):\n",
    "                if value.get('_type') == 'file':\n",
    "                    stats.append({\n",
    "                        'type': 'file',\n",
    "                        'path': value.get('_path', ''),\n",
    "                        'name': key,\n",
    "                        'size_bytes': value.get('size_bytes', 0),\n",
    "                        'extension': os.path.splitext(key)[1]\n",
    "                    })\n",
    "                elif value.get('_type') == 'directory':\n",
    "                    stats.append({\n",
    "                        'type': 'directory',\n",
    "                        'path': value.get('_path', ''),\n",
    "                        'name': key,\n",
    "                        'size_bytes': 0,\n",
    "                        'extension': ''\n",
    "                    })\n",
    "                    if 'contents' in value:\n",
    "                        analyze_recursive(value['contents'], value.get('_path', ''))\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        analyze_recursive(structure['structure'])\n",
    "    else:\n",
    "        analyze_recursive(structure)\n",
    "    \n",
    "    df = pd.DataFrame(stats)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Add summary\n",
    "        print(\"\\n📊 Statistics:\")\n",
    "        print(f\"Total files: {len(df[df['type'] == 'file'])}\")\n",
    "        print(f\"Total directories: {len(df[df['type'] == 'directory'])}\")\n",
    "        print(f\"Total size: {df['size_bytes'].sum():,} bytes\")\n",
    "        \n",
    "        # File extensions\n",
    "        if len(df[df['type'] == 'file']) > 0:\n",
    "            ext_counts = df[df['type'] == 'file']['extension'].value_counts().head(10)\n",
    "            print(\"\\nTop file extensions:\")\n",
    "            print(ext_counts)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✅ Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Bucket: s3://nasa-disasters/drcs_activations/\n",
      "  File filter: .tif files only\n",
      "  Output file: s3_tif_files_structure.json\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = \"nasa-disasters\"  # Change this to your target bucket\n",
    "PREFIX = \"drcs_activations/\"     # Leave empty for entire bucket, or specify a path like \"drcs_activations/\"\n",
    "OUTPUT_FILE = \"s3_tif_files_structure.json\"  # Output filename\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/{PREFIX}\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ S3 client not initialized. Please check your AWS credentials above.\n"
     ]
    }
   ],
   "source": [
    "# Initialize crawler\n",
    "if s3_client is None:\n",
    "    print(\"❌ S3 client not initialized. Please check your AWS credentials above.\")\n",
    "else:\n",
    "    crawler = S3BucketCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    try:\n",
    "        print(\"🚀 Starting crawl... This may take a while for large buckets.\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Perform the crawl (now filters for .tif files only)\n",
    "        result = crawler.crawl(\n",
    "            prefix=PREFIX,\n",
    "            show_progress=True\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n✅ Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchBucket':\n",
    "            print(f\"❌ Bucket '{BUCKET_NAME}' does not exist or you don't have access.\")\n",
    "        elif e.response['Error']['Code'] == 'AccessDenied':\n",
    "            print(f\"❌ Access denied to bucket '{BUCKET_NAME}'. Check your permissions.\")\n",
    "        else:\n",
    "            print(f\"❌ AWS Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during crawl: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is now redundant - see the cell above for the updated crawler initialization\n",
    "# The crawler now properly checks if s3_client exists before attempting to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preview Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to display. Run the crawl first.\n"
     ]
    }
   ],
   "source": [
    "# Display tree preview\n",
    "if 'result' in locals():\n",
    "    print(\"\\n🌳 Directory Structure Preview (max depth 3):\\n\")\n",
    "    print(f\"s3://{BUCKET_NAME}/\")\n",
    "    print_structure_preview(result['structure'], max_depth=3)\n",
    "else:\n",
    "    print(\"No results to display. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to save. Run the crawl first.\n"
     ]
    }
   ],
   "source": [
    "# Save the complete structure to JSON\n",
    "if 'result' in locals():\n",
    "    filepath = save_to_json(result, OUTPUT_FILE)\n",
    "    print(f\"\\n📝 Structure saved to: {filepath}\")\n",
    "    \n",
    "    # Option to save compact version (no indentation, smaller file)\n",
    "    compact_file = OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(result, compact_file, indent=None)\n",
    "    print(f\"📝 Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No results to save. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to analyze. Run the crawl first.\n"
     ]
    }
   ],
   "source": [
    "# Generate and display statistics\n",
    "if 'result' in locals():\n",
    "    df_stats = get_statistics(result)\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    if not df_stats.empty:\n",
    "        stats_file = OUTPUT_FILE.replace('.json', '_statistics.csv')\n",
    "        df_stats.to_csv(stats_file, index=False)\n",
    "        print(f\"\\n📊 Statistics saved to: {stats_file}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nSample of files:\")\n",
    "        display(df_stats[df_stats['type'] == 'file'].head())\n",
    "else:\n",
    "    print(\"No results to analyze. Run the crawl first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function defined: crawl_specific_files()\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Crawl with specific file extensions only\n",
    "def crawl_specific_files(bucket_name, extensions):\n",
    "    \"\"\"\n",
    "    Crawl only specific file types.\n",
    "    \"\"\"\n",
    "    crawler = S3BucketCrawler(bucket_name, s3_client)\n",
    "    \n",
    "    result = crawler.crawl(\n",
    "        prefix=\"\",\n",
    "        max_depth=3,  # Limit depth for faster results\n",
    "        include_metadata=True,\n",
    "        file_extensions=extensions,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Find only GeoTIFF and NetCDF files\n",
    "# geospatial_result = crawl_specific_files(\"nasa-disasters\", ['.tif', '.tiff', '.nc', '.nc4'])\n",
    "print(\"Function defined: crawl_specific_files()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the main crawl first to search.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Search for specific patterns in the structure\n",
    "def search_structure(structure, pattern):\n",
    "    \"\"\"\n",
    "    Search for files/directories matching a pattern.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    def search_recursive(obj, current_path=\"\"):\n",
    "        for key, value in obj.items():\n",
    "            if key.startswith('_'):\n",
    "                continue\n",
    "            \n",
    "            if pattern.lower() in key.lower():\n",
    "                matches.append({\n",
    "                    'name': key,\n",
    "                    'path': value.get('_path', current_path + key),\n",
    "                    'type': value.get('_type', 'unknown')\n",
    "                })\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                if 'contents' in value:\n",
    "                    search_recursive(value['contents'], value.get('_path', ''))\n",
    "                elif not value.get('_type'):\n",
    "                    search_recursive(value, current_path + key + '/')\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        search_recursive(structure['structure'])\n",
    "    else:\n",
    "        search_recursive(structure)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example usage:\n",
    "if 'result' in locals():\n",
    "    # Search for items containing \"flood\"\n",
    "    flood_items = search_structure(result, \"flood\")\n",
    "    print(f\"Found {len(flood_items)} items containing 'flood':\")\n",
    "    for item in flood_items[:5]:  # Show first 5\n",
    "        print(f\"  {item['type']}: {item['name']}\")\n",
    "else:\n",
    "    print(\"Run the main crawl first to search.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the main crawl first to extract file paths.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Create a flat list of all file paths\n",
    "def get_all_file_paths(structure):\n",
    "    \"\"\"\n",
    "    Extract all file paths from nested structure.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    def extract_recursive(obj):\n",
    "        for key, value in obj.items():\n",
    "            if key.startswith('_'):\n",
    "                continue\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                if value.get('_type') == 'file':\n",
    "                    file_paths.append(value.get('_path', key))\n",
    "                elif 'contents' in value:\n",
    "                    extract_recursive(value['contents'])\n",
    "                else:\n",
    "                    extract_recursive(value)\n",
    "    \n",
    "    if 'structure' in structure:\n",
    "        extract_recursive(structure['structure'])\n",
    "    else:\n",
    "        extract_recursive(structure)\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# Example usage:\n",
    "if 'result' in locals():\n",
    "    all_files = get_all_file_paths(result)\n",
    "    print(f\"\\nTotal files in bucket: {len(all_files)}\")\n",
    "    print(\"\\nFirst 10 file paths:\")\n",
    "    for path in all_files[:10]:\n",
    "        print(f\"  s3://{BUCKET_NAME}/{path}\")\n",
    "    \n",
    "    # Save file list\n",
    "    file_list_path = OUTPUT_FILE.replace('.json', '_file_list.txt')\n",
    "    with open(file_list_path, 'w') as f:\n",
    "        for path in all_files:\n",
    "            f.write(f\"s3://{BUCKET_NAME}/{path}\\n\")\n",
    "    print(f\"\\n📄 File list saved to: {file_list_path}\")\n",
    "else:\n",
    "    print(\"Run the main crawl first to extract file paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load and Explore Saved JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load previously saved JSON file\ndef load_json_structure(filename):\n    \"\"\"\n    Load a previously saved JSON structure.\n    \"\"\"\n    with open(filename, 'r', encoding='utf-8') as f:\n        data = json.load(f)\n    \n    print(f\"✅ Loaded structure from: {filename}\")\n    \n    if '_metadata' in data:\n        meta = data['_metadata']\n        print(f\"\\nMetadata:\")\n        print(f\"  Bucket: {meta.get('bucket')}\")\n        print(f\"  Crawled at: {meta.get('crawled_at')}\")\n        print(f\"  Total files: {meta.get('total_files')}\")\n        print(f\"  Total directories: {meta.get('total_directories')}\")\n        print(f\"  Total size: {meta.get('total_size_readable')}\")\n    \n    return data\n\n# Load the saved file\n# Make sure OUTPUT_FILE is defined, or use a default\nif 'OUTPUT_FILE' not in locals():\n    OUTPUT_FILE = \"s3_tif_files_structure.json\"\n    \nif os.path.exists(OUTPUT_FILE):\n    loaded_data = load_json_structure(OUTPUT_FILE)\nelse:\n    print(f\"File {OUTPUT_FILE} not found. Run the crawl first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. DRCS Activations - Focused Disaster Event Crawling\n",
    "\n",
    "Use the S3DisastersCrawler for a simplified, organized view of disaster activation events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for DRCS Activations\n",
    "DRCS_OUTPUT_FILE = \"drcs_activations_tif_files.json\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Bucket: s3://{BUCKET_NAME}/drcs_activations/\")\n",
    "print(f\"  File filter: .tif files only\")\n",
    "print(f\"  Output file: {DRCS_OUTPUT_FILE}\")\n",
    "\n",
    "if s3_client is None:\n",
    "    print(\"\\n❌ S3 client not initialized. Please check your AWS credentials above.\")\n",
    "else:\n",
    "    # Initialize the disasters crawler\n",
    "    disasters_crawler = S3DisastersCrawler(BUCKET_NAME, s3_client)\n",
    "    \n",
    "    # Crawl drcs_activations\n",
    "    print(\"\\n🚀 Starting focused crawl of drcs_activations...\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Perform the crawl\n",
    "    drcs_result = disasters_crawler.crawl_drcs_activations(show_progress=True)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n✅ Crawl completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Show sample activation events\n",
    "    if '_metadata' in drcs_result:\n",
    "        events = drcs_result['_metadata']['activation_events'][:10]\n",
    "        print(f\"\\nSample activation events:\")\n",
    "        for event in events:\n",
    "            print(f\"  - {event}\")\n",
    "        if len(drcs_result['_metadata']['activation_events']) > 10:\n",
    "            print(f\"  ... and {len(drcs_result['_metadata']['activation_events']) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the structure\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"📊 Structure Preview:\\n\")\n",
    "    \n",
    "    # Show first 3 activation events and their structure\n",
    "    events = list(drcs_result['drcs_activations'].keys())[:3]\n",
    "    \n",
    "    for event in events:\n",
    "        print(f\"📁 {event}/\")\n",
    "        event_data = drcs_result['drcs_activations'][event]\n",
    "        \n",
    "        # Show subdirectories and file counts\n",
    "        for subdir, content in event_data.items():\n",
    "            if isinstance(content, dict):\n",
    "                if '_files' in content:\n",
    "                    print(f\"  └── {subdir}/ ({len(content['_files'])} .tif files)\")\n",
    "                else:\n",
    "                    # Count nested files\n",
    "                    total = sum(len(v['_files']) if isinstance(v, dict) and '_files' in v else 0 \n",
    "                               for v in content.values())\n",
    "                    print(f\"  └── {subdir}/ ({total} .tif files in subdirs)\")\n",
    "            elif isinstance(content, list):\n",
    "                print(f\"  └── {len(content)} .tif files directly in {event}/\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No DRCS results to preview. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DRCS structure\n",
    "if 'drcs_result' in locals():\n",
    "    filepath = save_to_json(drcs_result, DRCS_OUTPUT_FILE)\n",
    "    print(f\"\\n📝 DRCS structure saved to: {filepath}\")\n",
    "    \n",
    "    # Also save a compact version\n",
    "    compact_file = DRCS_OUTPUT_FILE.replace('.json', '_compact.json')\n",
    "    compact_path = save_to_json(drcs_result, compact_file, indent=None)\n",
    "    print(f\"📝 Compact version saved to: {compact_path}\")\n",
    "else:\n",
    "    print(\"No DRCS results to save. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access example - show how to work with the DRCS data\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"🔍 Access Examples:\\n\")\n",
    "    \n",
    "    # Get all activation events\n",
    "    all_events = list(drcs_result['drcs_activations'].keys())\n",
    "    print(f\"Total activation events: {len(all_events)}\")\n",
    "    \n",
    "    # Access files for a specific event\n",
    "    if all_events:\n",
    "        sample_event = all_events[0]\n",
    "        print(f\"\\nAccessing files for '{sample_event}':\")\n",
    "        \n",
    "        event_data = drcs_result['drcs_activations'][sample_event]\n",
    "        \n",
    "        # Count total files in this event\n",
    "        def count_files(obj):\n",
    "            total = 0\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    total += len(obj['_files'])\n",
    "                for value in obj.values():\n",
    "                    if isinstance(value, dict):\n",
    "                        total += count_files(value)\n",
    "            return total\n",
    "        \n",
    "        total_files = count_files(event_data)\n",
    "        print(f\"  Total .tif files: {total_files}\")\n",
    "        \n",
    "        # Show how to iterate through all files\n",
    "        def get_all_files(obj, prefix=''):\n",
    "            files = []\n",
    "            if isinstance(obj, dict):\n",
    "                if '_files' in obj:\n",
    "                    for f in obj['_files']:\n",
    "                        files.append(prefix + f)\n",
    "                for key, value in obj.items():\n",
    "                    if key != '_files' and isinstance(value, dict):\n",
    "                        files.extend(get_all_files(value, prefix + key + '/'))\n",
    "            return files\n",
    "        \n",
    "        all_files = get_all_files(event_data, f'{sample_event}/')\n",
    "        print(f\"\\n  First 5 file paths:\")\n",
    "        for f in all_files[:5]:\n",
    "            print(f\"    s3://nasa-disasters/drcs_activations/{f}\")\n",
    "else:\n",
    "    print(\"No DRCS results available. Run the crawler first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Split DRCS Data by Year\n",
    "\n",
    "Automatically split the crawled DRCS activation data into separate files for each year (2020-2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year_from_event_name(event_name):\n",
    "    \"\"\"Extract year from event name.\"\"\"\n",
    "    # Check if it's a plain year (e.g., \"2020\")\n",
    "    if event_name in [\"2020\", \"2021\", \"2022\", \"2023\", \"2024\", \"2025\"]:\n",
    "        return int(event_name)\n",
    "    \n",
    "    # Check if it starts with a year (e.g., \"202301_Flood_CA\" or \"20230719_SevereWx_NC\")\n",
    "    if event_name[:4].isdigit():\n",
    "        year = int(event_name[:4])\n",
    "        if 2020 <= year <= 2025:\n",
    "            return year\n",
    "    \n",
    "    return None\n",
    "\n",
    "def count_files_recursive(data):\n",
    "    \"\"\"Count total .tif files in a nested structure.\"\"\"\n",
    "    total = 0\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        if '_files' in data:\n",
    "            total += len(data['_files'])\n",
    "        for key, value in data.items():\n",
    "            if key != '_files' and isinstance(value, dict):\n",
    "                total += count_files_recursive(value)\n",
    "    \n",
    "    return total\n",
    "\n",
    "def split_drcs_by_year(drcs_result):\n",
    "    \"\"\"\n",
    "    Split DRCS activation data into separate files by year.\n",
    "    \n",
    "    Args:\n",
    "        drcs_result: The crawled DRCS data dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping years to their data\n",
    "    \"\"\"\n",
    "    # Initialize year-based dictionaries\n",
    "    years_data = {year: {} for year in range(2020, 2026)}\n",
    "    \n",
    "    # Process each activation event\n",
    "    drcs_data = drcs_result.get('drcs_activations', {})\n",
    "    \n",
    "    for event_name, event_data in drcs_data.items():\n",
    "        year = extract_year_from_event_name(event_name)\n",
    "        \n",
    "        if year and year in years_data:\n",
    "            # If it's a plain year folder (e.g., \"2020\"), merge its contents\n",
    "            if event_name == str(year):\n",
    "                # This is a year folder, add all its contents\n",
    "                for sub_event, sub_data in event_data.items():\n",
    "                    years_data[year][sub_event] = sub_data\n",
    "            else:\n",
    "                # This is a named event (e.g., \"202301_Flood_CA\")\n",
    "                years_data[year][event_name] = event_data\n",
    "    \n",
    "    return years_data\n",
    "\n",
    "print(\"✅ Year-splitting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically split DRCS data by year after saving\n",
    "if 'drcs_result' in locals():\n",
    "    print(\"\\n📅 Splitting DRCS data by year (2020-2025)...\\n\")\n",
    "    \n",
    "    # Split the data by year\n",
    "    years_data = split_drcs_by_year(drcs_result)\n",
    "    \n",
    "    # Create separate JSON files for each year\n",
    "    for year in range(2020, 2026):\n",
    "        output_file = f'drcs_activations_{year}.json'\n",
    "        \n",
    "        # Count statistics\n",
    "        event_count = len(years_data[year])\n",
    "        total_files = sum(count_files_recursive(event_data) \n",
    "                         for event_data in years_data[year].values())\n",
    "        \n",
    "        # Create output structure\n",
    "        output_data = {\n",
    "            \"drcs_activations\": years_data[year],\n",
    "            \"_metadata\": {\n",
    "                \"year\": year,\n",
    "                \"extracted_from\": DRCS_OUTPUT_FILE,\n",
    "                \"created_at\": datetime.now().isoformat(),\n",
    "                \"total_events\": event_count,\n",
    "                \"total_tif_files\": total_files,\n",
    "                \"events\": sorted(list(years_data[year].keys()))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ Created {output_file}\")\n",
    "        print(f\"   - Events: {event_count}\")\n",
    "        print(f\"   - Total .tif files: {total_files}\")\n",
    "        if event_count > 0:\n",
    "            sample_events = list(years_data[year].keys())[:3]\n",
    "            if event_count > 3:\n",
    "                sample_events.append(f\"... and {event_count - 3} more\")\n",
    "            print(f\"   - Sample events: {sample_events}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"📊 Year-based splitting complete!\")\n",
    "else:\n",
    "    print(\"⚠️ No DRCS data available. Run the DRCS crawler first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete solution for crawling AWS S3 bucket structures and saving them to JSON format.\n",
    "\n",
    "### Key Features:\n",
    "- ✅ Recursive traversal of S3 buckets\n",
    "- ✅ Nested JSON output preserving directory structure\n",
    "- ✅ File metadata collection (size, modified date, etc.)\n",
    "- ✅ Configurable depth limits and file filtering\n",
    "- ✅ Progress tracking for large buckets\n",
    "- ✅ Statistics generation and analysis\n",
    "- ✅ Multiple output formats (structured JSON, file list, statistics CSV)\n",
    "\n",
    "### Next Steps:\n",
    "1. Modify `BUCKET_NAME` to your target bucket\n",
    "2. Adjust `PREFIX` to focus on specific directories\n",
    "3. Set `MAX_DEPTH` to limit traversal depth\n",
    "4. Use file extension filters for specific file types\n",
    "5. Analyze the generated JSON for your specific use case\n",
    "\n",
    "### Tips:\n",
    "- For very large buckets, consider using prefixes to crawl in sections\n",
    "- The compact JSON format saves space for large structures\n",
    "- Use the statistics CSV for data analysis and reporting\n",
    "- The file list output is useful for batch processing operations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}