{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced S3 to COG Converter with Automatic AWS Authentication\n",
    "\n",
    "This notebook converts TIF files from S3 to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Automatic AWS credential detection** (no .env file needed)\n",
    "- **Download caching** to avoid re-downloading large files\n",
    "- **COG validation** before uploading\n",
    "- **Support for multiple AWS authentication methods**\n",
    "\n",
    "Author: Kyle Lesinger (Enhanced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import s3fs\n",
    "import fsspec\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add path for importing custom modules\nimport sys\nfrom pathlib import Path\n\n# Add the scripts directory to the Python path\nscripts_dir = Path('../scripts').resolve()\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\n# Import functions from list_s3crawler_files module\nfrom list_s3crawler_files import (\n    load_drcs_data,\n    get_tif_files_from_path,\n    get_files_with_full_paths,\n    list_available_directories\n)\n\n# Import COG and cache utilities\nfrom cog_utilities import (\n    check_cache_status,\n    clear_cache,\n    validate_cog\n)\n\n# Import AWS S3 utilities\nfrom aws_s3_utils import (\n    initialize_s3_client,\n    verify_s3_client,\n    get_all_s3_keys\n)\n\n# Import batch processing utilities\nfrom batch_processing import (\n    process_file_batch,\n    print_batch_summary\n)\n\nprint(\"‚úÖ Custom modules imported successfully!\")\nprint(f\"   Module path: {scripts_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "[drcs_activations OLD Directory](https://data.disasters.openveda.cloud/browseui/browseui/#drcs_activations/)\n",
    "\n",
    "[VEDA docs for file naming conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of new 2nd level directories\n",
    "\n",
    "    \"Sentinel-1\"\n",
    "    \"Sentinel-2\"\n",
    "    \"Landsat\"\n",
    "    \"MODIS\"\n",
    "    \"VIIRS\"\n",
    "    \"ASTER\"\n",
    "    \"MASTER\"\n",
    "    \"ECOSTRESS\"\n",
    "    \"Planet\"\n",
    "    \"Maxar\"\n",
    "    \"HLS\"\n",
    "    \"IMERG\"\n",
    "    \"GOES\"\n",
    "    \"SMAP\"\n",
    "    \"ICESat\"\n",
    "    \"GEDI\"\n",
    "    \"COMSAR\"\n",
    "    \"UAVSAR\"\n",
    "    \"WB-57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "DIR_OLD_BASE = 'drcs_activations'\n",
    "DIR_NEW_BASE = 'drcs_activations_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_NAME = '202405_Flood_TX'\n",
    "PRODUCT_NAME = 'sentinel1'\n",
    "\n",
    "RENAME_PRODUCT = 'Sentinel-1'\n",
    "\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}'  # Updated to use actual available directory\n",
    "DIRECTORY_NEW = f'{DIR_NEW_BASE}/{RENAME_PRODUCT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TIF Files from DRCS Data\n",
    "\n",
    "This cell loads the pre-analyzed DRCS activation data from `drcs_activations_tif_files.json` which contains a complete inventory of all .tif files in the NASA Disasters S3 bucket.\n",
    "\n",
    "The code will:\n",
    "1. Load the JSON file containing the file inventory\n",
    "2. Parse the `PATH_OLD` variable to find the corresponding directory\n",
    "3. Extract all .tif filenames from that directory\n",
    "4. Store them in `files_to_process` for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded DRCS data from ../../s3-crawler/drcs_activations_tif_files.json\n",
      "\n",
      "üìÅ Found 11 .tif files in drcs_activations/202405_Flood_TX/sentinel1:\n",
      "\n",
      "First 10 files:\n",
      "   1. S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif\n",
      "   2. S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif\n",
      "   3. S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif\n",
      "   4. S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif\n",
      "   5. S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif\n",
      "   6. S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif\n",
      "   7. S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif\n",
      "   8. S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif\n",
      "   9. S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif\n",
      "  10. S1_20240430_20240507_WM_diff.tif\n",
      "  ... and 1 more files\n",
      "\n",
      "‚úÖ Files ready for processing. Stored in 'files_to_process' variable.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-analyzed DRCS TIF files data using imported functions\n",
    "# The JSON path is relative to the notebook location\n",
    "json_path = Path('../../s3-crawler/drcs_activations_tif_files.json')\n",
    "\n",
    "# Load DRCS data\n",
    "drcs_data = load_drcs_data(json_path)\n",
    "\n",
    "if drcs_data:\n",
    "    # Get TIF files from the specified PATH_OLD using the imported function\n",
    "    tif_files = get_tif_files_from_path(PATH_OLD, drcs_data, DIR_OLD_BASE)\n",
    "    \n",
    "    if tif_files:\n",
    "        print(f\"\\nüìÅ Found {len(tif_files)} .tif files in {PATH_OLD}:\")\n",
    "        print(\"\\nFirst 10 files:\")\n",
    "        for i, file in enumerate(tif_files[:10], 1):\n",
    "            print(f\"  {i:2d}. {file}\")\n",
    "        if len(tif_files) > 10:\n",
    "            print(f\"  ... and {len(tif_files) - 10} more files\")\n",
    "        \n",
    "        # Get files with full paths using the imported function\n",
    "        files_to_process = get_files_with_full_paths(PATH_OLD, drcs_data, DIR_OLD_BASE, json_path)\n",
    "        print(f\"\\n‚úÖ Files ready for processing. Stored in 'files_to_process' variable.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No files found. Please check the PATH_OLD variable.\")\n",
    "        files_to_process = []\n",
    "else:\n",
    "    print(f\"\\n‚ùå Could not load DRCS data.\")\n",
    "    files_to_process = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: List available activation events using the imported function\n",
    "# print(\"üìÇ Available activation events in DRCS data:\")\n",
    "# events = list_available_directories('drcs_activations', drcs_data, json_path)\n",
    "\n",
    "# # Show first 10 events\n",
    "# for event in events[:10]:\n",
    "#     print(f\"  - {event}\")\n",
    "# if len(events) > 10:\n",
    "#     print(f\"  ... and {len(events) - 10} more events\")\n",
    "\n",
    "# # Example: List subdirectories for a specific event\n",
    "# print(f\"\\nüìÅ Subdirectories in {EVENT_NAME}:\")\n",
    "# subdirs = list_available_directories(f'drcs_activations/{EVENT_NAME}', drcs_data, json_path)\n",
    "# for subdir in subdirs:\n",
    "#     print(f\"  - {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For these we can see three different types of files\n",
    "\n",
    "1. WM = water mask\n",
    "2. rgb = red green blue\n",
    "3. WM_diff = water mask difference between dates\n",
    "\n",
    "### We are going to need 2 different directories for these!!!\n",
    "\n",
    "We will keep WaterMask (WM) and rgb as separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, let's use python list comprehension to return the files\n",
    "# We may need to rename them in different ways for different products\n",
    "# We will do a similar process later\n",
    "\n",
    "## NOTE --- We can actually use these objects since they have the same path as the s3 files. We will call them again later\n",
    "\n",
    "water_mask = [f for f in files_to_process if \"_WM.tif\" in f]\n",
    "rgb = [f for f in files_to_process if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in files_to_process if \"WM_diff.tif\" in f]\n",
    "water_mask_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the diff files, we need to add diff at the 1st date before it\n",
    "# Otherwise VEDA will think that the first date is the most important\n",
    "\n",
    "# Example S1_diff20240430_20240507_WM.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_WM = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}\n",
    "\n",
    "config_rgb = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/rgb\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Add configuration for water mask diff files\nconfig_WM_diff = {\n    \"data_acquisition_method\": \"s3\",\n    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n    \"raw_data_prefix\": F\"{PATH_OLD}\",\n    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM_diff\",\n    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n    \"transformation\": {}\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure bucket and paths (no need to create session manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/WM\n"
     ]
    }
   ],
   "source": [
    "# Configure bucket and paths (no need to create session manually)\n",
    "bucket_name = config_WM[\"cog_data_bucket\"]\n",
    "raw_data_bucket = config_WM[\"raw_data_bucket\"]\n",
    "raw_data_prefix = config_WM[\"raw_data_prefix\"]\n",
    "\n",
    "cog_data_bucket = config_WM['cog_data_bucket']\n",
    "cog_data_prefix = config_WM[\"cog_data_prefix\"]\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Source bucket: {raw_data_bucket}\")\n",
    "print(f\"  Source prefix: {raw_data_prefix}\")\n",
    "print(f\"  Target bucket: {cog_data_bucket}\")\n",
    "print(f\"  Target prefix: {cog_data_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS S3 Client with automatic credential detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è S3 client initialized (limited bucket list access)\n",
      "‚úÖ Confirmed access to nasa-disasters bucket\n",
      "‚úÖ S3 filesystem (fsspec) initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize AWS S3 Client using the imported function\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name='nasa-disasters', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 client ready for operations\n",
      "   Bucket: nasa-disasters\n",
      "   Ready to process files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify S3 client is ready using the imported function\n",
    "verify_s3_client(s3_client, bucket_name='nasa-disasters', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 11 .tif files in the S3 bucket.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TIF files using the imported function\n",
    "keys = get_all_s3_keys(s3_client, raw_data_bucket, raw_data_prefix, \".tif\") if s3_client else []\n",
    "\n",
    "if keys:\n",
    "    print(f\"‚úÖ Found {len(keys)} .tif files in the S3 bucket.\")\n",
    "else:\n",
    "    print(\"No keys found or S3 client not initialized\")\n",
    "    \n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30T00:26:53Z\n"
     ]
    }
   ],
   "source": [
    "def convert_sentinel_datetime(datetime_str):\n",
    "    \"\"\"\n",
    "    Convert Sentinel datetime format to ISO 8601 format with UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        datetime_str: String like '20240430T002653'\n",
    "    \n",
    "    Returns:\n",
    "        String like '2024-04-30T00:26:53Z'\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    year = datetime_str[0:4]\n",
    "    month = datetime_str[4:6]\n",
    "    day = datetime_str[6:8]\n",
    "    hour = datetime_str[9:11]\n",
    "    minute = datetime_str[11:13]\n",
    "    second = datetime_str[13:15]\n",
    "    \n",
    "    # Format with dashes and colons, add Z for UTC\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}:{second}Z\"\n",
    "\n",
    "# Test\n",
    "datetime_str = '20240430T002653'\n",
    "result = convert_sentinel_datetime(datetime_str)\n",
    "print(result)  # 2024-04-30T00:26:53Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_cog_filename_WM(f, EVENT_NAME):\n    \"\"\"Create COG filename for water mask files.\"\"\"\n    f2 = Path(f).stem\n    fsplit = f2.split('_')\n    \n    # Check if it's a diff file\n    if \"WM_diff\" in f:\n        # For diff files: S1_20240430_20240507_WM_diff.tif\n        # Need to add \"diff\" before the first date\n        # Result: S1_diff20240430_20240507_WM.tif\n        return f'{EVENT_NAME}_S1_diff{fsplit[1]}_{fsplit[2]}_WM.tif'\n    else:\n        # Regular WM files\n        cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'\n        return cog_filename\n\ndef create_cog_filename_rgb(f, EVENT_NAME):\n    \"\"\"Create COG filename for RGB files.\"\"\"\n    f2 = Path(f).stem\n    fsplit = f2.split('_')\n    \n    # For RGB files, similar to WM but keep rgb suffix\n    cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_rgb_{convert_sentinel_datetime(fsplit[2])}.tif'\n    return cog_filename\n\ndef create_cog_filename_diff(f, EVENT_NAME):\n    \"\"\"Create COG filename for water mask diff files.\"\"\"\n    f2 = Path(f).stem\n    fsplit = f2.split('_')\n    \n    # For diff files: S1_20240430_20240507_WM_diff.tif\n    # Need to add \"diff\" before the first date\n    # Result: 202405_Flood_TX_S1_diff20240430_20240507_WM.tif\n    return f'{EVENT_NAME}_S1_diff{fsplit[1]}_{fsplit[2]}_WM.tif'\n\n# Test functions\nprint(\"Testing WM filename:\")\nprint(create_cog_filename_WM('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif', EVENT_NAME))\n\nprint(\"\\nTesting RGB filename:\")\nprint(create_cog_filename_rgb('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif', EVENT_NAME))\n\nprint(\"\\nTesting diff filename:\")\nprint(create_cog_filename_diff('drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif', EVENT_NAME))"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define COG profile for rasterio\n",
    "COG_PROFILE = {\n",
    "    \"driver\": \"COG\",\n",
    "    \"compress\": \"DEFLATE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Define COG Conversion Function\n\nThis function handles the conversion of files to Cloud Optimized GeoTIFFs with proper CRS and caching.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def convert_to_proper_CRS_and_cogify(name, cog_filename, cog_data_bucket, cog_data_prefix, local_output_dir=None):\n    \"\"\"\n    Convert a file to Cloud Optimized GeoTIFF with proper CRS.\n    \n    This function includes:\n    - Download caching to avoid re-downloading files\n    - CRS reprojection to EPSG:4326\n    - COG validation before upload\n    - Upload to S3\n    \"\"\"\n    s3_key = f\"{cog_data_prefix}/{cog_filename}\"\n    reproject_filename = f\"reproj/{cog_filename}\"\n    \n    # Create necessary directories\n    os.makedirs(\"reproj\", exist_ok=True)\n    \n    # Create data_download directory for caching\n    data_download_dir = \"data_download\"\n    os.makedirs(data_download_dir, exist_ok=True)\n    \n    # Create subdirectory structure to match S3 path\n    s3_path_parts = name.split('/')\n    local_subdir = os.path.join(data_download_dir, *s3_path_parts[:-1])\n    os.makedirs(local_subdir, exist_ok=True)\n    \n    # Local path for the downloaded file (persistent storage)\n    local_download_path = os.path.join(data_download_dir, name)\n    \n    # Temporary file for processing\n    temp_input_file = f\"temp_{os.path.basename(name)}\"\n\n    try:\n        # Check if file already exists locally\n        if os.path.exists(local_download_path):\n            print(f\"   [CACHE HIT] Using cached file: {local_download_path}\")\n            import shutil\n            shutil.copy(local_download_path, temp_input_file)\n        else:\n            # Download the file from S3\n            print(f\"   [DOWNLOAD] Downloading from S3...\")\n            s3_client.download_file(raw_data_bucket, name, local_download_path)\n            print(f\"   [DOWNLOAD] ‚úÖ Saved to cache\")\n            import shutil\n            shutil.copy(local_download_path, temp_input_file)\n        \n        # Reproject to EPSG:4326\n        print(f\"   [REPROJECT] Converting to EPSG:4326...\")\n        with rasterio.open(temp_input_file) as src:\n            dst_crs = \"EPSG:4326\"\n            \n            # Check if reprojection is needed\n            if src.crs and src.crs.to_string() == dst_crs:\n                print(f\"   [REPROJECT] Already in {dst_crs}, skipping reprojection\")\n                import shutil\n                shutil.copy(temp_input_file, reproject_filename)\n            else:\n                transform, width, height = calculate_default_transform(\n                    src.crs, dst_crs, src.width, src.height, *src.bounds\n                )\n                kwargs = src.meta.copy()\n                kwargs.update({\n                    \"driver\": \"COG\",\n                    \"compress\": \"DEFLATE\",\n                    \"crs\": dst_crs,\n                    \"transform\": transform,\n                    \"width\": width,\n                    \"height\": height\n                })\n\n                with rasterio.open(reproject_filename, \"w\", **kwargs) as dst:\n                    for band_idx in range(1, src.count + 1):\n                        reproject(\n                            source=rasterio.band(src, band_idx),\n                            destination=rasterio.band(dst, band_idx),\n                            src_transform=src.transform,\n                            src_crs=src.crs,\n                            dst_transform=transform,\n                            dst_crs=dst_crs,\n                            resampling=Resampling.nearest,\n                            wrapdateline=True\n                        )\n\n        # COGify & upload\n        print(f\"   [COGIFY] Creating COG...\")\n        ds = rxr.open_rasterio(reproject_filename)\n        \n        # Handle coordinate naming\n        if \"y\" in ds.dims and \"x\" in ds.dims:\n            ds = ds.rename({\"y\": \"lat\", \"x\": \"lon\"})\n            ds.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n        \n        ds.rio.write_nodata(-9999, inplace=True)\n\n        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n            tmp_name = tmp.name\n            ds.rio.to_raster(tmp_name, **COG_PROFILE)\n            \n            # Validate COG\n            print(f\"   [VALIDATE] Checking COG validity...\")\n            is_valid_cog, validation_details = validate_cog(tmp_name)\n            \n            if is_valid_cog:\n                print(f\"   [VALIDATE] ‚úÖ Valid COG\")\n            else:\n                print(f\"   [VALIDATE] ‚ö†Ô∏è COG validation warnings\")\n                critical_errors = [e for e in validation_details['errors'] if 'Invalid driver' in e]\n                if critical_errors:\n                    raise ValueError(f\"Critical COG validation failed\")\n            \n            # Upload to S3\n            print(f\"   [UPLOAD] Uploading to S3...\")\n            s3_client.upload_file(\n                Filename=tmp_name,\n                Bucket=cog_data_bucket,\n                Key=s3_key\n            )\n            print(f\"   [SUCCESS] ‚úÖ Uploaded to s3://{cog_data_bucket}/{s3_key}\")\n            \n            # Save locally if specified\n            if local_output_dir:\n                os.makedirs(local_output_dir, exist_ok=True)\n                local_path = os.path.join(local_output_dir, cog_filename)\n                import shutil\n                shutil.copy(tmp_name, local_path)\n            \n    except Exception as e:\n        print(f\"   [ERROR] Failed: {str(e)}\")\n        raise\n            \n    finally:\n        # Clean up temporary files\n        for temp_file in [temp_input_file, reproject_filename]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n        if 'tmp_name' in locals() and os.path.exists(tmp_name):\n            os.remove(tmp_name)\n\nprint(\"‚úÖ COG conversion function defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cache directory does not exist: data_download/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current cache status using the imported function\n",
    "check_cache_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Process files using batch processing function\n\n# Separate files by type\nwater_mask = [f for f in keys if \"_WM.tif\" in f and \"WM_diff\" not in f]\nrgb = [f for f in keys if \"rgb.tif\" in f]\nwater_mask_diff = [f for f in keys if \"WM_diff.tif\" in f]\n\nprint(\"üìä File categorization:\")\nprint(f\"  - Water mask files: {len(water_mask)}\")\nprint(f\"  - RGB files: {len(rgb)}\")\nprint(f\"  - Water mask diff files: {len(water_mask_diff)}\")\nprint(f\"  - Total files: {len(keys)}\")\n\n# Initialize combined results DataFrame\nall_files_processed = pd.DataFrame()\n\n# Process water mask files\nif water_mask:\n    print(\"\\n\" + \"=\"*50)\n    print(\"üåä Processing Water Mask Files\")\n    print(\"=\"*50)\n    \n    wm_results = process_file_batch(\n        file_list=water_mask,\n        s3_client=s3_client,\n        config=config_WM,\n        filename_creator_func=create_cog_filename_WM,\n        processing_func=convert_to_proper_CRS_and_cogify,\n        event_name=EVENT_NAME,\n        save_metadata=True,\n        save_csv=True,\n        verbose=True\n    )\n    all_files_processed = pd.concat([all_files_processed, wm_results], ignore_index=True)\n\n# Process RGB files\nif rgb:\n    print(\"\\n\" + \"=\"*50)\n    print(\"üé® Processing RGB Files\")\n    print(\"=\"*50)\n    \n    rgb_results = process_file_batch(\n        file_list=rgb,\n        s3_client=s3_client,\n        config=config_rgb,\n        filename_creator_func=create_cog_filename_rgb,\n        processing_func=convert_to_proper_CRS_and_cogify,\n        event_name=EVENT_NAME,\n        save_metadata=True,\n        save_csv=True,\n        verbose=True\n    )\n    all_files_processed = pd.concat([all_files_processed, rgb_results], ignore_index=True)\n\n# Process water mask diff files\nif water_mask_diff:\n    print(\"\\n\" + \"=\"*50)\n    print(\"üîÑ Processing Water Mask Diff Files\")\n    print(\"=\"*50)\n    \n    diff_results = process_file_batch(\n        file_list=water_mask_diff,\n        s3_client=s3_client,\n        config=config_WM_diff,\n        filename_creator_func=create_cog_filename_diff,\n        processing_func=convert_to_proper_CRS_and_cogify,\n        event_name=EVENT_NAME,\n        save_metadata=True,\n        save_csv=True,\n        verbose=True\n    )\n    all_files_processed = pd.concat([all_files_processed, diff_results], ignore_index=True)\n\n# Print overall summary\nprint_batch_summary(all_files_processed)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata if there are processed files\n",
    "if len(files_processed) > 0:\n",
    "    # Get metadata from one of the processed files\n",
    "    sample_file = files_processed.iloc[0]['file_name']\n",
    "    temp_sample_file = f\"temp_{os.path.basename(sample_file)}\"\n",
    "    \n",
    "    # Download sample file to extract metadata\n",
    "    s3_client.download_file(raw_data_bucket, sample_file, temp_sample_file)\n",
    "    \n",
    "    with rasterio.open(temp_sample_file) as src:\n",
    "        metadata = {\n",
    "            \"description\": src.tags(),\n",
    "            \"driver\": src.driver,\n",
    "            \"dtype\": str(src.dtypes[0]),\n",
    "            \"nodata\": src.nodata,\n",
    "            \"width\": src.width,\n",
    "            \"height\": src.height,\n",
    "            \"count\": src.count,\n",
    "            \"crs\": str(src.crs),\n",
    "            \"transform\": list(src.transform),\n",
    "            \"bounds\": list(src.bounds),\n",
    "            \"total_files_processed\": len(files_processed),\n",
    "            \"year\": \"2000\"\n",
    "        }\n",
    "    \n",
    "    # Upload metadata\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "        fp.flush()\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            Filename=fp.name,\n",
    "            Bucket=bucket_name,\n",
    "            Key=f\"{cog_data_prefix}/metadata.json\",\n",
    "        )\n",
    "        print(f\"Uploaded metadata to s3://{bucket_name}/{cog_data_prefix}/metadata.json\")\n",
    "    \n",
    "    # Clean up sample file\n",
    "    if os.path.exists(temp_sample_file):\n",
    "        os.remove(temp_sample_file)\n",
    "\n",
    "# Save the files_processed DataFrame to CSV using the same s3_client\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".csv\") as fp:\n",
    "    files_processed.to_csv(fp.name, index=False)\n",
    "    fp.flush()\n",
    "    \n",
    "    s3_client.upload_file(\n",
    "        Filename=fp.name,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{cog_data_prefix}/files_converted.csv\",\n",
    "    )\n",
    "    print(f\"Saved processing log to s3://{bucket_name}/{cog_data_prefix}/files_converted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display final results\nprint(f\"\\nüìä Final Processing Results:\")\nprint(f\"Total files processed: {len(all_files_processed)}\")\nprint(f\"\\nProcessed files DataFrame:\")\nall_files_processed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Features in This Version\n",
    "\n",
    "This enhanced notebook includes several improvements over the original:\n",
    "\n",
    "### üîê **Automatic AWS Authentication**\n",
    "- No need for `.env` files or manual credential configuration\n",
    "- Automatically detects credentials from:\n",
    "  - Environment variables\n",
    "  - AWS CLI configuration\n",
    "  - IAM roles (EC2/Lambda)\n",
    "\n",
    "### üöÄ **Simplified Setup**\n",
    "- Removed dependency on `python-dotenv`\n",
    "- Direct boto3 client initialization\n",
    "- Better error handling for authentication issues\n",
    "\n",
    "### üìä **Additional Features**\n",
    "- fsspec filesystem integration for alternative S3 operations\n",
    "- Graceful handling of limited S3 permissions\n",
    "- Download caching to avoid re-downloading large files\n",
    "- COG validation before upload\n",
    "- Comprehensive error messages\n",
    "\n",
    "### üí° **Usage Tips**\n",
    "1. Ensure AWS credentials are configured via one of the standard methods\n",
    "2. The notebook will automatically detect and use available credentials\n",
    "3. Check the authentication cell output to confirm S3 access\n",
    "4. Use the cache management utilities to monitor downloaded files\n",
    "\n",
    "This enhanced version follows AWS best practices and makes the notebook more portable and easier to use across different environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}