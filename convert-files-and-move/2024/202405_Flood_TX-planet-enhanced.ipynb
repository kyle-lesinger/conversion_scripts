{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced S3 to COG Converter with Automatic AWS Authentication\n",
    "\n",
    "This notebook converts TIF files from S3 to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Automatic AWS credential detection** (no .env file needed)\n",
    "- **Download caching** to avoid re-downloading large files\n",
    "- **COG validation** before uploading\n",
    "- **Support for multiple AWS authentication methods**\n",
    "\n",
    "Author: Kyle Lesinger (Enhanced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import s3fs\n",
    "import fsspec\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom S3 crawler functions imported successfully!\n",
      "   Module path: /home/jovyan/conversion_scripts/convert-files-and-move/scripts\n"
     ]
    }
   ],
   "source": [
    "# Add path for importing custom modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the scripts directory to the Python path\n",
    "scripts_dir = Path('../scripts').resolve()\n",
    "if str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "# Import functions from list_s3crawler_files module\n",
    "from list_s3crawler_files import (\n",
    "    load_drcs_data,\n",
    "    get_tif_files_from_path,\n",
    "    get_files_with_full_paths,\n",
    "    list_available_directories\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom S3 crawler functions imported successfully!\")\n",
    "print(f\"   Module path: {scripts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "[drcs_activations OLD Directory](https://data.disasters.openveda.cloud/browseui/browseui/#drcs_activations/)\n",
    "\n",
    "[VEDA docs for file naming conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of new 2nd level directories\n",
    "\n",
    "    \"Sentinel-1\"\n",
    "    \"Sentinel-2\"\n",
    "    \"Landsat\"\n",
    "    \"MODIS\"\n",
    "    \"VIIRS\"\n",
    "    \"ASTER\"\n",
    "    \"MASTER\"\n",
    "    \"ECOSTRESS\"\n",
    "    \"Planet\"\n",
    "    \"Maxar\"\n",
    "    \"HLS\"\n",
    "    \"IMERG\"\n",
    "    \"GOES\"\n",
    "    \"SMAP\"\n",
    "    \"ICESat\"\n",
    "    \"GEDI\"\n",
    "    \"COMSAR\"\n",
    "    \"UAVSAR\"\n",
    "    \"WB-57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "DIR_OLD_BASE = 'drcs_activations'\n",
    "DIR_NEW_BASE = 'drcs_activations_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_NAME = '202405_Flood_TX'\n",
    "PRODUCT_NAME = 'sentinel1'\n",
    "\n",
    "RENAME_PRODUCT = 'Sentinel-1'\n",
    "\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}'  # Updated to use actual available directory\n",
    "DIRECTORY_NEW = f'{DIR_NEW_BASE}/{RENAME_PRODUCT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TIF Files from DRCS Data\n",
    "\n",
    "This cell loads the pre-analyzed DRCS activation data from `drcs_activations_tif_files.json` which contains a complete inventory of all .tif files in the NASA Disasters S3 bucket.\n",
    "\n",
    "The code will:\n",
    "1. Load the JSON file containing the file inventory\n",
    "2. Parse the `PATH_OLD` variable to find the corresponding directory\n",
    "3. Extract all .tif filenames from that directory\n",
    "4. Store them in `files_to_process` for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded DRCS data from ../../s3-crawler/drcs_activations_tif_files.json\n",
      "\n",
      "üìÅ Found 11 .tif files in drcs_activations/202405_Flood_TX/sentinel1:\n",
      "\n",
      "First 10 files:\n",
      "   1. S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif\n",
      "   2. S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif\n",
      "   3. S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif\n",
      "   4. S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif\n",
      "   5. S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif\n",
      "   6. S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif\n",
      "   7. S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif\n",
      "   8. S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif\n",
      "   9. S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif\n",
      "  10. S1_20240430_20240507_WM_diff.tif\n",
      "  ... and 1 more files\n",
      "\n",
      "‚úÖ Files ready for processing. Stored in 'files_to_process' variable.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-analyzed DRCS TIF files data using imported functions\n",
    "# The JSON path is relative to the notebook location\n",
    "json_path = Path('../../s3-crawler/drcs_activations_tif_files.json')\n",
    "\n",
    "# Load DRCS data\n",
    "drcs_data = load_drcs_data(json_path)\n",
    "\n",
    "if drcs_data:\n",
    "    # Get TIF files from the specified PATH_OLD using the imported function\n",
    "    tif_files = get_tif_files_from_path(PATH_OLD, drcs_data, DIR_OLD_BASE)\n",
    "    \n",
    "    if tif_files:\n",
    "        print(f\"\\nüìÅ Found {len(tif_files)} .tif files in {PATH_OLD}:\")\n",
    "        print(\"\\nFirst 10 files:\")\n",
    "        for i, file in enumerate(tif_files[:10], 1):\n",
    "            print(f\"  {i:2d}. {file}\")\n",
    "        if len(tif_files) > 10:\n",
    "            print(f\"  ... and {len(tif_files) - 10} more files\")\n",
    "        \n",
    "        # Get files with full paths using the imported function\n",
    "        files_to_process = get_files_with_full_paths(PATH_OLD, drcs_data, DIR_OLD_BASE, json_path)\n",
    "        print(f\"\\n‚úÖ Files ready for processing. Stored in 'files_to_process' variable.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå No files found. Please check the PATH_OLD variable.\")\n",
    "        files_to_process = []\n",
    "else:\n",
    "    print(f\"\\n‚ùå Could not load DRCS data.\")\n",
    "    files_to_process = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: List available activation events using the imported function\n",
    "# print(\"üìÇ Available activation events in DRCS data:\")\n",
    "# events = list_available_directories('drcs_activations', drcs_data, json_path)\n",
    "\n",
    "# # Show first 10 events\n",
    "# for event in events[:10]:\n",
    "#     print(f\"  - {event}\")\n",
    "# if len(events) > 10:\n",
    "#     print(f\"  ... and {len(events) - 10} more events\")\n",
    "\n",
    "# # Example: List subdirectories for a specific event\n",
    "# print(f\"\\nüìÅ Subdirectories in {EVENT_NAME}:\")\n",
    "# subdirs = list_available_directories(f'drcs_activations/{EVENT_NAME}', drcs_data, json_path)\n",
    "# for subdir in subdirs:\n",
    "#     print(f\"  - {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For these we can see three different types of files\n",
    "\n",
    "1. WM = water mask\n",
    "2. rgb = red green blue\n",
    "3. WM_diff = water mask difference between dates\n",
    "\n",
    "### We are going to need 2 different directories for these!!!\n",
    "\n",
    "We will keep WaterMask (WM) and rgb as separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, let's use python list comprehension to return the files\n",
    "# We may need to rename them in different ways\n",
    "# We will do a similar process later\n",
    "\n",
    "water_mask = [f for f in files_to_process if \"_WM.tif\" in f]\n",
    "rgb = [f for f in files_to_process if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in files_to_process if \"WM_diff.tif\" in f]\n",
    "water_mask_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the diff files, we need to add diff at the 1st date before it\n",
    "# Otherwise VEDA will think that the first date is the most important\n",
    "\n",
    "# Example S1_diff20240430_20240507_WM.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_WM = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}\n",
    "\n",
    "config_rgb = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/rgb\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS S3 Client with automatic credential detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è S3 client initialized (limited bucket list access)\n",
      "‚úÖ Confirmed access to nasa-disasters bucket\n",
      "‚úÖ S3 filesystem (fsspec) initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize AWS S3 Client with automatic credential detection\n",
    "try:\n",
    "    # Create S3 client - will automatically use AWS credentials from:\n",
    "    # 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    # 2. AWS CLI configuration (~/.aws/credentials)\n",
    "    # 3. IAM role (if running on EC2/Lambda)\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Test connection\n",
    "    try:\n",
    "        # Try to list buckets (might fail due to permissions, that's OK)\n",
    "        response = s3_client.list_buckets()\n",
    "        print(f\"‚úÖ S3 client initialized successfully\")\n",
    "        print(f\"   Found {len(response.get('Buckets', []))} accessible buckets\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'AccessDenied':\n",
    "            print(f\"‚ö†Ô∏è S3 client initialized (limited bucket list access)\")\n",
    "            # Test access to nasa-disasters bucket specifically\n",
    "            try:\n",
    "                s3_client.head_bucket(Bucket='nasa-disasters')\n",
    "                print(f\"‚úÖ Confirmed access to nasa-disasters bucket\")\n",
    "            except ClientError as bucket_error:\n",
    "                print(f\"‚ùå Cannot access nasa-disasters bucket: {bucket_error}\")\n",
    "                s3_client = None\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "    # Also initialize fsspec filesystem for S3\n",
    "    fs_read = fsspec.filesystem(\"s3\", anon=False, skip_instance_cache=False)\n",
    "    print(f\"‚úÖ S3 filesystem (fsspec) initialized\")\n",
    "    \n",
    "except NoCredentialsError:\n",
    "    print(\"‚ùå No AWS credentials found. Please configure credentials using:\")\n",
    "    print(\"   - Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\")\n",
    "    print(\"   - AWS CLI: aws configure\")\n",
    "    print(\"   - IAM role (if on EC2)\")\n",
    "    s3_client = None\n",
    "    fs_read = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing S3 client: {e}\")\n",
    "    s3_client = None\n",
    "    fs_read = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S3 client ready for operations\n",
      "   Bucket: nasa-disasters\n",
      "   Ready to process files\n"
     ]
    }
   ],
   "source": [
    "# Verify S3 client is ready\n",
    "if s3_client is None:\n",
    "    print(\"‚ùå S3 client not initialized. Please check your AWS credentials.\")\n",
    "    print(\"   The notebook will not be able to download files from S3.\")\n",
    "else:\n",
    "    print(\"‚úÖ S3 client ready for operations\")\n",
    "    print(\"   Bucket: nasa-disasters\")\n",
    "    print(\"   Ready to process files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure bucket and paths (no need to create session manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/WM\n"
     ]
    }
   ],
   "source": [
    "# Configure bucket and paths (no need to create session manually)\n",
    "bucket_name = config_WM[\"cog_data_bucket\"]\n",
    "raw_data_bucket = config_WM[\"raw_data_bucket\"]\n",
    "raw_data_prefix = config_WM[\"raw_data_prefix\"]\n",
    "\n",
    "cog_data_bucket = config_WM['cog_data_bucket']\n",
    "cog_data_prefix = config_WM[\"cog_data_prefix\"]\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  Source bucket: {raw_data_bucket}\")\n",
    "print(f\"  Source prefix: {raw_data_prefix}\")\n",
    "print(f\"  Target bucket: {cog_data_bucket}\")\n",
    "print(f\"  Target prefix: {cog_data_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 11 .tif files in the S3 bucket.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_s3_keys(bucket, model_name, ext):\n",
    "    \"\"\"Get a list of all keys in an S3 bucket.\"\"\"\n",
    "    if s3_client is None:\n",
    "        print(\"‚ùå S3 client not initialized\")\n",
    "        return []\n",
    "        \n",
    "    keys = []\n",
    "\n",
    "    kwargs = {\"Bucket\": bucket, \"Prefix\": f\"{model_name}/\"}\n",
    "    while True:\n",
    "        try:\n",
    "            resp = s3_client.list_objects_v2(**kwargs)\n",
    "            if 'Contents' in resp:\n",
    "                for obj in resp[\"Contents\"]:\n",
    "                    if obj[\"Key\"].endswith(ext) and \"historical\" not in obj[\"Key\"]:\n",
    "                        keys.append(obj[\"Key\"])\n",
    "        except ClientError as e:\n",
    "            print(f\"‚ùå Error listing objects: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            kwargs[\"ContinuationToken\"] = resp.get(\"NextContinuationToken\")\n",
    "            if not kwargs[\"ContinuationToken\"]:\n",
    "                break\n",
    "        except KeyError:\n",
    "            break\n",
    "\n",
    "    return keys\n",
    "\n",
    "# Get all TIF files\n",
    "keys = get_all_s3_keys(raw_data_bucket, raw_data_prefix, \".tif\") if s3_client else []\n",
    "if keys:\n",
    "    print(f\"‚úÖ Found {len(keys)} .tif files in the S3 bucket.\")\n",
    "else:\n",
    "    print(\"No keys found or S3 client not initialized\")\n",
    "    \n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30T00:26:53Z\n"
     ]
    }
   ],
   "source": [
    "def convert_sentinel_datetime(datetime_str):\n",
    "    \"\"\"\n",
    "    Convert Sentinel datetime format to ISO 8601 format with UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        datetime_str: String like '20240430T002653'\n",
    "    \n",
    "    Returns:\n",
    "        String like '2024-04-30T00:26:53Z'\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    year = datetime_str[0:4]\n",
    "    month = datetime_str[4:6]\n",
    "    day = datetime_str[6:8]\n",
    "    hour = datetime_str[9:11]\n",
    "    minute = datetime_str[11:13]\n",
    "    second = datetime_str[13:15]\n",
    "    \n",
    "    # Format with dashes and colons, add Z for UTC\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}:{second}Z\"\n",
    "\n",
    "# Test\n",
    "datetime_str = '20240430T002653'\n",
    "result = convert_sentinel_datetime(datetime_str)\n",
    "print(result)  # 2024-04-30T00:26:53Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_2024-04-30T00:26:53Z.tif'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_cog_filename_WM(f, EVENT_NAME,):\n",
    "    \n",
    "    f2 = Path(f).stem\n",
    "    f2\n",
    "    fsplit = f2.split('_')\n",
    "    fsplit\n",
    "    \n",
    "    cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'\n",
    "\n",
    "    return cog_filename\n",
    "\n",
    "# Test function below\n",
    "create_cog_filename_WM(f='drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif', EVENT_NAME=EVENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define COG profile for rasterio\n",
    "COG_PROFILE = {\n",
    "    \"driver\": \"COG\",\n",
    "    \"compress\": \"DEFLATE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COG validation function added\n"
     ]
    }
   ],
   "source": [
    "def validate_cog(filepath):\n",
    "    \"\"\"\n",
    "    Validate that a file is a proper Cloud Optimized GeoTIFF.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the file to validate\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (is_valid, details_dict) where is_valid is boolean and \n",
    "               details_dict contains validation information\n",
    "    \"\"\"\n",
    "    import rasterio\n",
    "    from rasterio.env import Env\n",
    "    \n",
    "    validation_details = {\n",
    "        'is_cog': False,\n",
    "        'has_tiles': False,\n",
    "        'has_overviews': False,\n",
    "        'tile_size': None,\n",
    "        'overview_levels': [],\n",
    "        'compression': None,\n",
    "        'driver': None,\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with Env(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n",
    "            with rasterio.open(filepath) as src:\n",
    "                # Check driver\n",
    "                validation_details['driver'] = src.driver\n",
    "                \n",
    "                # Check if it's a GeoTIFF\n",
    "                if src.driver != 'GTiff' and src.driver != 'COG':\n",
    "                    validation_details['errors'].append(f\"Invalid driver: {src.driver}, expected GTiff or COG\")\n",
    "                    return False, validation_details\n",
    "                \n",
    "                # Check for tiling\n",
    "                if src.profile.get('tiled', False):\n",
    "                    validation_details['has_tiles'] = True\n",
    "                    validation_details['tile_size'] = (\n",
    "                        src.profile.get('blockxsize', 0),\n",
    "                        src.profile.get('blockysize', 0)\n",
    "                    )\n",
    "                else:\n",
    "                    validation_details['errors'].append(\"File is not tiled\")\n",
    "                \n",
    "                # Check for overviews\n",
    "                overviews = src.overviews(1)  # Check band 1\n",
    "                if overviews:\n",
    "                    validation_details['has_overviews'] = True\n",
    "                    validation_details['overview_levels'] = overviews\n",
    "                else:\n",
    "                    validation_details['errors'].append(\"No overviews found\")\n",
    "                \n",
    "                # Check compression\n",
    "                compression = src.profile.get('compress', None)\n",
    "                validation_details['compression'] = compression\n",
    "                if compression not in ['DEFLATE', 'LZW', 'ZSTD', 'WEBP', 'JPEG']:\n",
    "                    validation_details['errors'].append(f\"Compression '{compression}' may not be optimal for COG\")\n",
    "                \n",
    "                # Check if file structure is cloud optimized\n",
    "                # A COG should have IFD (Image File Directory) offsets arranged properly\n",
    "                # This is a simplified check - true COG validation would check IFD ordering\n",
    "                is_likely_cog = (\n",
    "                    validation_details['has_tiles'] and \n",
    "                    validation_details['has_overviews'] and\n",
    "                    validation_details['compression'] in ['DEFLATE', 'LZW', 'ZSTD', 'WEBP', 'JPEG']\n",
    "                )\n",
    "                \n",
    "                validation_details['is_cog'] = is_likely_cog\n",
    "                \n",
    "                # Additional check for internal structure\n",
    "                if hasattr(src, 'is_tiled') and src.is_tiled:\n",
    "                    # Check tile size is reasonable (typically 256 or 512)\n",
    "                    tile_x, tile_y = validation_details['tile_size']\n",
    "                    if tile_x not in [256, 512, 1024] or tile_y not in [256, 512, 1024]:\n",
    "                        validation_details['errors'].append(f\"Non-standard tile size: {tile_x}x{tile_y}\")\n",
    "                \n",
    "                return is_likely_cog, validation_details\n",
    "                \n",
    "    except Exception as e:\n",
    "        validation_details['errors'].append(f\"Validation error: {str(e)}\")\n",
    "        return False, validation_details\n",
    "\n",
    "print(\"‚úÖ COG validation function added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Cache Management\n",
    "\n",
    "The updated `convert_to_proper_CRS_and_cogify` function now includes intelligent caching:\n",
    "\n",
    "1. **First-time download**: Files are downloaded from S3 and saved to `data_download/` directory\n",
    "2. **Subsequent runs**: Files are loaded from the local cache, avoiding re-download\n",
    "3. **Cache structure**: Preserves the original S3 directory structure for organization\n",
    "\n",
    "Benefits:\n",
    "- ‚ö° **Faster processing** on repeated runs\n",
    "- üíæ **Bandwidth savings** - large files aren't re-downloaded\n",
    "- üîÑ **Resumable** - if processing fails, downloads are preserved\n",
    "- üìÅ **Organized** - maintains S3 directory structure locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Cache directory does not exist: data_download/\n"
     ]
    }
   ],
   "source": [
    "# Cache utilities\n",
    "def check_cache_status():\n",
    "    \"\"\"Check the status of the download cache.\"\"\"\n",
    "    data_download_dir = \"data_download\"\n",
    "    \n",
    "    if not os.path.exists(data_download_dir):\n",
    "        print(f\"üìÅ Cache directory does not exist: {data_download_dir}/\")\n",
    "        return\n",
    "    \n",
    "    # Count files and calculate total size\n",
    "    total_files = 0\n",
    "    total_size = 0\n",
    "    file_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_download_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.tif'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                total_files += 1\n",
    "                total_size += file_size\n",
    "                file_list.append((file_path.replace(data_download_dir + '/', ''), file_size))\n",
    "    \n",
    "    print(f\"üìä Cache Status:\")\n",
    "    print(f\"  - Directory: {data_download_dir}/\")\n",
    "    print(f\"  - Total files: {total_files}\")\n",
    "    print(f\"  - Total size: {total_size / (1024**3):.2f} GB\")\n",
    "    \n",
    "    if file_list:\n",
    "        print(f\"\\nüìÅ Cached files (first 10):\")\n",
    "        for file_path, file_size in sorted(file_list)[:10]:\n",
    "            print(f\"  - {file_path} ({file_size / (1024**2):.1f} MB)\")\n",
    "        if len(file_list) > 10:\n",
    "            print(f\"  ... and {len(file_list) - 10} more files\")\n",
    "    \n",
    "    return total_files, total_size\n",
    "\n",
    "def clear_cache(confirm=False):\n",
    "    \"\"\"Clear the download cache.\"\"\"\n",
    "    data_download_dir = \"data_download\"\n",
    "    \n",
    "    if not os.path.exists(data_download_dir):\n",
    "        print(f\"Cache directory does not exist: {data_download_dir}/\")\n",
    "        return\n",
    "    \n",
    "    if not confirm:\n",
    "        print(\"‚ö†Ô∏è This will delete all cached downloads!\")\n",
    "        print(f\"Directory: {data_download_dir}/\")\n",
    "        print(\"To confirm, run: clear_cache(confirm=True)\")\n",
    "        return\n",
    "    \n",
    "    import shutil\n",
    "    shutil.rmtree(data_download_dir)\n",
    "    print(f\"‚úÖ Cache cleared: {data_download_dir}/ removed\")\n",
    "\n",
    "# Check current cache status\n",
    "check_cache_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_proper_CRS_and_cogify(name, cog_filename, cog_data_bucket, cog_data_prefix, local_output_dir=None):\n",
    "    s3_key = f\"{cog_data_prefix}/{cog_filename}\"\n",
    "    reproject_filename = f\"reproj/{cog_filename}\"\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs(\"reproj\", exist_ok=True)\n",
    "    \n",
    "    # Create data_download directory at the same level as the script\n",
    "    data_download_dir = \"data_download\"\n",
    "    os.makedirs(data_download_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectory structure to match S3 path\n",
    "    # This preserves the original directory structure for better organization\n",
    "    s3_path_parts = name.split('/')\n",
    "    local_subdir = os.path.join(data_download_dir, *s3_path_parts[:-1])\n",
    "    os.makedirs(local_subdir, exist_ok=True)\n",
    "    \n",
    "    # Local path for the downloaded file (persistent storage)\n",
    "    local_download_path = os.path.join(data_download_dir, name)\n",
    "    \n",
    "    # Temporary file for processing (will be cleaned up)\n",
    "    temp_input_file = f\"temp_{os.path.basename(name)}\"\n",
    "\n",
    "    try:\n",
    "        # Check if file already exists locally\n",
    "        if os.path.exists(local_download_path):\n",
    "            print(f\"[CACHE HIT] Found existing file: {local_download_path}\")\n",
    "            print(f\"[CACHE] File size: {os.path.getsize(local_download_path):,} bytes\")\n",
    "            # Copy from local cache to temp file for processing\n",
    "            import shutil\n",
    "            shutil.copy(local_download_path, temp_input_file)\n",
    "        else:\n",
    "            # Download the file from S3\n",
    "            print(f\"[DOWNLOAD] Downloading {name} from S3...\")\n",
    "            print(f\"[DOWNLOAD] Target: {local_download_path}\")\n",
    "            \n",
    "            # Download directly to the persistent location\n",
    "            s3_client.download_file(raw_data_bucket, name, local_download_path)\n",
    "            print(f\"[DOWNLOAD] ‚úÖ Saved to cache: {local_download_path}\")\n",
    "            print(f\"[DOWNLOAD] File size: {os.path.getsize(local_download_path):,} bytes\")\n",
    "            \n",
    "            # Copy to temp file for processing\n",
    "            import shutil\n",
    "            shutil.copy(local_download_path, temp_input_file)\n",
    "        \n",
    "        # Reproject using the temp file\n",
    "        print(f\"[REPROJECT] {name} ‚Üí {reproject_filename} (EPSG:4326)\")\n",
    "        with rasterio.open(temp_input_file) as src:\n",
    "            # Check current CRS\n",
    "            print(f\"[REPROJECT] Source CRS: {src.crs}\")\n",
    "            \n",
    "            dst_crs = \"EPSG:4326\"\n",
    "            \n",
    "            # Check if reprojection is needed\n",
    "            if src.crs and src.crs.to_string() == dst_crs:\n",
    "                print(f\"[REPROJECT] Already in {dst_crs}, skipping reprojection\")\n",
    "                # Just copy the file\n",
    "                import shutil\n",
    "                shutil.copy(temp_input_file, reproject_filename)\n",
    "            else:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "                )\n",
    "                kwargs = src.meta.copy()\n",
    "                kwargs.update({\n",
    "                    \"driver\": \"COG\",                 # write a COG instead of plain GTiff\n",
    "                    \"compress\": \"DEFLATE\",           # or \"LZW\"\n",
    "                    \"crs\": dst_crs,\n",
    "                    \"transform\": transform,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height\n",
    "                })\n",
    "\n",
    "                with rasterio.open(f\"{reproject_filename}\", \"w\", **kwargs) as dst:\n",
    "                    for band_idx in range(1, src.count + 1):\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, band_idx),\n",
    "                            destination=rasterio.band(dst, band_idx),\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=Resampling.nearest,\n",
    "                            wrapdateline=True\n",
    "                        )\n",
    "\n",
    "        # 3) COGify & upload\n",
    "        print(f\"[COGIFY] {reproject_filename} ‚Üí s3://{cog_data_bucket}/{s3_key}\")\n",
    "        ds = rxr.open_rasterio(reproject_filename)\n",
    "        \n",
    "        # Handle coordinate naming based on what's present\n",
    "        if \"y\" in ds.dims and \"x\" in ds.dims:\n",
    "            ds = ds.rename({\"y\": \"lat\", \"x\": \"lon\"})\n",
    "            ds.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n",
    "        elif \"lat\" not in ds.dims or \"lon\" not in ds.dims:\n",
    "            print(f\"[COGIFY] Warning: Unexpected dimension names: {list(ds.dims)}\")\n",
    "        \n",
    "        ds.rio.write_nodata(-9999, inplace=True)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "            tmp_name = tmp.name\n",
    "            ds.rio.to_raster(tmp_name, **COG_PROFILE)\n",
    "            \n",
    "            # Validate COG before uploading\n",
    "            print(f\"[VALIDATE] Checking if {cog_filename} is a valid COG...\")\n",
    "            is_valid_cog, validation_details = validate_cog(tmp_name)\n",
    "            \n",
    "            if is_valid_cog:\n",
    "                print(f\"[VALIDATE] ‚úÖ Valid COG confirmed:\")\n",
    "                print(f\"  - Tiled: {validation_details['has_tiles']} (tile size: {validation_details['tile_size']})\")\n",
    "                print(f\"  - Overviews: {len(validation_details['overview_levels'])} levels {validation_details['overview_levels']}\")\n",
    "                print(f\"  - Compression: {validation_details['compression']}\")\n",
    "            else:\n",
    "                print(f\"[VALIDATE] ‚ö†Ô∏è COG validation warnings:\")\n",
    "                if validation_details['errors']:\n",
    "                    for error in validation_details['errors']:\n",
    "                        print(f\"  - {error}\")\n",
    "                \n",
    "                # Decide whether to continue or fail based on severity\n",
    "                critical_errors = [e for e in validation_details['errors'] if 'Invalid driver' in e]\n",
    "                if critical_errors:\n",
    "                    raise ValueError(f\"Critical COG validation failed: {', '.join(critical_errors)}\")\n",
    "                else:\n",
    "                    print(f\"[VALIDATE] Proceeding with upload despite warnings...\")\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3_client.upload_file(\n",
    "                Filename = tmp_name, \n",
    "                Bucket = cog_data_bucket, \n",
    "                Key = s3_key)\n",
    "            print(f\"[SUCCESS] Uploaded to s3://{cog_data_bucket}/{s3_key}\")\n",
    "            \n",
    "            # Save locally if output directory is specified (this is for COGs, separate from downloads)\n",
    "            if local_output_dir:\n",
    "                os.makedirs(local_output_dir, exist_ok=True)\n",
    "                local_path = os.path.join(local_output_dir, cog_filename)\n",
    "                \n",
    "                # Copy the COG file to local directory\n",
    "                import shutil\n",
    "                shutil.copy(tmp_name, local_path)\n",
    "                print(f\"[LOCAL SAVE] Saved COG to {local_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {name}: {str(e)}\")\n",
    "        # Remove incomplete download if it exists\n",
    "        if not os.path.exists(local_download_path) or os.path.getsize(local_download_path) == 0:\n",
    "            if os.path.exists(local_download_path):\n",
    "                os.remove(local_download_path)\n",
    "                print(f\"[CLEANUP] Removed incomplete download: {local_download_path}\")\n",
    "        raise\n",
    "            \n",
    "    finally:\n",
    "        # Clean up temporary files (but NOT the cached download)\n",
    "        if os.path.exists(temp_input_file):\n",
    "            os.remove(temp_input_file)\n",
    "            print(f\"[CLEANUP] Removed temporary input file {temp_input_file}\")\n",
    "            \n",
    "        # Clean up local intermediate\n",
    "        if os.path.exists(reproject_filename):\n",
    "            os.remove(reproject_filename)\n",
    "            print(f\"[CLEANUP] Removed intermediate {reproject_filename}\")\n",
    "            \n",
    "        # Clean up temp COG file\n",
    "        if 'tmp_name' in locals() and os.path.exists(tmp_name):\n",
    "            os.remove(tmp_name)\n",
    "            print(f\"[CLEANUP] Removed temporary COG file\")\n",
    "        \n",
    "        # Report cache status\n",
    "        print(f\"[CACHE] Downloaded files are preserved in: {data_download_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_mask = [f for f in keys if \"_WM.tif\" in f]\n",
    "rgb = [f for f in keys if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in keys if \"WM_diff.tif\" in f]\n",
    "water_mask_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame to track processed files\n",
    "files_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\n",
    "\n",
    "# Get local output directory from config\n",
    "local_output_dir = config.get(\"local_output_dir\")\n",
    "\n",
    "# Create output directories\n",
    "if local_output_dir:\n",
    "    os.makedirs(local_output_dir, exist_ok=True)\n",
    "    print(f\"Local COGs will be saved to: {local_output_dir}\")\n",
    "\n",
    "# Process all files\n",
    "for name in sorted(keys):\n",
    "    cog_filename = create_cog_filename(name)\n",
    "    print(f\"\\nProcessing: {name}\")\n",
    "    print(f\"Output filename: {cog_filename}\")\n",
    "    \n",
    "    # Process the file with local output directory\n",
    "    convert_to_proper_CRS_and_cogify(name, cog_filename, cog_data_bucket, cog_data_prefix, local_output_dir)\n",
    "    \n",
    "    # Add to tracking DataFrame\n",
    "    files_processed = files_processed._append(\n",
    "        {\"file_name\": name, \"COGs_created\": cog_filename},\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    print(f\"Generated and saved COG: {cog_filename}\")\n",
    "\n",
    "print(\"\\nDone generating COGs\")\n",
    "if local_output_dir:\n",
    "    print(f\"COGs saved locally to: {local_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata if there are processed files\n",
    "if len(files_processed) > 0:\n",
    "    # Get metadata from one of the processed files\n",
    "    sample_file = files_processed.iloc[0]['file_name']\n",
    "    temp_sample_file = f\"temp_{os.path.basename(sample_file)}\"\n",
    "    \n",
    "    # Download sample file to extract metadata\n",
    "    s3_client.download_file(raw_data_bucket, sample_file, temp_sample_file)\n",
    "    \n",
    "    with rasterio.open(temp_sample_file) as src:\n",
    "        metadata = {\n",
    "            \"description\": src.tags(),\n",
    "            \"driver\": src.driver,\n",
    "            \"dtype\": str(src.dtypes[0]),\n",
    "            \"nodata\": src.nodata,\n",
    "            \"width\": src.width,\n",
    "            \"height\": src.height,\n",
    "            \"count\": src.count,\n",
    "            \"crs\": str(src.crs),\n",
    "            \"transform\": list(src.transform),\n",
    "            \"bounds\": list(src.bounds),\n",
    "            \"total_files_processed\": len(files_processed),\n",
    "            \"year\": \"2000\"\n",
    "        }\n",
    "    \n",
    "    # Upload metadata\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "        fp.flush()\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            Filename=fp.name,\n",
    "            Bucket=bucket_name,\n",
    "            Key=f\"{cog_data_prefix}/metadata.json\",\n",
    "        )\n",
    "        print(f\"Uploaded metadata to s3://{bucket_name}/{cog_data_prefix}/metadata.json\")\n",
    "    \n",
    "    # Clean up sample file\n",
    "    if os.path.exists(temp_sample_file):\n",
    "        os.remove(temp_sample_file)\n",
    "\n",
    "# Save the files_processed DataFrame to CSV using the same s3_client\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".csv\") as fp:\n",
    "    files_processed.to_csv(fp.name, index=False)\n",
    "    fp.flush()\n",
    "    \n",
    "    s3_client.upload_file(\n",
    "        Filename=fp.name,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{cog_data_prefix}/files_converted.csv\",\n",
    "    )\n",
    "    print(f\"Saved processing log to s3://{bucket_name}/{cog_data_prefix}/files_converted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(f\"\\nProcessing Summary:\")\n",
    "print(f\"Total files found: {len(keys)}\")\n",
    "print(f\"Files processed: {len(files_processed)}\")\n",
    "print(f\"\\nProcessed files:\")\n",
    "files_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Features in This Version\n",
    "\n",
    "This enhanced notebook includes several improvements over the original:\n",
    "\n",
    "### üîê **Automatic AWS Authentication**\n",
    "- No need for `.env` files or manual credential configuration\n",
    "- Automatically detects credentials from:\n",
    "  - Environment variables\n",
    "  - AWS CLI configuration\n",
    "  - IAM roles (EC2/Lambda)\n",
    "\n",
    "### üöÄ **Simplified Setup**\n",
    "- Removed dependency on `python-dotenv`\n",
    "- Direct boto3 client initialization\n",
    "- Better error handling for authentication issues\n",
    "\n",
    "### üìä **Additional Features**\n",
    "- fsspec filesystem integration for alternative S3 operations\n",
    "- Graceful handling of limited S3 permissions\n",
    "- Download caching to avoid re-downloading large files\n",
    "- COG validation before upload\n",
    "- Comprehensive error messages\n",
    "\n",
    "### üí° **Usage Tips**\n",
    "1. Ensure AWS credentials are configured via one of the standard methods\n",
    "2. The notebook will automatically detect and use available credentials\n",
    "3. Check the authentication cell output to confirm S3 access\n",
    "4. Use the cache management utilities to monitor downloaded files\n",
    "\n",
    "This enhanced version follows AWS best practices and makes the notebook more portable and easier to use across different environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
