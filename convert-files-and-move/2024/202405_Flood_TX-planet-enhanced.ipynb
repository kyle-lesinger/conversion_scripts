{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced S3 to COG Converter with Automatic AWS Authentication\n",
    "\n",
    "This notebook converts TIF files from S3 to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Automatic AWS credential detection** (no .env file needed)\n",
    "- **Download caching** to avoid re-downloading large files\n",
    "- **COG validation** before uploading\n",
    "- **Support for multiple AWS authentication methods**\n",
    "\n",
    "Author: Kyle Lesinger (Enhanced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import s3fs\n",
    "import fsspec\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Custom modules imported successfully!\n",
      "   Module path: /home/jovyan/conversion_scripts/convert-files-and-move/scripts\n"
     ]
    }
   ],
   "source": [
    "# Add path for importing custom modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the scripts directory to the Python path\n",
    "scripts_dir = Path('../scripts').resolve()\n",
    "if str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "# Import functions from list_s3crawler_files module\n",
    "from list_s3crawler_files import (\n",
    "    load_drcs_data,\n",
    "    get_tif_files_from_path,\n",
    "    get_files_with_full_paths,\n",
    "    list_available_directories\n",
    ")\n",
    "\n",
    "# Import COG and cache utilities\n",
    "from cog_utilities import (\n",
    "    check_cache_status,\n",
    "    clear_cache,\n",
    "    validate_cog\n",
    ")\n",
    "\n",
    "# Import AWS S3 utilities\n",
    "from aws_s3_utils import (\n",
    "    initialize_s3_client,\n",
    "    verify_s3_client,\n",
    "    get_all_s3_keys\n",
    ")\n",
    "\n",
    "# Import batch processing utilities\n",
    "from batch_processing import (\n",
    "    process_file_batch,\n",
    "    print_batch_summary\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom modules imported successfully!\")\n",
    "print(f\"   Module path: {scripts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "[drcs_activations OLD Directory](https://data.disasters.openveda.cloud/browseui/browseui/#drcs_activations/)\n",
    "\n",
    "[VEDA docs for file naming conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of new 2nd level directories\n",
    "\n",
    "    \"Sentinel-1\"\n",
    "    \"Sentinel-2\"\n",
    "    \"Landsat\"\n",
    "    \"MODIS\"\n",
    "    \"VIIRS\"\n",
    "    \"ASTER\"\n",
    "    \"MASTER\"\n",
    "    \"ECOSTRESS\"\n",
    "    \"Planet\"\n",
    "    \"Maxar\"\n",
    "    \"HLS\"\n",
    "    \"IMERG\"\n",
    "    \"GOES\"\n",
    "    \"SMAP\"\n",
    "    \"ICESat\"\n",
    "    \"GEDI\"\n",
    "    \"COMSAR\"\n",
    "    \"UAVSAR\"\n",
    "    \"WB-57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "DIR_OLD_BASE = 'drcs_activations'\n",
    "DIR_NEW_BASE = 'drcs_activations_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_NAME = '202405_Flood_TX'\n",
    "PRODUCT_NAME = 'sentinel1'\n",
    "\n",
    "RENAME_PRODUCT = 'Sentinel-1'\n",
    "\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}'  # Updated to use actual available directory\n",
    "DIRECTORY_NEW = f'{DIR_NEW_BASE}/{RENAME_PRODUCT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS S3 Client with automatic credential detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è S3 client initialized (limited bucket list access)\n",
      "‚úÖ Confirmed access to nasa-disasters bucket\n",
      "‚úÖ S3 filesystem (fsspec) initialized\n",
      "‚úÖ S3 client ready for operations\n",
      "   Bucket: nasa-disasters\n",
      "   Ready to process files\n",
      "‚úÖ Found 11 .tif files in the S3 bucket.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize AWS S3 Client using the imported function\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name='nasa-disasters', verbose=True)\n",
    "\n",
    "# Verify S3 client is ready using the imported function\n",
    "verify_s3_client(s3_client, bucket_name='nasa-disasters', verbose=True)\n",
    "\n",
    "# Get all TIF files using the imported function\n",
    "keys = get_all_s3_keys(s3_client, 'nasa-disasters', PATH_OLD, \".tif\") if s3_client else []\n",
    "\n",
    "if keys:\n",
    "    print(f\"‚úÖ Found {len(keys)} .tif files in the S3 bucket.\")\n",
    "else:\n",
    "    print(\"No keys found or S3 client not initialized\")\n",
    "    \n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TIF Files from DRCS Data\n",
    "### This may assist with diagnosing any issues that occur if no files are found in the code block above\n",
    "\n",
    "This cell loads the pre-analyzed DRCS activation data from `drcs_activations_tif_files.json` which contains a complete inventory of all .tif files in the NASA Disasters S3 bucket.\n",
    "\n",
    "The code will:\n",
    "1. Load the JSON file containing the file inventory\n",
    "2. Parse the `PATH_OLD` variable to find the corresponding directory\n",
    "3. Extract all .tif filenames from that directory\n",
    "4. Store them in `files_to_process` for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the pre-analyzed DRCS TIF files data using imported functions\n",
    "# # The JSON path is relative to the notebook location\n",
    "# json_path = Path('../../s3-crawler/drcs_activations_tif_files.json')\n",
    "\n",
    "# # Load DRCS data\n",
    "# drcs_data = load_drcs_data(json_path)\n",
    "\n",
    "# if drcs_data:\n",
    "#     # Get TIF files from the specified PATH_OLD using the imported function\n",
    "#     tif_files = get_tif_files_from_path(PATH_OLD, drcs_data, DIR_OLD_BASE)\n",
    "    \n",
    "#     if tif_files:\n",
    "#         print(f\"\\nüìÅ Found {len(tif_files)} .tif files in {PATH_OLD}:\")\n",
    "#         print(\"\\nFirst 10 files:\")\n",
    "#         for i, file in enumerate(tif_files[:10], 1):\n",
    "#             print(f\"  {i:2d}. {file}\")\n",
    "#         if len(tif_files) > 10:\n",
    "#             print(f\"  ... and {len(tif_files) - 10} more files\")\n",
    "        \n",
    "#         # Get files with full paths using the imported function\n",
    "#         files_to_process = get_files_with_full_paths(PATH_OLD, drcs_data, DIR_OLD_BASE, json_path)\n",
    "#         print(f\"\\n‚úÖ Files ready for processing. Stored in 'files_to_process' variable.\")\n",
    "#     else:\n",
    "#         print(f\"\\n‚ùå No files found. Please check the PATH_OLD variable.\")\n",
    "#         files_to_process = []\n",
    "# else:\n",
    "#     print(f\"\\n‚ùå Could not load DRCS data.\")\n",
    "#     files_to_process = []\n",
    "\n",
    "# files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: List available activation events using the imported function\n",
    "# print(\"üìÇ Available activation events in DRCS data:\")\n",
    "# events = list_available_directories('drcs_activations', drcs_data, json_path)\n",
    "\n",
    "# # Show first 10 events\n",
    "# for event in events[:10]:\n",
    "#     print(f\"  - {event}\")\n",
    "# if len(events) > 10:\n",
    "#     print(f\"  ... and {len(events) - 10} more events\")\n",
    "\n",
    "# # Example: List subdirectories for a specific event\n",
    "# print(f\"\\nüìÅ Subdirectories in {EVENT_NAME}:\")\n",
    "# subdirs = list_available_directories(f'drcs_activations/{EVENT_NAME}', drcs_data, json_path)\n",
    "# for subdir in subdirs:\n",
    "#     print(f\"  - {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For these we can see three different types of files\n",
    "\n",
    "1. WM = water mask\n",
    "2. rgb = red green blue\n",
    "3. WM_diff = water mask difference between dates\n",
    "\n",
    "### We are going to need 2 different directories for these!!!\n",
    "\n",
    "We will keep WaterMask (WM) and rgb as separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, let's use python list comprehension to return the files\n",
    "# We may need to rename them in different ways for different products\n",
    "# We will do a similar process later\n",
    "\n",
    "## NOTE --- We can actually use these objects since they have the same path as the s3 files. We will call them again later\n",
    "\n",
    "water_mask = [f for f in keys if \"_WM.tif\" in f]\n",
    "rgb = [f for f in keys if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in keys if \"WM_diff.tif\" in f]\n",
    "water_mask_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the diff files, we need to add diff at the 1st date before it\n",
    "# Otherwise VEDA will think that the first date is the most important\n",
    "\n",
    "# Example S1_diff20240430_20240507_WM.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_WM = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM\",  #We changed this!!!!!!!\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}\n",
    "\n",
    "config_rgb = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/rgb\", #We changed this!!!!!!!\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add configuration for water mask diff files\n",
    "config_WM_diff = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM_diff\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure bucket and paths (no need to create session manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/WM\n",
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/rgb\n"
     ]
    }
   ],
   "source": [
    "def return_bucket_info(config):\n",
    "    \"\"\"\n",
    "    Extract bucket information from configuration and return as dictionary.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary containing bucket and prefix information\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bucket and prefix information\n",
    "    \"\"\"\n",
    "    # Configure bucket and paths (no need to create session manually)\n",
    "    bucket_name = config[\"cog_data_bucket\"]\n",
    "    raw_data_bucket = config[\"raw_data_bucket\"]\n",
    "    raw_data_prefix = config[\"raw_data_prefix\"]\n",
    "    \n",
    "    cog_data_bucket = config['cog_data_bucket']\n",
    "    cog_data_prefix = config[\"cog_data_prefix\"]\n",
    "    \n",
    "    print(f\"Configuration loaded:\")\n",
    "    print(f\"  Source bucket: {raw_data_bucket}\")\n",
    "    print(f\"  Source prefix: {raw_data_prefix}\")\n",
    "    print(f\"  Target bucket: {cog_data_bucket}\")\n",
    "    print(f\"  Target prefix: {cog_data_prefix}\")\n",
    "\n",
    "    return {\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"raw_data_bucket\": raw_data_bucket,\n",
    "        \"raw_data_prefix\": raw_data_prefix,\n",
    "        \"cog_data_bucket\": cog_data_bucket,\n",
    "        \"cog_data_prefix\": cog_data_prefix\n",
    "    }\n",
    "\n",
    "# Use the function with config_WM\n",
    "wm_bucket = return_bucket_info(config_WM)\n",
    "rgb_bucket = return_bucket_info(config_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30T00:26:53Z\n"
     ]
    }
   ],
   "source": [
    "def convert_sentinel_datetime(datetime_str):\n",
    "    \"\"\"\n",
    "    Convert Sentinel datetime format to ISO 8601 format with UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        datetime_str: String like '20240430T002653'\n",
    "    \n",
    "    Returns:\n",
    "        String like '2024-04-30T00:26:53Z'\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    year = datetime_str[0:4]\n",
    "    month = datetime_str[4:6]\n",
    "    day = datetime_str[6:8]\n",
    "    hour = datetime_str[9:11]\n",
    "    minute = datetime_str[11:13]\n",
    "    second = datetime_str[13:15]\n",
    "    \n",
    "    # Format with dashes and colons, add Z for UTC\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}:{second}Z\"\n",
    "\n",
    "# Test\n",
    "datetime_str = '20240430T002653'\n",
    "result = convert_sentinel_datetime(datetime_str)\n",
    "print(result)  # 2024-04-30T00:26:53Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing WM filename:\n",
      "202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_2024-04-30T00:26:53Z.tif\n",
      "\n",
      "Testing RGB filename:\n",
      "202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_2024-04-30T00:26:53Z.tif\n"
     ]
    }
   ],
   "source": [
    "def create_cog_filename_rgb_WM(f, EVENT_NAME):\n",
    "    \"\"\"Create COG filename for RGB files.\"\"\"\n",
    "    f2 = Path(f).stem\n",
    "    fsplit = f2.split('_')\n",
    "    \n",
    "    # For RGB files, similar to WM but keep rgb suffix\n",
    "    cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'\n",
    "    return cog_filename\n",
    "\n",
    "\n",
    "# Test functions\n",
    "print(\"Testing WM filename:\")\n",
    "print(create_cog_filename_rgb_WM('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif', EVENT_NAME))\n",
    "\n",
    "print(\"\\nTesting RGB filename:\")\n",
    "print(create_cog_filename_rgb_WM('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif', EVENT_NAME))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define COG profile for rasterio\n",
    "COG_PROFILE = {\n",
    "    \"driver\": \"COG\",\n",
    "    \"compress\": \"DEFLATE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define COG Conversion Function\n",
    "\n",
    "This function handles the conversion of files to Cloud Optimized GeoTIFFs with proper CRS and caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ COG conversion function defined with smart nodata handling\n"
     ]
    }
   ],
   "source": [
    "def convert_to_proper_CRS_and_cogify(name, cog_filename, cog_data_bucket, cog_data_prefix, local_output_dir=None):\n",
    "    \"\"\"\n",
    "    Convert a file to Cloud Optimized GeoTIFF with proper CRS.\n",
    "    \n",
    "    This function includes:\n",
    "    - Download caching to avoid re-downloading files\n",
    "    - CRS reprojection to EPSG:4326\n",
    "    - COG validation before upload\n",
    "    - Upload to S3\n",
    "    - Smart nodata value handling based on data type\n",
    "    \"\"\"\n",
    "    s3_key = f\"{cog_data_prefix}/{cog_filename}\"\n",
    "    reproject_filename = f\"reproj/{cog_filename}\"\n",
    "    \n",
    "    # Create necessary directories\n",
    "    os.makedirs(\"reproj\", exist_ok=True)\n",
    "    \n",
    "    # Create data_download directory for caching\n",
    "    data_download_dir = \"data_download\"\n",
    "    os.makedirs(data_download_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectory structure to match S3 path\n",
    "    s3_path_parts = name.split('/')\n",
    "    local_subdir = os.path.join(data_download_dir, *s3_path_parts[:-1])\n",
    "    os.makedirs(local_subdir, exist_ok=True)\n",
    "    \n",
    "    # Local path for the downloaded file (persistent storage)\n",
    "    local_download_path = os.path.join(data_download_dir, name)\n",
    "    \n",
    "    # Temporary file for processing\n",
    "    temp_input_file = f\"temp_{os.path.basename(name)}\"\n",
    "\n",
    "    try:\n",
    "        # Check if file already exists locally\n",
    "        if os.path.exists(local_download_path):\n",
    "            print(f\"   [CACHE HIT] Using cached file: {local_download_path}\")\n",
    "            import shutil\n",
    "            shutil.copy(local_download_path, temp_input_file)\n",
    "        else:\n",
    "            # Download the file from S3\n",
    "            print(f\"   [DOWNLOAD] Downloading from S3...\")\n",
    "            s3_client.download_file(raw_data_bucket, name, local_download_path)\n",
    "            print(f\"   [DOWNLOAD] ‚úÖ Saved to cache\")\n",
    "            import shutil\n",
    "            shutil.copy(local_download_path, temp_input_file)\n",
    "        \n",
    "        # Reproject to EPSG:4326\n",
    "        print(f\"   [REPROJECT] Converting to EPSG:4326...\")\n",
    "        with rasterio.open(temp_input_file) as src:\n",
    "            dst_crs = \"EPSG:4326\"\n",
    "            \n",
    "            # Check if reprojection is needed\n",
    "            if src.crs and src.crs.to_string() == dst_crs:\n",
    "                print(f\"   [REPROJECT] Already in {dst_crs}, skipping reprojection\")\n",
    "                import shutil\n",
    "                shutil.copy(temp_input_file, reproject_filename)\n",
    "            else:\n",
    "                transform, width, height = calculate_default_transform(\n",
    "                    src.crs, dst_crs, src.width, src.height, *src.bounds\n",
    "                )\n",
    "                kwargs = src.meta.copy()\n",
    "                kwargs.update({\n",
    "                    \"driver\": \"COG\",\n",
    "                    \"compress\": \"DEFLATE\",\n",
    "                    \"crs\": dst_crs,\n",
    "                    \"transform\": transform,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height\n",
    "                })\n",
    "\n",
    "                with rasterio.open(reproject_filename, \"w\", **kwargs) as dst:\n",
    "                    for band_idx in range(1, src.count + 1):\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, band_idx),\n",
    "                            destination=rasterio.band(dst, band_idx),\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=Resampling.nearest,\n",
    "                            wrapdateline=True\n",
    "                        )\n",
    "\n",
    "        # COGify & upload\n",
    "        print(f\"   [COGIFY] Creating COG...\")\n",
    "        ds = rxr.open_rasterio(reproject_filename)\n",
    "        \n",
    "        # Handle coordinate naming\n",
    "        if \"y\" in ds.dims and \"x\" in ds.dims:\n",
    "            ds = ds.rename({\"y\": \"lat\", \"x\": \"lon\"})\n",
    "            ds.rio.set_spatial_dims(\"lon\", \"lat\", inplace=True)\n",
    "        \n",
    "        # Smart nodata value handling based on data type\n",
    "        print(f\"   [NODATA] Data type: {ds.dtype}\")\n",
    "        if ds.dtype == 'uint8':\n",
    "            # For RGB images (uint8), use 0 as nodata (black pixels)\n",
    "            nodata_value = 0\n",
    "            print(f\"   [NODATA] Using nodata value {nodata_value} for uint8 data\")\n",
    "        elif ds.dtype == 'uint16':\n",
    "            # For uint16, use 0 as nodata\n",
    "            nodata_value = 0\n",
    "            print(f\"   [NODATA] Using nodata value {nodata_value} for uint16 data\")\n",
    "        else:\n",
    "            # For float32, int16, int32, etc., use -9999\n",
    "            nodata_value = -9999\n",
    "            print(f\"   [NODATA] Using nodata value {nodata_value} for {ds.dtype} data\")\n",
    "        \n",
    "        ds.rio.write_nodata(nodata_value, inplace=True)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(suffix='.tif', delete=False) as tmp:\n",
    "            tmp_name = tmp.name\n",
    "            ds.rio.to_raster(tmp_name, **COG_PROFILE)\n",
    "            \n",
    "            # Validate COG\n",
    "            print(f\"   [VALIDATE] Checking COG validity...\")\n",
    "            is_valid_cog, validation_details = validate_cog(tmp_name)\n",
    "            \n",
    "            if is_valid_cog:\n",
    "                print(f\"   [VALIDATE] ‚úÖ Valid COG\")\n",
    "            else:\n",
    "                print(f\"   [VALIDATE] ‚ö†Ô∏è COG validation warnings\")\n",
    "                critical_errors = [e for e in validation_details['errors'] if 'Invalid driver' in e]\n",
    "                if critical_errors:\n",
    "                    raise ValueError(f\"Critical COG validation failed\")\n",
    "            \n",
    "            # Upload to S3\n",
    "            print(f\"   [UPLOAD] Uploading to S3...\")\n",
    "            s3_client.upload_file(\n",
    "                Filename=tmp_name,\n",
    "                Bucket=cog_data_bucket,\n",
    "                Key=s3_key\n",
    "            )\n",
    "            print(f\"   [SUCCESS] ‚úÖ Uploaded to s3://{cog_data_bucket}/{s3_key}\")\n",
    "            \n",
    "            # Save locally if specified\n",
    "            if local_output_dir:\n",
    "                os.makedirs(local_output_dir, exist_ok=True)\n",
    "                local_path = os.path.join(local_output_dir, cog_filename)\n",
    "                import shutil\n",
    "                shutil.copy(tmp_name, local_path)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed: {str(e)}\")\n",
    "        raise\n",
    "            \n",
    "    finally:\n",
    "        # Clean up temporary files\n",
    "        for temp_file in [temp_input_file, reproject_filename]:\n",
    "            if os.path.exists(temp_file):\n",
    "                os.remove(temp_file)\n",
    "        if 'tmp_name' in locals() and os.path.exists(tmp_name):\n",
    "            os.remove(tmp_name)\n",
    "\n",
    "print(\"‚úÖ COG conversion function defined with smart nodata handling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Cache Status:\n",
      "  - Directory: data_download/\n",
      "  - Total files: 9\n",
      "  - Total size: 0.87 GB\n",
      "\n",
      "üìÅ Cached files (first 10):\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif (3.1 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif (258.2 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif (2.1 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif (289.2 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif (2.7 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif (321.6 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif (9.1 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif (2.4 MB)\n",
      "  - drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif (2.7 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 934413481)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current cache status using the imported function\n",
    "check_cache_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process RGB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate files by type\n",
    "water_mask = [f for f in keys if \"_WM.tif\" in f and \"WM_diff\" not in f]\n",
    "rgb = [f for f in keys if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in keys if \"WM_diff.tif\" in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing WM filename:\n",
      "  202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_WM_2024-04-30T00:26:53Z.tif\n",
      "\n",
      "Testing RGB filename:\n",
      "  202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_rgb_2024-04-30T00:26:53Z.tif\n",
      "\n",
      "Testing diff filename:\n",
      "  202405_Flood_TX_S1_WM_diff20240430_20240507.tif\n"
     ]
    }
   ],
   "source": [
    "# Define filename creator functions for different file types\n",
    "\n",
    "def create_cog_filename_WM(f, EVENT_NAME):\n",
    "    \"\"\"Create COG filename for water mask files.\"\"\"\n",
    "    f2 = Path(f).stem\n",
    "    fsplit = f2.split('_')\n",
    "    \n",
    "    # Check if it's a diff file\n",
    "    if \"WM_diff\" in f:\n",
    "        # For diff files: S1_20240430_20240507_WM_diff.tif\n",
    "        # Need to add \"diff\" before the first date\n",
    "        # Result: S1_diff20240430_20240507_WM.tif\n",
    "        return f'{EVENT_NAME}_S1_WM_diff{fsplit[1]}_{fsplit[2]}.tif'\n",
    "    else:\n",
    "        # Regular WM files\n",
    "        cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_WM_{convert_sentinel_datetime(fsplit[2])}.tif'\n",
    "        return cog_filename\n",
    "\n",
    "def create_cog_filename_rgb(f, EVENT_NAME):\n",
    "    \"\"\"Create COG filename for RGB files.\"\"\"\n",
    "    f2 = Path(f).stem\n",
    "    fsplit = f2.split('_')\n",
    "    \n",
    "    # For RGB files, similar to WM but keep rgb suffix in the output\n",
    "    # Extract parts and rebuild with rgb indicator\n",
    "    cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_rgb_{convert_sentinel_datetime(fsplit[2])}.tif'\n",
    "    return cog_filename\n",
    "\n",
    "def create_cog_filename_diff(f, EVENT_NAME):\n",
    "    \"\"\"Create COG filename for water mask diff files.\"\"\"\n",
    "    f2 = Path(f).stem\n",
    "    fsplit = f2.split('_')\n",
    "    \n",
    "    # For diff files: S1_20240430_20240507_WM_diff.tif\n",
    "    # Need to add \"diff\" before the first date\n",
    "    # Result: 202405_Flood_TX_S1_diff20240430_20240507_WM.tif\n",
    "    return f'{EVENT_NAME}_S1_WM_diff{fsplit[1]}_{fsplit[2]}.tif'\n",
    "\n",
    "# Test functions\n",
    "print(\"Testing WM filename:\")\n",
    "test_wm = create_cog_filename_WM('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif', EVENT_NAME)\n",
    "print(f\"  {test_wm}\")\n",
    "\n",
    "print(\"\\nTesting RGB filename:\")\n",
    "test_rgb = create_cog_filename_rgb('drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif', EVENT_NAME)\n",
    "print(f\"  {test_rgb}\")\n",
    "\n",
    "print(\"\\nTesting diff filename:\")\n",
    "test_diff = create_cog_filename_diff('drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif', EVENT_NAME)\n",
    "print(f\"  {test_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä File categorization:\n",
      "  - Water mask files: 6\n",
      "  - RGB files: 3\n",
      "  - Water mask diff files: 2\n",
      "  - Total files: 11\n",
      "\n",
      "==================================================\n",
      "üåä Processing Water Mask Files\n",
      "==================================================\n",
      "‚úÖ Local output directory ready: output/202405_Flood_TX\n",
      "\n",
      "[1/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_WM_2024-04-30T00:26:53Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_WM_2024-04-30T00:26:53Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_WM_2024-04-30T00:26:53Z.tif\n",
      "\n",
      "[2/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_WM_2024-04-30T00:27:19Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_WM_2024-04-30T00:27:19Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_WM_2024-04-30T00:27:19Z.tif\n",
      "\n",
      "[3/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_WM_2024-05-07T12:23:23Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_WM_2024-05-07T12:23:23Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_WM_2024-05-07T12:23:23Z.tif\n",
      "\n",
      "[4/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_EC9C_WM_2024-05-12T00:26:55Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_EC9C_WM_2024-05-12T00:26:55Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_EC9C_WM_2024-05-12T00:26:55Z.tif\n",
      "\n",
      "[5/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_D32B_WM_2024-05-12T00:27:20Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_D32B_WM_2024-05-12T00:27:20Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_D32B_WM_2024-05-12T00:27:20Z.tif\n",
      "\n",
      "[6/6] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_3F78_WM_2024-05-12T00:27:45Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_3F78_WM_2024-05-12T00:27:45Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_3F78_WM_2024-05-12T00:27:45Z.tif\n",
      "\n",
      "‚úÖ Batch processing complete: 6 files processed\n",
      "üìä Uploaded metadata to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/metadata.json\n",
      "üìù Saved processing log to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM/files_converted.csv\n",
      "üìÅ COGs saved locally to: output/202405_Flood_TX\n",
      "\n",
      "==================================================\n",
      "üé® Processing RGB Files\n",
      "==================================================\n",
      "‚úÖ Local output directory ready: output/202405_Flood_TX\n",
      "\n",
      "[1/3] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_rgb_2024-04-30T00:26:53Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/rgb/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_rgb_2024-04-30T00:26:53Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_rgb_2024-04-30T00:26:53Z.tif\n",
      "\n",
      "[2/3] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_rgb_2024-04-30T00:27:19Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/rgb/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_rgb_2024-04-30T00:27:19Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_F141_rgb_2024-04-30T00:27:19Z.tif\n",
      "\n",
      "[3/3] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif\n",
      "   Output filename: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_rgb_2024-05-07T12:23:23Z.tif\n",
      "   [CACHE HIT] Using cached file: data_download/drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif\n",
      "   [REPROJECT] Converting to EPSG:4326...\n",
      "   [COGIFY] Creating COG...\n",
      "   [NODATA] Data type: uint8\n",
      "   [NODATA] Using nodata value 0 for uint8 data\n",
      "   [VALIDATE] Checking COG validity...\n",
      "   [VALIDATE] ‚ö†Ô∏è COG validation warnings\n",
      "   [UPLOAD] Uploading to S3...\n",
      "   [SUCCESS] ‚úÖ Uploaded to s3://nasa-disasters/drcs_activations_new/Sentinel-1/rgb/202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_rgb_2024-05-07T12:23:23Z.tif\n",
      "   ‚úÖ Generated and saved COG: 202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_5BA0_rgb_2024-05-07T12:23:23Z.tif\n",
      "\n",
      "‚úÖ Batch processing complete: 3 files processed\n",
      "üìä Uploaded metadata to s3://nasa-disasters/drcs_activations_new/Sentinel-1/rgb/metadata.json\n",
      "üìù Saved processing log to s3://nasa-disasters/drcs_activations_new/Sentinel-1/rgb/files_converted.csv\n",
      "üìÅ COGs saved locally to: output/202405_Flood_TX\n",
      "\n",
      "==================================================\n",
      "üîÑ Processing Water Mask Diff Files\n",
      "==================================================\n",
      "‚úÖ Local output directory ready: output/202405_Flood_TX\n",
      "\n",
      "[1/2] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif\n",
      "   Output filename: 202405_Flood_TX_S1_WM_diff20240430_20240507.tif\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [ERROR] Failed: name 'raw_data_bucket' is not defined\n",
      "   ‚ùå Error processing drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif: name 'raw_data_bucket' is not defined\n",
      "\n",
      "[2/2] Processing: drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif\n",
      "   Output filename: 202405_Flood_TX_S1_WM_diff20240507_20240512.tif\n",
      "   [DOWNLOAD] Downloading from S3...\n",
      "   [ERROR] Failed: name 'raw_data_bucket' is not defined\n",
      "   ‚ùå Error processing drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif: name 'raw_data_bucket' is not defined\n",
      "\n",
      "‚úÖ Batch processing complete: 2 files processed\n",
      "‚ö†Ô∏è No successfully processed files to extract metadata from\n",
      "üìù Saved processing log to s3://nasa-disasters/drcs_activations_new/Sentinel-1/WM_diff/files_converted.csv\n",
      "üìÅ COGs saved locally to: output/202405_Flood_TX\n",
      "\n",
      "==================================================\n",
      "üìä BATCH PROCESSING SUMMARY\n",
      "==================================================\n",
      "Total files processed: 11\n",
      "Successful: 9\n",
      "Failed: 2\n",
      "Success rate: 81.8%\n",
      "Timestamp: 2025-08-29T15:47:35.822761\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "## Process files using batch processing function\n",
    "\n",
    "print(\"üìä File categorization:\")\n",
    "print(f\"  - Water mask files: {len(water_mask)}\")\n",
    "print(f\"  - RGB files: {len(rgb)}\")\n",
    "print(f\"  - Water mask diff files: {len(water_mask_diff)}\")\n",
    "print(f\"  - Total files: {len(keys)}\")\n",
    "\n",
    "# Initialize combined results DataFrame\n",
    "all_files_processed = pd.DataFrame()\n",
    "\n",
    "# Process water mask files\n",
    "if water_mask:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üåä Processing Water Mask Files\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    wm_results = process_file_batch(\n",
    "        file_list=water_mask,\n",
    "        s3_client=s3_client,\n",
    "        config=config_WM,\n",
    "        filename_creator_func=create_cog_filename_WM,\n",
    "        processing_func=convert_to_proper_CRS_and_cogify,\n",
    "        event_name=EVENT_NAME,\n",
    "        save_metadata=True,\n",
    "        save_csv=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    all_files_processed = pd.concat([all_files_processed, wm_results], ignore_index=True)\n",
    "\n",
    "# Process RGB files\n",
    "if rgb:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üé® Processing RGB Files\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    rgb_results = process_file_batch(\n",
    "        file_list=rgb,\n",
    "        s3_client=s3_client,\n",
    "        config=config_rgb,\n",
    "        filename_creator_func=create_cog_filename_rgb,\n",
    "        processing_func=convert_to_proper_CRS_and_cogify,\n",
    "        event_name=EVENT_NAME,\n",
    "        save_metadata=True,\n",
    "        save_csv=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    all_files_processed = pd.concat([all_files_processed, rgb_results], ignore_index=True)\n",
    "\n",
    "# Process water mask diff files\n",
    "if water_mask_diff:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üîÑ Processing Water Mask Diff Files\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    diff_results = process_file_batch(\n",
    "        file_list=water_mask_diff,\n",
    "        s3_client=s3_client,\n",
    "        config=config_WM_diff,\n",
    "        filename_creator_func=create_cog_filename_diff,\n",
    "        processing_func=convert_to_proper_CRS_and_cogify,\n",
    "        event_name=EVENT_NAME,\n",
    "        save_metadata=True,\n",
    "        save_csv=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    all_files_processed = pd.concat([all_files_processed, diff_results], ignore_index=True)\n",
    "\n",
    "# Print overall summary\n",
    "print_batch_summary(all_files_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final results\n",
    "print(f\"\\nüìä Final Processing Results:\")\n",
    "print(f\"Total files processed: {len(all_files_processed)}\")\n",
    "print(f\"\\nProcessed files DataFrame:\")\n",
    "all_files_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check STATUS\n",
    "[Disasters Bucket](https://data.disasters.openveda.cloud/browseui/browseui/#drcs_activations_new/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
