{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced S3 to COG Converter with Automatic AWS Authentication\n",
    "\n",
    "This notebook converts TIF files from S3 to Cloud Optimized GeoTIFFs (COGs) with:\n",
    "- **Automatic AWS credential detection** (no .env file needed)\n",
    "- **Download caching** to avoid re-downloading large files\n",
    "- **COG validation** before uploading\n",
    "- **Support for multiple AWS authentication methods**\n",
    "\n",
    "Author: Kyle Lesinger (Enhanced version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "Boto3 version: 1.37.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import tempfile\n",
    "import boto3\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import s3fs\n",
    "import fsspec\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom modules imported successfully!\n",
      "   Module path: /home/jovyan/conversion_scripts/convert-files-and-move/scripts\n"
     ]
    }
   ],
   "source": [
    "# Add path for importing custom modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the scripts directory to the Python path\n",
    "scripts_dir = Path('../scripts').resolve()\n",
    "if str(scripts_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "# Import functions from list_s3crawler_files module\n",
    "from list_s3crawler_files import (\n",
    "    load_drcs_data,\n",
    "    get_tif_files_from_path,\n",
    "    get_files_with_full_paths,\n",
    "    list_available_directories\n",
    ")\n",
    "\n",
    "# Import COG and cache utilities\n",
    "from cog_utilities import (\n",
    "    check_cache_status,\n",
    "    clear_cache,\n",
    "    validate_cog\n",
    ")\n",
    "\n",
    "# Import AWS S3 utilities\n",
    "from aws_s3_utils import (\n",
    "    initialize_s3_client,\n",
    "    verify_s3_client,\n",
    "    get_all_s3_keys\n",
    ")\n",
    "\n",
    "print(\"✅ Custom modules imported successfully!\")\n",
    "print(f\"   Module path: {scripts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful links\n",
    "\n",
    "[drcs_activations OLD Directory](https://data.disasters.openveda.cloud/browseui/browseui/#drcs_activations/)\n",
    "\n",
    "[VEDA docs for file naming conventions](https://docs.openveda.cloud/user-guide/content-curation/dataset-ingestion/file-preparation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of new 2nd level directories\n",
    "\n",
    "    \"Sentinel-1\"\n",
    "    \"Sentinel-2\"\n",
    "    \"Landsat\"\n",
    "    \"MODIS\"\n",
    "    \"VIIRS\"\n",
    "    \"ASTER\"\n",
    "    \"MASTER\"\n",
    "    \"ECOSTRESS\"\n",
    "    \"Planet\"\n",
    "    \"Maxar\"\n",
    "    \"HLS\"\n",
    "    \"IMERG\"\n",
    "    \"GOES\"\n",
    "    \"SMAP\"\n",
    "    \"ICESat\"\n",
    "    \"GEDI\"\n",
    "    \"COMSAR\"\n",
    "    \"UAVSAR\"\n",
    "    \"WB-57\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting editing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "DIR_OLD_BASE = 'drcs_activations'\n",
    "DIR_NEW_BASE = 'drcs_activations_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVENT_NAME = '202405_Flood_TX'\n",
    "PRODUCT_NAME = 'sentinel1'\n",
    "\n",
    "RENAME_PRODUCT = 'Sentinel-1'\n",
    "\n",
    "PATH_OLD = f'{DIR_OLD_BASE}/{EVENT_NAME}/{PRODUCT_NAME}'  # Updated to use actual available directory\n",
    "DIRECTORY_NEW = f'{DIR_NEW_BASE}/{RENAME_PRODUCT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize AWS S3 Client with automatic credential detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ S3 client initialized (limited bucket list access)\n",
      "✅ Confirmed access to nasa-disasters bucket\n",
      "✅ S3 filesystem (fsspec) initialized\n",
      "✅ S3 client ready for operations\n",
      "   Bucket: nasa-disasters\n",
      "   Ready to process files\n",
      "✅ Found 11 .tif files in the S3 bucket. These are the files we will be processing!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002719_DVR_RTC20_G_gpuned_F141_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240507T122323_DVR_RTC20_G_gpuned_5BA0_rgb.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002655_DVR_RTC20_G_gpuned_EC9C_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002720_DVR_RTC20_G_gpuned_D32B_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240512T002745_DVR_RTC20_G_gpuned_3F78_WM.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize AWS S3 Client using the imported function\n",
    "s3_client, fs_read = initialize_s3_client(bucket_name='nasa-disasters', verbose=True)\n",
    "# Verify S3 client is ready using the imported function\n",
    "verify_s3_client(s3_client, bucket_name='nasa-disasters', verbose=True)\n",
    "\n",
    "# Get all TIF files using the imported function\n",
    "keys = get_all_s3_keys(s3_client, 'nasa-disasters', PATH_OLD, \".tif\") if s3_client else []\n",
    "\n",
    "if keys:\n",
    "    print(f\"✅ Found {len(keys)} .tif files in the S3 bucket. These are the files we will be processing!\")\n",
    "else:\n",
    "    print(\"No keys found or S3 client not initialized\")\n",
    "    \n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File types\n",
    "### For these we can see three different types of files\n",
    "\n",
    "1. WM = water mask\n",
    "2. rgb = red green blue\n",
    "3. WM_diff = water mask difference between dates\n",
    "\n",
    "### We are going to need 2 different directories for these!!!\n",
    "\n",
    "We will keep WaterMask (WM) and rgb as separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drcs_activations/202405_Flood_TX/sentinel1/S1_20240430_20240507_WM_diff.tif',\n",
       " 'drcs_activations/202405_Flood_TX/sentinel1/S1_20240507_20240512_WM_diff.tif']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For simplicity, let's use python list comprehension to return the files\n",
    "# We may need to rename them in different ways for different products\n",
    "# We will do a similar process later\n",
    "\n",
    "## NOTE --- We can actually use these objects since they have the same path as the s3 files. We will call them again later\n",
    "\n",
    "water_mask = [f for f in keys if \"_WM.tif\" in f]\n",
    "rgb = [f for f in keys if \"rgb.tif\" in f]\n",
    "water_mask_diff = [f for f in keys if \"WM_diff.tif\" in f]\n",
    "water_mask_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE for the diff files, we need to add diff at the 1st date before it\n",
    "# Otherwise VEDA will think that the first date is the most important\n",
    "\n",
    "# Example S1_diff20240430_20240507_WM.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TIF Files from DRCS Data\n",
    "### If no files are found in the previous block (for keys). This section could help to diagnose any issues.\n",
    "\n",
    "This cell loads the pre-analyzed DRCS activation data from `drcs_activations_tif_files.json` which contains a complete inventory of all .tif files in the NASA Disasters S3 bucket.\n",
    "\n",
    "The code will:\n",
    "1. Load the JSON file containing the file inventory\n",
    "2. Parse the `PATH_OLD` variable to find the corresponding directory\n",
    "3. Extract all .tif filenames from that directory\n",
    "4. Store them in `files_to_process` for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the pre-analyzed DRCS TIF files data using imported functions\n",
    "# # The JSON path is relative to the notebook location\n",
    "\n",
    "# # Load DRCS data\n",
    "# drcs_data = load_drcs_data(Path('../../s3-crawler/drcs_activations_tif_files.json'))\n",
    "\n",
    "# if drcs_data:\n",
    "#     # Get TIF files from the specified PATH_OLD using the imported function\n",
    "#     tif_files = get_tif_files_from_path(PATH_OLD, drcs_data, DIR_OLD_BASE)\n",
    "    \n",
    "#     if tif_files:\n",
    "#         print(f\"\\n📁 Found {len(tif_files)} .tif files in {PATH_OLD}:\")\n",
    "#         print(\"\\nFirst 10 files:\")\n",
    "#         for i, file in enumerate(tif_files[:10], 1):\n",
    "#             print(f\"  {i:2d}. {file}\")\n",
    "#         if len(tif_files) > 10:\n",
    "#             print(f\"  ... and {len(tif_files) - 10} more files\")\n",
    "        \n",
    "#         # Get files with full paths using the imported function\n",
    "#         files_to_process = get_files_with_full_paths(PATH_OLD, drcs_data, DIR_OLD_BASE, json_path)\n",
    "#         print(f\"\\n✅ Files ready for processing. Stored in 'files_to_process' variable.\")\n",
    "#     else:\n",
    "#         print(f\"\\n❌ No files found. Please check the PATH_OLD variable.\")\n",
    "#         files_to_process = []\n",
    "# else:\n",
    "#     print(f\"\\n❌ Could not load DRCS data.\")\n",
    "#     files_to_process = []\n",
    "# files_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: List available activation events using the imported function\n",
    "# print(\"📂 Available activation events in DRCS data:\")\n",
    "# events = list_available_directories('drcs_activations', drcs_data, json_path)\n",
    "\n",
    "# # Show first 10 events\n",
    "# for event in events[:10]:\n",
    "#     print(f\"  - {event}\")\n",
    "# if len(events) > 10:\n",
    "#     print(f\"  ... and {len(events) - 10} more events\")\n",
    "\n",
    "# # Example: List subdirectories for a specific event\n",
    "# print(f\"\\n📁 Subdirectories in {EVENT_NAME}:\")\n",
    "# subdirs = list_available_directories(f'drcs_activations/{EVENT_NAME}', drcs_data, json_path)\n",
    "# for subdir in subdirs:\n",
    "#     print(f\"  - {subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure bucket and paths (no need to create session manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                  \n",
    "def return_bucket_info(config):\n",
    "   \"\"\"\n",
    "   Extract bucket information from configuration and return as dictionary.\n",
    "   \n",
    "   Args:\n",
    "       config: Configuration dictionary containing bucket and prefix information\n",
    "   \n",
    "   Returns:\n",
    "       Dictionary with bucket and prefix information\n",
    "   \"\"\"\n",
    "   # Configure bucket and paths (no need to create session manually)\n",
    "   bucket_name = config[\"cog_data_bucket\"]\n",
    "   raw_data_bucket = config[\"raw_data_bucket\"]\n",
    "   raw_data_prefix = config[\"raw_data_prefix\"]\n",
    "\n",
    "   cog_data_bucket = config['cog_data_bucket']\n",
    "   cog_data_prefix = config[\"cog_data_prefix\"]\n",
    "\n",
    "   print(f\"Configuration loaded:\")\n",
    "   print(f\"  Source bucket: {raw_data_bucket}\")\n",
    "   print(f\"  Source prefix: {raw_data_prefix}\")\n",
    "   print(f\"  Target bucket: {cog_data_bucket}\")\n",
    "   print(f\"  Target prefix: {cog_data_prefix}\")\n",
    "\n",
    "   return {\n",
    "       \"bucket_name\": bucket_name,\n",
    "       \"raw_data_bucket\": raw_data_bucket,\n",
    "       \"raw_data_prefix\": raw_data_prefix,\n",
    "       \"cog_data_bucket\": cog_data_bucket,\n",
    "       \"cog_data_prefix\": cog_data_prefix\n",
    "   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to make a new set of configs for every different file type... These will be the final directory in the new S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_WM = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/WM\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}\n",
    "\n",
    "config_rgb = {\n",
    "    \"data_acquisition_method\": \"s3\",\n",
    "    \"raw_data_bucket\" : \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"raw_data_prefix\": F\"{PATH_OLD}\",\n",
    "    \"cog_data_bucket\": \"nasa-disasters\", #DO NOT CHANGE\n",
    "    \"cog_data_prefix\": f\"{DIRECTORY_NEW}/rgb\",\n",
    "    \"local_output_dir\": f\"output/{EVENT_NAME}\",  # Local directory to save COGs\n",
    "    \"transformation\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/WM\n",
      "Configuration loaded:\n",
      "  Source bucket: nasa-disasters\n",
      "  Source prefix: drcs_activations/202405_Flood_TX/sentinel1\n",
      "  Target bucket: nasa-disasters\n",
      "  Target prefix: drcs_activations_new/Sentinel-1/rgb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bucket_name': 'nasa-disasters',\n",
       " 'raw_data_bucket': 'nasa-disasters',\n",
       " 'raw_data_prefix': 'drcs_activations/202405_Flood_TX/sentinel1',\n",
       " 'cog_data_bucket': 'nasa-disasters',\n",
       " 'cog_data_prefix': 'drcs_activations_new/Sentinel-1/rgb'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create bucket information\n",
    "wm_bucket = return_bucket_info(config_WM)\n",
    "rgb_bucket = return_bucket_info(config_rgb)\n",
    "\n",
    "rgb_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-30T00:26:53Z\n"
     ]
    }
   ],
   "source": [
    "def convert_sentinel_datetime(datetime_str):\n",
    "    \"\"\"\n",
    "    Convert Sentinel datetime format to ISO 8601 format with UTC timezone.\n",
    "    \n",
    "    Args:\n",
    "        datetime_str: String like '20240430T002653'\n",
    "    \n",
    "    Returns:\n",
    "        String like '2024-04-30T00:26:53Z'\n",
    "    \"\"\"\n",
    "    # Extract components\n",
    "    year = datetime_str[0:4]\n",
    "    month = datetime_str[4:6]\n",
    "    day = datetime_str[6:8]\n",
    "    hour = datetime_str[9:11]\n",
    "    minute = datetime_str[11:13]\n",
    "    second = datetime_str[13:15]\n",
    "    \n",
    "    # Format with dashes and colons, add Z for UTC\n",
    "    return f\"{year}-{month}-{day}T{hour}:{minute}:{second}Z\"\n",
    "\n",
    "# Test\n",
    "datetime_str = '20240430T002653'\n",
    "result = convert_sentinel_datetime(datetime_str)\n",
    "print(result)  # 2024-04-30T00:26:53Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [File Types](#file-types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'202405_Flood_TX_S1A_IW_DVR_RTC20_G_gpuned_0610_2024-04-30T00:26:53Z.tif'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_cog_filename_WM_and_rgb(f, EVENT_NAME,):\n",
    "    \n",
    "    f2 = Path(f).stem\n",
    "    f2\n",
    "    fsplit = f2.split('_')\n",
    "    fsplit\n",
    "    \n",
    "    cog_filename = f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'\n",
    "\n",
    "    return cog_filename\n",
    "\n",
    "# Test function below\n",
    "# create_cog_filename_WM_and_rgb(f='drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_WM.tif', EVENT_NAME=EVENT_NAME)\n",
    "create_cog_filename_WM_and_rgb(f='drcs_activations/202405_Flood_TX/sentinel1/S1A_IW_20240430T002653_DVR_RTC20_G_gpuned_0610_rgb.tif', EVENT_NAME=EVENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f'{EVENT_NAME}_{\"_\".join(fsplit[0:2])}_{\"_\".join(fsplit[3:8])}_{convert_sentinel_datetime(fsplit[2])}.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define COG profile for rasterio\n",
    "COG_PROFILE = {\n",
    "    \"driver\": \"COG\",\n",
    "    \"compress\": \"DEFLATE\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Cache directory does not exist: data_download/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check current cache status using the imported function\n",
    "check_cache_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame to track processed files\n",
    "files_processed = pd.DataFrame(columns=[\"file_name\", \"COGs_created\"])\n",
    "\n",
    "# Get local output directory from config\n",
    "local_output_dir = config.get(\"local_output_dir\")\n",
    "\n",
    "# Create output directories\n",
    "if local_output_dir:\n",
    "    os.makedirs(local_output_dir, exist_ok=True)\n",
    "    print(f\"Local COGs will be saved to: {local_output_dir}\")\n",
    "\n",
    "# Process all files\n",
    "for name in sorted(keys):\n",
    "    cog_filename = create_cog_filename(name)\n",
    "    print(f\"\\nProcessing: {name}\")\n",
    "    print(f\"Output filename: {cog_filename}\")\n",
    "    \n",
    "    # Process the file with local output directory\n",
    "    convert_to_proper_CRS_and_cogify(name, cog_filename, cog_data_bucket, cog_data_prefix, local_output_dir)\n",
    "    \n",
    "    # Add to tracking DataFrame\n",
    "    files_processed = files_processed._append(\n",
    "        {\"file_name\": name, \"COGs_created\": cog_filename},\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    print(f\"Generated and saved COG: {cog_filename}\")\n",
    "\n",
    "print(\"\\nDone generating COGs\")\n",
    "if local_output_dir:\n",
    "    print(f\"COGs saved locally to: {local_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata if there are processed files\n",
    "if len(files_processed) > 0:\n",
    "    # Get metadata from one of the processed files\n",
    "    sample_file = files_processed.iloc[0]['file_name']\n",
    "    temp_sample_file = f\"temp_{os.path.basename(sample_file)}\"\n",
    "    \n",
    "    # Download sample file to extract metadata\n",
    "    s3_client.download_file(raw_data_bucket, sample_file, temp_sample_file)\n",
    "    \n",
    "    with rasterio.open(temp_sample_file) as src:\n",
    "        metadata = {\n",
    "            \"description\": src.tags(),\n",
    "            \"driver\": src.driver,\n",
    "            \"dtype\": str(src.dtypes[0]),\n",
    "            \"nodata\": src.nodata,\n",
    "            \"width\": src.width,\n",
    "            \"height\": src.height,\n",
    "            \"count\": src.count,\n",
    "            \"crs\": str(src.crs),\n",
    "            \"transform\": list(src.transform),\n",
    "            \"bounds\": list(src.bounds),\n",
    "            \"total_files_processed\": len(files_processed),\n",
    "            \"year\": \"2000\"\n",
    "        }\n",
    "    \n",
    "    # Upload metadata\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "        fp.flush()\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            Filename=fp.name,\n",
    "            Bucket=bucket_name,\n",
    "            Key=f\"{cog_data_prefix}/metadata.json\",\n",
    "        )\n",
    "        print(f\"Uploaded metadata to s3://{bucket_name}/{cog_data_prefix}/metadata.json\")\n",
    "    \n",
    "    # Clean up sample file\n",
    "    if os.path.exists(temp_sample_file):\n",
    "        os.remove(temp_sample_file)\n",
    "\n",
    "# Save the files_processed DataFrame to CSV using the same s3_client\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".csv\") as fp:\n",
    "    files_processed.to_csv(fp.name, index=False)\n",
    "    fp.flush()\n",
    "    \n",
    "    s3_client.upload_file(\n",
    "        Filename=fp.name,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{cog_data_prefix}/files_converted.csv\",\n",
    "    )\n",
    "    print(f\"Saved processing log to s3://{bucket_name}/{cog_data_prefix}/files_converted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(f\"\\nProcessing Summary:\")\n",
    "print(f\"Total files found: {len(keys)}\")\n",
    "print(f\"Files processed: {len(files_processed)}\")\n",
    "print(f\"\\nProcessed files:\")\n",
    "files_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Features in This Version\n",
    "\n",
    "This enhanced notebook includes several improvements over the original:\n",
    "\n",
    "### 🔐 **Automatic AWS Authentication**\n",
    "- No need for `.env` files or manual credential configuration\n",
    "- Automatically detects credentials from:\n",
    "  - Environment variables\n",
    "  - AWS CLI configuration\n",
    "  - IAM roles (EC2/Lambda)\n",
    "\n",
    "### 🚀 **Simplified Setup**\n",
    "- Removed dependency on `python-dotenv`\n",
    "- Direct boto3 client initialization\n",
    "- Better error handling for authentication issues\n",
    "\n",
    "### 📊 **Additional Features**\n",
    "- fsspec filesystem integration for alternative S3 operations\n",
    "- Graceful handling of limited S3 permissions\n",
    "- Download caching to avoid re-downloading large files\n",
    "- COG validation before upload\n",
    "- Comprehensive error messages\n",
    "\n",
    "### 💡 **Usage Tips**\n",
    "1. Ensure AWS credentials are configured via one of the standard methods\n",
    "2. The notebook will automatically detect and use available credentials\n",
    "3. Check the authentication cell output to confirm S3 access\n",
    "4. Use the cache management utilities to monitor downloaded files\n",
    "\n",
    "This enhanced version follows AWS best practices and makes the notebook more portable and easier to use across different environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
